{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FineGrain-zipperpull.ipynb","provenance":[],"machine_shape":"hm","mount_file_id":"17fwbyex7f76XIVs58enSGYE8haGQUBiR","authorship_tag":"ABX9TyPh+Q3gfrzh4FedsJc0owIy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":[""],"metadata":{"id":"5fSsu0wul-Y0"}},{"cell_type":"markdown","source":["# This Notebook trains an [API-Net](https://github.com/PeiqinZhuang/API-Net) on LV data set for *main_zipper_pull*. For more details of implementation please refer to each cell."],"metadata":{"id":"yEMKYZw2k_6X"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YfLL45TvwEl3"},"outputs":[],"source":["#In this cell some utility functions are implemented. \n","\n","import torch\n","\n","def save_checkpoint(state, is_best, filename='checkpoint_main-zipper-pull.pth.tar'):\n","    torch.save(state, '/content/drive/MyDrive/'+filename)\n","    if is_best:\n","        shutil.copyfile('/content/drive/MyDrive/'+filename, '/content/drive/MyDrive/main-zipper-pull_model_best.pth.tar')\n","\n","\n","class AverageMeter(object):\n","    \"\"\"\n","    Keeps track of most recent, average, sum, and count of a metric.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","\n","def accuracy(scores, targets, k):\n","    \"\"\"\n","    Computes top-k accuracy, from predicted and true labels.\n","\n","    :param scores: scores from the model\n","    :param targets: true labels\n","    :param k: k in top-k accuracy\n","    :return: top-k accuracy\n","    \"\"\"\n","\n","    batch_size = targets.size(0)\n","    _, ind = scores.topk(k, 1, True, True)\n","    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n","    correct_total = correct.view(-1).float().sum()  # 0D tensor\n","    return correct_total.item() * (100.0 / batch_size)\n"]},{"cell_type":"code","source":["# Importing required packages.\n","from torch import nn\n","\n","import torchvision\n","from torchvision import models\n","import torchvision.transforms as transforms\n","\n","import torch.utils.data\n","from torch.utils.data import Dataset\n","from torch.utils.data.sampler import BatchSampler\n","\n","import torch.nn.functional as F\n","import torch.optim\n","\n","from PIL import Image\n","import numpy as np\n","import os\n","import time\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","import torch.backends.cudnn as cudnn\n"],"metadata":{"id":"WAp9ax85wfXZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# In this cell we implement a custom dataset that inherits from pytorch's dataset. \n","# The RandomDataset is used for validation set and BatchDataset is for training set\n","# They read the images and their labels from a .txt file that is formatted as follows\n","# /path/to/imagefile/.jpg label\n","\n","def default_loader(path):\n","    try:\n","        img = Image.open(path).convert('RGB')\n","    except:\n","        with open('read_error.txt', 'a') as fid:\n","            fid.write(path + '\\n')\n","        return Image.new('RGB', (224, 224), 'white')\n","    return img\n","\n","\n","class RandomDataset(Dataset):\n","    def __init__(self, transform=None, dataloader=default_loader):\n","        self.transform = transform\n","        self.dataloader = dataloader\n","\n","        with open('/content/drive/MyDrive/LV_data/main_zipper_pull/val/val.txt', 'r') as fid:\n","            self.imglist = fid.readlines()\n","\n","        self.labels = []\n","        for line in self.imglist:\n","            image_path, label = line.strip().split()\n","            self.labels.append(int(label))\n","        self.labels = np.array(self.labels)\n","        self.labels = torch.LongTensor(self.labels)\n","\n","    def __getitem__(self, index):\n","        image_name, label = self.imglist[index].strip().split()\n","        image_path = image_name\n","        img = self.dataloader(image_path)\n","        img = self.transform(img)\n","        label = int(label)\n","        label = torch.LongTensor([label])\n","\n","        return img, label\n","\n","    def __len__(self):\n","        return len(self.imglist)\n","\n","\n","class BatchDataset(Dataset):\n","    def __init__(self, transform=None, dataloader=default_loader):\n","        self.transform = transform\n","        self.dataloader = dataloader\n","\n","        with open('/content/drive/MyDrive/LV_data/main_zipper_pull/train/train.txt', 'r') as fid:\n","            self.imglist = fid.readlines()\n","\n","        self.labels = []\n","        for line in self.imglist:\n","            #print(i)\n","            image_path, label = line.strip().split()\n","            self.labels.append(int(label))\n","        self.labels = np.array(self.labels)\n","        self.labels = torch.LongTensor(self.labels)\n","\n","    def __getitem__(self, index):\n","        image_name, label = self.imglist[index].strip().split()\n","        image_path = image_name\n","        img = self.dataloader(image_path)\n","        img = self.transform(img)\n","        label = int(label)\n","        label = torch.LongTensor([label])\n","\n","        return img, label\n","\n","    def __len__(self):\n","        return len(self.imglist)\n","\n","\n","class BalancedBatchSampler(BatchSampler):\n","    def __init__(self, dataset, n_classes, n_samples):\n","        self.labels = dataset.labels\n","        self.labels_set = list(set(self.labels.numpy()))\n","        self.label_to_indices = {label: np.where(self.labels.numpy() == label)[0]\n","                                 for label in self.labels_set}\n","        for l in self.labels_set:\n","            np.random.shuffle(self.label_to_indices[l])\n","        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n","        self.count = 0\n","        self.n_classes = n_classes\n","        self.n_samples = n_samples\n","        self.dataset = dataset\n","        self.batch_size = self.n_samples * self.n_classes\n","\n","    def __iter__(self):\n","        self.count = 0\n","        while self.count + self.batch_size < len(self.dataset):\n","            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n","            indices = []\n","            for class_ in classes:\n","                indices.extend(self.label_to_indices[class_][\n","                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n","                                                                         class_] + self.n_samples])\n","                self.used_label_indices_count[class_] += self.n_samples\n","                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n","                    np.random.shuffle(self.label_to_indices[class_])\n","                    self.used_label_indices_count[class_] = 0\n","            yield indices\n","            self.count += self.n_classes * self.n_samples\n","\n","    def __len__(self):\n","        return len(self.dataset) // self.batch_size\n"],"metadata":{"id":"OUaDhjyXwfjY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This is the actual implementation of API-Net. For details please see the corresponding paper \n","# in official github page in above link \n","def pdist(vectors):\n","    distance_matrix = -2 * vectors.mm(torch.t(vectors)) + vectors.pow(2).sum(dim=1).view(1, -1) + vectors.pow(2).sum(\n","        dim=1).view(-1, 1)\n","    return distance_matrix\n","\n","class API_Net(nn.Module):\n","    def __init__(self):\n","        super(API_Net, self).__init__()\n","\n","        resnet101 = models.resnet101(pretrained=True)\n","        layers = list(resnet101.children())[:-2]\n","\n","        self.conv = nn.Sequential(*layers)\n","        self.avg = nn.AvgPool2d(kernel_size=14, stride=1)\n","        self.map1 = nn.Linear(2048 * 2, 512)\n","        self.map2 = nn.Linear(512, 2048)\n","        self.fc = nn.Linear(2048, 2)\n","        self.drop = nn.Dropout(p=0.5)\n","        self.sigmoid = nn.Sigmoid()\n","\n","\n","    def forward(self, images, targets=None, flag='train'):\n","        conv_out = self.conv(images)\n","        pool_out = self.avg(conv_out).squeeze()\n","\n","        if flag == 'train':\n","            intra_pairs, inter_pairs, \\\n","                    intra_labels, inter_labels = self.get_pairs(pool_out, targets)\n","\n","            features1 = torch.cat([pool_out[intra_pairs[:, 0]], pool_out[inter_pairs[:, 0]]], dim=0)\n","            features2 = torch.cat([pool_out[intra_pairs[:, 1]], pool_out[inter_pairs[:, 1]]], dim=0)\n","            labels1 = torch.cat([intra_labels[:, 0], inter_labels[:, 0]], dim=0)\n","            labels2 = torch.cat([intra_labels[:, 1], inter_labels[:, 1]], dim=0)\n","\n","\n","            mutual_features = torch.cat([features1, features2], dim=1)\n","            map1_out = self.map1(mutual_features)\n","            map2_out = self.drop(map1_out)\n","            map2_out = self.map2(map2_out)\n","\n","\n","            gate1 = torch.mul(map2_out, features1)\n","            gate1 = self.sigmoid(gate1)\n","\n","            gate2 = torch.mul(map2_out, features2)\n","            gate2 = self.sigmoid(gate2)\n","\n","            features1_self = torch.mul(gate1, features1) + features1\n","            features1_other = torch.mul(gate2, features1) + features1\n","\n","            features2_self = torch.mul(gate2, features2) + features2\n","            features2_other = torch.mul(gate1, features2) + features2\n","\n","            logit1_self = self.fc(self.drop(features1_self))\n","            logit1_other = self.fc(self.drop(features1_other))\n","            logit2_self = self.fc(self.drop(features2_self))\n","            logit2_other = self.fc(self.drop(features2_other))\n","\n","            return logit1_self, logit1_other, logit2_self, logit2_other, labels1, labels2\n","\n","        elif flag == 'val':\n","            return self.fc(pool_out)\n","\n","\n","    def get_pairs(self, embeddings, labels):\n","        distance_matrix = pdist(embeddings).detach().cpu().numpy()\n","\n","        labels = labels.detach().cpu().numpy().reshape(-1,1)\n","        num = labels.shape[0]\n","        dia_inds = np.diag_indices(num)\n","        lb_eqs = (labels == labels.T)\n","        lb_eqs[dia_inds] = False\n","        dist_same = distance_matrix.copy()\n","        dist_same[lb_eqs == False] = np.inf\n","        intra_idxs = np.argmin(dist_same, axis=1)\n","\n","        dist_diff = distance_matrix.copy()\n","        lb_eqs[dia_inds] = True\n","        dist_diff[lb_eqs == True] = np.inf\n","        inter_idxs = np.argmin(dist_diff, axis=1)\n","\n","        intra_pairs = np.zeros([embeddings.shape[0], 2])\n","        inter_pairs  = np.zeros([embeddings.shape[0], 2])\n","        intra_labels = np.zeros([embeddings.shape[0], 2])\n","        inter_labels = np.zeros([embeddings.shape[0], 2])\n","        for i in range(embeddings.shape[0]):\n","            intra_labels[i, 0] = labels[i]\n","            intra_labels[i, 1] = labels[intra_idxs[i]]\n","            intra_pairs[i, 0] = i\n","            intra_pairs[i, 1] = intra_idxs[i]\n","\n","            inter_labels[i, 0] = labels[i]\n","            inter_labels[i, 1] = labels[inter_idxs[i]]\n","            inter_pairs[i, 0] = i\n","            inter_pairs[i, 1] = inter_idxs[i]\n","\n","        intra_labels = torch.from_numpy(intra_labels).long().to(device)\n","        intra_pairs = torch.from_numpy(intra_pairs).long().to(device)\n","        inter_labels = torch.from_numpy(inter_labels).long().to(device)\n","        inter_pairs = torch.from_numpy(inter_pairs).long().to(device)\n","\n","        return intra_pairs, inter_pairs, intra_labels, inter_labels\n"],"metadata":{"id":"Dl2pxpRYwftW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Some hyper parameters. If you try to resume the training then pytorch will load\n","# the trained weight defined in variable \"resume\".\n","workers=2\n","epochs=100\n","start_epoch=0\n","lr=0.01\n","momentum=0.9\n","weight_decay=5e-4\n","print_freq=20\n","evaluate_freq=20\n","resume='/content/drive/MyDrive/checkpoint_main-zipper-pull.pth.tar'\n","n_classes=2\n","n_samples=8"],"metadata":{"id":"7J2W3e7Xwm67"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The function for training the model. notice that the \"validate\" function defined in next cell\n","# is called inside \"train\" function\n","def train(train_loader, model, criterion, optimizer_conv, scheduler_conv, optimizer_fc, scheduler_fc, epoch, step):\n","    best_prec1 = 0\n","    batch_time = AverageMeter()\n","    data_time = AverageMeter()\n","    softmax_losses = AverageMeter()\n","    rank_losses = AverageMeter()\n","    losses = AverageMeter()\n","    top1 = AverageMeter()\n","    \n","    # switch to train mode\n","    end = time.time()\n","    rank_criterion = nn.MarginRankingLoss(margin=0.05)\n","    softmax_layer = nn.Softmax(dim=0).to(device)\n","\n","    for i, (input, target) in enumerate(train_loader):\n","        model.train()\n","\n","        # measure data loading time\n","        data_time.update(time.time() - end)\n","        input_var = input.to(device)\n","        target_var = target.to(device).squeeze()\n","\n","        # compute output\n","        logit1_self, logit1_other, logit2_self, logit2_other, labels1, labels2 = model(input_var, target_var,flag='train')\n","        batch_size = logit1_self.shape[0]\n","        labels1 = labels1.to(device)\n","        labels2 = labels2.to(device)\n","\n","        self_logits = torch.zeros(2 * batch_size, 2).to(device)\n","        other_logits = torch.zeros(2 * batch_size, 2).to(device)\n","        self_logits[:batch_size] = logit1_self\n","        self_logits[batch_size:] = logit2_self\n","        other_logits[:batch_size] = logit1_other\n","        other_logits[batch_size:] = logit2_other\n","\n","        # compute loss\n","        logits = torch.cat([self_logits, other_logits], dim=0)\n","        targets = torch.cat([labels1, labels2, labels1, labels2], dim=0)\n","    \n","        softmax_loss = criterion(logits, targets)\n","\n","        self_scores = softmax_layer(self_logits)[torch.arange(2 * batch_size).to(device).long(),\n","                                                 torch.cat([labels1, labels2], dim=0)]\n","        other_scores = softmax_layer(other_logits)[torch.arange(2 * batch_size).to(device).long(),\n","                                                   torch.cat([labels1, labels2], dim=0)]\n","        flag = torch.ones([2 * batch_size, ]).to(device)\n","        rank_loss = rank_criterion(self_scores, other_scores, flag)\n","\n","        loss = softmax_loss + rank_loss\n","\n","        # measure accuracy and record loss\n","        prec1 = accuracy(logits, targets, 1)\n","        #prec2 = accuracy(logits, targets, 2)\n","        losses.update(loss.item(), 2 * batch_size)\n","        softmax_losses.update(softmax_loss.item(), 4 * batch_size)\n","        rank_losses.update(rank_loss.item(), 2 * batch_size)\n","        top1.update(prec1, 4 * batch_size)\n","        #top2.update(prec2, 4 * batch_size)\n","\n","        # compute gradient and do SGD step\n","        optimizer_conv.zero_grad()\n","        optimizer_fc.zero_grad()\n","        loss.backward()\n","        if epoch >= 8:\n","            optimizer_conv.step()\n","        optimizer_fc.step()\n","        scheduler_conv.step()\n","        scheduler_fc.step()\n","\n","        # measure elapsed time\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","\n","        if i % print_freq == 0:\n","            print('Time: {time}\\nStep: {step}\\t Epoch: [{0}][{1}/{2}]\\t'\n","                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n","                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","                  'SoftmaxLoss {softmax_loss.val:.4f} ({softmax_loss.avg:.4f})\\t'\n","                  'RankLoss {rank_loss.val:.4f} ({rank_loss.avg:.4f})\\t'\n","                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n","                epoch, i, len(train_loader), batch_time=batch_time,\n","                data_time=data_time, loss=losses, softmax_loss=softmax_losses, rank_loss=rank_losses,\n","                top1=top1, step=step, time=time.asctime(time.localtime(time.time()))))\n","\n","        if i == len(train_loader) - 1:\n","            val_dataset = RandomDataset(transform=transforms.Compose([\n","                transforms.Resize([512, 512]),\n","                transforms.CenterCrop([448, 448]),\n","                transforms.ToTensor(),\n","                transforms.Normalize(\n","                    mean=(0.485, 0.456, 0.406),\n","                    std=(0.229, 0.224, 0.225)\n","                )]))\n","            val_loader = torch.utils.data.DataLoader(\n","                val_dataset, batch_size=batch_size, shuffle=False,\n","                num_workers=workers, pin_memory=True)\n","            prec1 = validate(val_loader, model, criterion)\n","\n","            # remember best prec@1 and save checkpoint\n","            is_best = prec1 > best_prec1\n","            best_prec1 = max(prec1, best_prec1)\n","            save_checkpoint({\n","                'epoch': epoch + 1,\n","                'state_dict': model.state_dict(),\n","                'best_prec1': best_prec1,\n","                'optimizer_conv': optimizer_conv.state_dict(),\n","                'optimizer_fc': optimizer_fc.state_dict(),\n","            }, is_best)\n","\n","        step = step + 1\n","    return step"],"metadata":{"id":"dlJYdkMwwnCq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def validate(val_loader, model, criterion):\n","    batch_time = AverageMeter()\n","    softmax_losses = AverageMeter()\n","    top1 = AverageMeter()\n","    #top2 = AverageMeter()\n","\n","    # switch to evaluate mode\n","    model.eval()\n","    end = time.time()\n","\n","    with torch.no_grad():\n","        for i, (input, target) in enumerate(val_loader):\n","\n","            input_var = input.to(device)\n","            target_var = target.to(device).squeeze()\n","\n","            # compute output\n","            logits = model(input_var, targets=None, flag='val')\n","            #print(logits)\n","            #print(target_var)\n","            softmax_loss = criterion(logits, target_var)\n","            \n","\n","            prec1 = accuracy(logits, target_var, 1)\n","            #prec2 = accuracy(logits, target_var, 2)\n","            softmax_losses.update(softmax_loss.item(), logits.size(0))\n","            top1.update(prec1, logits.size(0))\n","            #top2.update(prec2, logits.size(0))\n","\n","            # measure elapsed time\n","            batch_time.update(time.time() - end)\n","            end = time.time()\n","\n","            if i % print_freq == 0:\n","                print('Time: {time}\\nTest: [{0}/{1}]\\t'\n","                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","                      'SoftmaxLoss {softmax_loss.val:.4f} ({softmax_loss.avg:.4f})\\t'\n","                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n","                    i, len(val_loader), batch_time=batch_time, softmax_loss=softmax_losses,\n","                    top1=top1, time=time.asctime(time.localtime(time.time()))))\n","        print(' * Prec@1 {top1.avg:.3f}'.format(top1=top1))\n","\n","    return top1.avg\n"],"metadata":{"id":"gUytnwXzwnNI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#The main cell. if you've done with above cells you can start training by running this cell.\n","torch.manual_seed(20)\n","torch.cuda.manual_seed_all(20)\n","np.random.seed(25)\n","epochs=100\n","# create model\n","model = API_Net()\n","model = model.to(device)\n","model.conv = nn.DataParallel(model.conv)\n","\n","# define loss function (criterion) and optimizer\n","criterion = nn.CrossEntropyLoss().to(device)\n","optimizer_conv = torch.optim.SGD(model.conv.parameters(), lr,\n","                                  momentum=momentum,\n","                                  weight_decay=weight_decay)\n","\n","fc_parameters = [value for name, value in model.named_parameters() if 'conv' not in name]\n","optimizer_fc = torch.optim.SGD(fc_parameters, lr,\n","                                momentum=momentum,\n","                                weight_decay=weight_decay)\n","if resume:\n","    if os.path.isfile(resume):\n","        print('loading checkpoint {}'.format(resume))\n","        checkpoint = torch.load(resume)\n","        start_epoch = checkpoint['epoch']\n","        best_prec1 = checkpoint['best_prec1']\n","        model.load_state_dict(checkpoint['state_dict'])\n","        optimizer_conv.load_state_dict(checkpoint['optimizer_conv'])\n","        optimizer_fc.load_state_dict(checkpoint['optimizer_fc'])\n","        print('loaded checkpoint {}(epoch {})'.format(resume, checkpoint['epoch']))\n","    else:\n","        print('no checkpoint found at {}'.format(resume))\n","\n","cudnn.benchmark = True\n","# Data loading code\n","train_dataset = BatchDataset(transform=transforms.Compose([\n","    transforms.Resize([512, 512]),\n","    transforms.RandomCrop([448, 448]),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        mean=(0.485, 0.456, 0.406),\n","        std=(0.229, 0.224, 0.225)\n","    )]))\n","\n","train_sampler = BalancedBatchSampler(train_dataset, n_classes, n_samples)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=train_sampler, num_workers=workers, pin_memory=True)\n","scheduler_conv = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_conv, 100 * len(train_loader))\n","scheduler_fc = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_fc, 100 * len(train_loader))\n","\n","step = 0\n","print('START TIME:', time.asctime(time.localtime(time.time())))\n","for epoch in range(start_epoch, epochs):\n","    step = train(train_loader, model, criterion, optimizer_conv, scheduler_conv, optimizer_fc, scheduler_fc, epoch, step)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hTTlt_HswyBP","executionInfo":{"status":"ok","timestamp":1648773520166,"user_tz":240,"elapsed":2515800,"user":{"displayName":"Reza hojjaty saeedy","userId":"18162372094943119950"}},"outputId":"939ceff0-1c90-4797-dfe9-3b9bdc5898e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["loading checkpoint /content/drive/MyDrive/checkpoint_main-zipper-pull.pth.tar\n","loaded checkpoint /content/drive/MyDrive/checkpoint_main-zipper-pull.pth.tar(epoch 80)\n","START TIME: Thu Mar 31 23:56:46 2022\n","Time: Thu Mar 31 23:56:47 2022\n","Step: 0\t Epoch: [80][0/204]\tTime 1.579 (1.579)\tData 1.065 (1.065)\tLoss 0.2291 (0.2291)\tSoftmaxLoss 0.1789 (0.1789)\tRankLoss 0.0501 (0.0501)\tPrec@1 84.375 (84.375)\n","Time: Thu Mar 31 23:56:58 2022\n","Step: 20\t Epoch: [80][20/204]\tTime 0.596 (0.573)\tData 0.099 (0.074)\tLoss 0.0674 (0.1043)\tSoftmaxLoss 0.0166 (0.0537)\tRankLoss 0.0508 (0.0506)\tPrec@1 100.000 (97.061)\n","Time: Thu Mar 31 23:57:08 2022\n","Step: 40\t Epoch: [80][40/204]\tTime 0.531 (0.552)\tData 0.027 (0.053)\tLoss 0.0857 (0.0970)\tSoftmaxLoss 0.0345 (0.0465)\tRankLoss 0.0512 (0.0505)\tPrec@1 100.000 (97.961)\n","Time: Thu Mar 31 23:57:20 2022\n","Step: 60\t Epoch: [80][60/204]\tTime 0.505 (0.555)\tData 0.004 (0.056)\tLoss 0.0512 (0.0893)\tSoftmaxLoss 0.0012 (0.0388)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (98.425)\n","Time: Thu Mar 31 23:57:30 2022\n","Step: 80\t Epoch: [80][80/204]\tTime 0.503 (0.546)\tData 0.003 (0.046)\tLoss 0.1284 (0.0884)\tSoftmaxLoss 0.0774 (0.0379)\tRankLoss 0.0510 (0.0506)\tPrec@1 97.656 (98.399)\n","Time: Thu Mar 31 23:57:41 2022\n","Step: 100\t Epoch: [80][100/204]\tTime 0.499 (0.545)\tData 0.002 (0.045)\tLoss 0.2766 (0.0933)\tSoftmaxLoss 0.2266 (0.0427)\tRankLoss 0.0500 (0.0506)\tPrec@1 82.812 (98.190)\n","Time: Thu Mar 31 23:57:51 2022\n","Step: 120\t Epoch: [80][120/204]\tTime 0.505 (0.542)\tData 0.003 (0.041)\tLoss 0.0503 (0.1038)\tSoftmaxLoss 0.0002 (0.0531)\tRankLoss 0.0502 (0.0507)\tPrec@1 100.000 (97.766)\n","Time: Thu Mar 31 23:58:01 2022\n","Step: 140\t Epoch: [80][140/204]\tTime 0.511 (0.536)\tData 0.003 (0.036)\tLoss 0.1427 (0.1086)\tSoftmaxLoss 0.0925 (0.0579)\tRankLoss 0.0502 (0.0507)\tPrec@1 96.094 (97.667)\n","Time: Thu Mar 31 23:58:12 2022\n","Step: 160\t Epoch: [80][160/204]\tTime 0.508 (0.535)\tData 0.003 (0.035)\tLoss 0.0528 (0.1053)\tSoftmaxLoss 0.0027 (0.0546)\tRankLoss 0.0500 (0.0507)\tPrec@1 100.000 (97.782)\n","Time: Thu Mar 31 23:58:22 2022\n","Step: 180\t Epoch: [80][180/204]\tTime 0.503 (0.534)\tData 0.002 (0.034)\tLoss 0.0600 (0.1019)\tSoftmaxLoss 0.0086 (0.0513)\tRankLoss 0.0514 (0.0507)\tPrec@1 100.000 (97.941)\n","Time: Thu Mar 31 23:58:33 2022\n","Step: 200\t Epoch: [80][200/204]\tTime 0.501 (0.531)\tData 0.002 (0.030)\tLoss 0.0506 (0.0980)\tSoftmaxLoss 0.0006 (0.0473)\tRankLoss 0.0500 (0.0507)\tPrec@1 100.000 (98.142)\n","Time: Thu Mar 31 23:58:36 2022\n","Test: [0/14]\tTime 1.688 (1.688)\tSoftmaxLoss 0.3390 (0.3390)\tPrec@1 87.500 (87.500)\n"," * Prec@1 88.591\n","Time: Thu Mar 31 23:58:52 2022\n","Step: 204\t Epoch: [81][0/204]\tTime 1.686 (1.686)\tData 1.180 (1.180)\tLoss 0.0911 (0.0911)\tSoftmaxLoss 0.0407 (0.0407)\tRankLoss 0.0505 (0.0505)\tPrec@1 100.000 (100.000)\n","Time: Thu Mar 31 23:59:03 2022\n","Step: 224\t Epoch: [81][20/204]\tTime 0.509 (0.618)\tData 0.004 (0.114)\tLoss 0.0599 (0.0907)\tSoftmaxLoss 0.0096 (0.0401)\tRankLoss 0.0502 (0.0506)\tPrec@1 100.000 (98.363)\n","Time: Thu Mar 31 23:59:14 2022\n","Step: 244\t Epoch: [81][40/204]\tTime 0.503 (0.582)\tData 0.003 (0.079)\tLoss 0.0714 (0.0790)\tSoftmaxLoss 0.0188 (0.0284)\tRankLoss 0.0526 (0.0506)\tPrec@1 100.000 (99.104)\n","Time: Thu Mar 31 23:59:25 2022\n","Step: 264\t Epoch: [81][60/204]\tTime 0.503 (0.564)\tData 0.003 (0.061)\tLoss 0.0709 (0.0781)\tSoftmaxLoss 0.0206 (0.0274)\tRankLoss 0.0503 (0.0506)\tPrec@1 100.000 (99.168)\n","Time: Thu Mar 31 23:59:35 2022\n","Step: 284\t Epoch: [81][80/204]\tTime 0.504 (0.552)\tData 0.002 (0.050)\tLoss 0.0711 (0.0756)\tSoftmaxLoss 0.0205 (0.0249)\tRankLoss 0.0506 (0.0507)\tPrec@1 100.000 (99.199)\n","Time: Thu Mar 31 23:59:45 2022\n","Step: 304\t Epoch: [81][100/204]\tTime 0.502 (0.543)\tData 0.003 (0.042)\tLoss 0.3943 (0.0803)\tSoftmaxLoss 0.3440 (0.0296)\tRankLoss 0.0502 (0.0507)\tPrec@1 78.906 (98.917)\n","Time: Thu Mar 31 23:59:55 2022\n","Step: 324\t Epoch: [81][120/204]\tTime 0.517 (0.538)\tData 0.003 (0.037)\tLoss 0.0798 (0.0785)\tSoftmaxLoss 0.0293 (0.0277)\tRankLoss 0.0505 (0.0507)\tPrec@1 100.000 (98.986)\n","Time: Fri Apr  1 00:00:06 2022\n","Step: 344\t Epoch: [81][140/204]\tTime 0.508 (0.539)\tData 0.003 (0.038)\tLoss 0.0674 (0.0768)\tSoftmaxLoss 0.0160 (0.0261)\tRankLoss 0.0514 (0.0507)\tPrec@1 100.000 (99.097)\n","Time: Fri Apr  1 00:00:17 2022\n","Step: 364\t Epoch: [81][160/204]\tTime 0.506 (0.538)\tData 0.003 (0.036)\tLoss 0.0580 (0.0752)\tSoftmaxLoss 0.0065 (0.0244)\tRankLoss 0.0515 (0.0507)\tPrec@1 100.000 (99.185)\n","Time: Fri Apr  1 00:00:27 2022\n","Step: 384\t Epoch: [81][180/204]\tTime 0.507 (0.535)\tData 0.003 (0.034)\tLoss 0.0534 (0.0731)\tSoftmaxLoss 0.0012 (0.0223)\tRankLoss 0.0523 (0.0508)\tPrec@1 100.000 (99.275)\n","Time: Fri Apr  1 00:00:38 2022\n","Step: 404\t Epoch: [81][200/204]\tTime 0.679 (0.534)\tData 0.177 (0.033)\tLoss 0.0563 (0.0718)\tSoftmaxLoss 0.0055 (0.0210)\tRankLoss 0.0508 (0.0508)\tPrec@1 100.000 (99.316)\n","Time: Fri Apr  1 00:00:41 2022\n","Test: [0/14]\tTime 1.721 (1.721)\tSoftmaxLoss 0.4099 (0.4099)\tPrec@1 87.500 (87.500)\n"," * Prec@1 88.143\n","Time: Fri Apr  1 00:00:57 2022\n","Step: 408\t Epoch: [82][0/204]\tTime 1.793 (1.793)\tData 1.273 (1.273)\tLoss 0.0501 (0.0501)\tSoftmaxLoss 0.0000 (0.0000)\tRankLoss 0.0501 (0.0501)\tPrec@1 100.000 (100.000)\n","Time: Fri Apr  1 00:01:08 2022\n","Step: 428\t Epoch: [82][20/204]\tTime 0.568 (0.633)\tData 0.064 (0.130)\tLoss 0.0520 (0.0811)\tSoftmaxLoss 0.0004 (0.0303)\tRankLoss 0.0516 (0.0508)\tPrec@1 100.000 (98.772)\n","Time: Fri Apr  1 00:01:19 2022\n","Step: 448\t Epoch: [82][40/204]\tTime 0.499 (0.588)\tData 0.003 (0.085)\tLoss 0.0503 (0.0866)\tSoftmaxLoss 0.0002 (0.0357)\tRankLoss 0.0501 (0.0510)\tPrec@1 100.000 (98.590)\n","Time: Fri Apr  1 00:01:30 2022\n","Step: 468\t Epoch: [82][60/204]\tTime 0.498 (0.571)\tData 0.002 (0.069)\tLoss 0.0521 (0.0948)\tSoftmaxLoss 0.0021 (0.0438)\tRankLoss 0.0500 (0.0510)\tPrec@1 100.000 (98.053)\n","Time: Fri Apr  1 00:01:40 2022\n","Step: 488\t Epoch: [82][80/204]\tTime 0.513 (0.559)\tData 0.003 (0.057)\tLoss 0.0952 (0.0947)\tSoftmaxLoss 0.0439 (0.0438)\tRankLoss 0.0513 (0.0509)\tPrec@1 100.000 (97.946)\n","Time: Fri Apr  1 00:01:50 2022\n","Step: 508\t Epoch: [82][100/204]\tTime 0.506 (0.548)\tData 0.003 (0.046)\tLoss 0.0718 (0.0939)\tSoftmaxLoss 0.0218 (0.0429)\tRankLoss 0.0500 (0.0510)\tPrec@1 100.000 (98.043)\n","Time: Fri Apr  1 00:02:01 2022\n","Step: 528\t Epoch: [82][120/204]\tTime 0.494 (0.541)\tData 0.002 (0.039)\tLoss 0.0657 (0.0932)\tSoftmaxLoss 0.0154 (0.0423)\tRankLoss 0.0503 (0.0509)\tPrec@1 100.000 (98.153)\n","Time: Fri Apr  1 00:02:11 2022\n","Step: 548\t Epoch: [82][140/204]\tTime 0.497 (0.537)\tData 0.002 (0.036)\tLoss 0.0519 (0.0962)\tSoftmaxLoss 0.0008 (0.0453)\tRankLoss 0.0511 (0.0510)\tPrec@1 100.000 (97.933)\n","Time: Fri Apr  1 00:02:22 2022\n","Step: 568\t Epoch: [82][160/204]\tTime 0.504 (0.537)\tData 0.002 (0.035)\tLoss 0.1444 (0.1048)\tSoftmaxLoss 0.0933 (0.0538)\tRankLoss 0.0512 (0.0509)\tPrec@1 97.656 (97.768)\n","Time: Fri Apr  1 00:02:32 2022\n","Step: 588\t Epoch: [82][180/204]\tTime 0.499 (0.537)\tData 0.002 (0.035)\tLoss 0.1412 (0.1194)\tSoftmaxLoss 0.0907 (0.0684)\tRankLoss 0.0504 (0.0509)\tPrec@1 95.312 (97.190)\n","Time: Fri Apr  1 00:02:43 2022\n","Step: 608\t Epoch: [82][200/204]\tTime 0.508 (0.536)\tData 0.003 (0.034)\tLoss 0.0711 (0.1376)\tSoftmaxLoss 0.0211 (0.0867)\tRankLoss 0.0500 (0.0509)\tPrec@1 100.000 (96.409)\n","Time: Fri Apr  1 00:02:46 2022\n","Test: [0/14]\tTime 1.709 (1.709)\tSoftmaxLoss 0.8324 (0.8324)\tPrec@1 78.125 (78.125)\n"," * Prec@1 78.523\n","Time: Fri Apr  1 00:03:03 2022\n","Step: 612\t Epoch: [83][0/204]\tTime 1.885 (1.885)\tData 1.357 (1.357)\tLoss 0.2428 (0.2428)\tSoftmaxLoss 0.1925 (0.1925)\tRankLoss 0.0502 (0.0502)\tPrec@1 87.500 (87.500)\n","Time: Fri Apr  1 00:03:15 2022\n","Step: 632\t Epoch: [83][20/204]\tTime 1.044 (0.651)\tData 0.536 (0.144)\tLoss 0.8718 (0.2855)\tSoftmaxLoss 0.8218 (0.2352)\tRankLoss 0.0500 (0.0503)\tPrec@1 66.406 (91.853)\n","Time: Fri Apr  1 00:03:26 2022\n","Step: 652\t Epoch: [83][40/204]\tTime 0.571 (0.599)\tData 0.073 (0.096)\tLoss 0.3631 (0.2738)\tSoftmaxLoss 0.3130 (0.2235)\tRankLoss 0.0501 (0.0503)\tPrec@1 78.906 (91.044)\n","Time: Fri Apr  1 00:03:37 2022\n","Step: 672\t Epoch: [83][60/204]\tTime 0.505 (0.581)\tData 0.004 (0.078)\tLoss 0.1388 (0.2560)\tSoftmaxLoss 0.0885 (0.2058)\tRankLoss 0.0502 (0.0502)\tPrec@1 93.750 (91.803)\n","Time: Fri Apr  1 00:03:47 2022\n","Step: 692\t Epoch: [83][80/204]\tTime 0.504 (0.565)\tData 0.003 (0.063)\tLoss 0.3434 (0.2300)\tSoftmaxLoss 0.2931 (0.1798)\tRankLoss 0.0503 (0.0502)\tPrec@1 84.375 (92.699)\n","Time: Fri Apr  1 00:03:58 2022\n","Step: 712\t Epoch: [83][100/204]\tTime 0.502 (0.556)\tData 0.002 (0.054)\tLoss 0.0531 (0.2118)\tSoftmaxLoss 0.0031 (0.1616)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (93.564)\n","Time: Fri Apr  1 00:04:08 2022\n","Step: 732\t Epoch: [83][120/204]\tTime 0.511 (0.554)\tData 0.003 (0.052)\tLoss 0.0746 (0.2001)\tSoftmaxLoss 0.0246 (0.1500)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (93.950)\n","Time: Fri Apr  1 00:04:19 2022\n","Step: 752\t Epoch: [83][140/204]\tTime 0.504 (0.547)\tData 0.003 (0.045)\tLoss 0.2904 (0.1957)\tSoftmaxLoss 0.2404 (0.1455)\tRankLoss 0.0500 (0.0502)\tPrec@1 85.156 (94.094)\n","Time: Fri Apr  1 00:04:29 2022\n","Step: 772\t Epoch: [83][160/204]\tTime 0.502 (0.543)\tData 0.002 (0.041)\tLoss 0.1007 (0.1913)\tSoftmaxLoss 0.0507 (0.1412)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (94.172)\n","Time: Fri Apr  1 00:04:40 2022\n","Step: 792\t Epoch: [83][180/204]\tTime 0.495 (0.542)\tData 0.002 (0.041)\tLoss 0.0911 (0.1921)\tSoftmaxLoss 0.0411 (0.1419)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (94.156)\n","Time: Fri Apr  1 00:04:50 2022\n","Step: 812\t Epoch: [83][200/204]\tTime 0.497 (0.542)\tData 0.001 (0.041)\tLoss 0.1962 (0.1837)\tSoftmaxLoss 0.1458 (0.1335)\tRankLoss 0.0504 (0.0502)\tPrec@1 90.625 (94.527)\n","Time: Fri Apr  1 00:04:54 2022\n","Test: [0/14]\tTime 1.710 (1.710)\tSoftmaxLoss 0.1662 (0.1662)\tPrec@1 90.625 (90.625)\n"," * Prec@1 87.919\n","Time: Fri Apr  1 00:05:10 2022\n","Step: 816\t Epoch: [84][0/204]\tTime 1.636 (1.636)\tData 1.115 (1.115)\tLoss 0.0521 (0.0521)\tSoftmaxLoss 0.0021 (0.0021)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n","Time: Fri Apr  1 00:05:21 2022\n","Step: 836\t Epoch: [84][20/204]\tTime 0.525 (0.587)\tData 0.019 (0.081)\tLoss 0.1490 (0.1043)\tSoftmaxLoss 0.0980 (0.0540)\tRankLoss 0.0510 (0.0503)\tPrec@1 96.094 (98.586)\n","Time: Fri Apr  1 00:05:32 2022\n","Step: 856\t Epoch: [84][40/204]\tTime 0.495 (0.574)\tData 0.002 (0.072)\tLoss 0.0766 (0.1073)\tSoftmaxLoss 0.0265 (0.0570)\tRankLoss 0.0501 (0.0503)\tPrec@1 100.000 (98.304)\n","Time: Fri Apr  1 00:05:43 2022\n","Step: 876\t Epoch: [84][60/204]\tTime 0.495 (0.567)\tData 0.002 (0.065)\tLoss 0.0512 (0.1030)\tSoftmaxLoss 0.0012 (0.0527)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (98.117)\n","Time: Fri Apr  1 00:05:54 2022\n","Step: 896\t Epoch: [84][80/204]\tTime 0.498 (0.561)\tData 0.002 (0.060)\tLoss 0.0592 (0.1011)\tSoftmaxLoss 0.0092 (0.0508)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (98.264)\n","Time: Fri Apr  1 00:06:04 2022\n","Step: 916\t Epoch: [84][100/204]\tTime 0.507 (0.554)\tData 0.003 (0.053)\tLoss 0.0562 (0.0972)\tSoftmaxLoss 0.0058 (0.0469)\tRankLoss 0.0504 (0.0503)\tPrec@1 100.000 (98.407)\n","Time: Fri Apr  1 00:06:15 2022\n","Step: 936\t Epoch: [84][120/204]\tTime 0.503 (0.549)\tData 0.003 (0.047)\tLoss 0.0574 (0.1053)\tSoftmaxLoss 0.0072 (0.0550)\tRankLoss 0.0502 (0.0503)\tPrec@1 100.000 (98.063)\n","Time: Fri Apr  1 00:06:25 2022\n","Step: 956\t Epoch: [84][140/204]\tTime 0.499 (0.543)\tData 0.003 (0.041)\tLoss 0.0508 (0.1015)\tSoftmaxLoss 0.0008 (0.0512)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (98.216)\n","Time: Fri Apr  1 00:06:35 2022\n","Step: 976\t Epoch: [84][160/204]\tTime 0.505 (0.540)\tData 0.002 (0.038)\tLoss 0.0547 (0.0971)\tSoftmaxLoss 0.0047 (0.0468)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (98.413)\n","Time: Fri Apr  1 00:06:46 2022\n","Step: 996\t Epoch: [84][180/204]\tTime 0.504 (0.538)\tData 0.003 (0.035)\tLoss 0.0628 (0.0961)\tSoftmaxLoss 0.0128 (0.0458)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (98.420)\n","Time: Fri Apr  1 00:06:56 2022\n","Step: 1016\t Epoch: [84][200/204]\tTime 0.573 (0.535)\tData 0.069 (0.033)\tLoss 0.0514 (0.0946)\tSoftmaxLoss 0.0006 (0.0443)\tRankLoss 0.0508 (0.0503)\tPrec@1 100.000 (98.434)\n","Time: Fri Apr  1 00:06:59 2022\n","Test: [0/14]\tTime 1.695 (1.695)\tSoftmaxLoss 0.3766 (0.3766)\tPrec@1 87.500 (87.500)\n"," * Prec@1 88.814\n","Time: Fri Apr  1 00:07:15 2022\n","Step: 1020\t Epoch: [85][0/204]\tTime 1.583 (1.583)\tData 1.071 (1.071)\tLoss 0.0825 (0.0825)\tSoftmaxLoss 0.0298 (0.0298)\tRankLoss 0.0526 (0.0526)\tPrec@1 100.000 (100.000)\n","Time: Fri Apr  1 00:07:26 2022\n","Step: 1040\t Epoch: [85][20/204]\tTime 0.648 (0.584)\tData 0.142 (0.079)\tLoss 0.0554 (0.0708)\tSoftmaxLoss 0.0049 (0.0202)\tRankLoss 0.0504 (0.0506)\tPrec@1 100.000 (99.256)\n","Time: Fri Apr  1 00:07:37 2022\n","Step: 1060\t Epoch: [85][40/204]\tTime 0.727 (0.570)\tData 0.228 (0.066)\tLoss 0.0510 (0.0649)\tSoftmaxLoss 0.0010 (0.0144)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (99.619)\n","Time: Fri Apr  1 00:07:47 2022\n","Step: 1080\t Epoch: [85][60/204]\tTime 0.500 (0.554)\tData 0.002 (0.052)\tLoss 0.0805 (0.0634)\tSoftmaxLoss 0.0292 (0.0130)\tRankLoss 0.0514 (0.0504)\tPrec@1 100.000 (99.693)\n","Time: Fri Apr  1 00:07:58 2022\n","Step: 1100\t Epoch: [85][80/204]\tTime 0.513 (0.546)\tData 0.003 (0.044)\tLoss 0.0504 (0.0610)\tSoftmaxLoss 0.0004 (0.0105)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (99.769)\n","Time: Fri Apr  1 00:08:08 2022\n","Step: 1120\t Epoch: [85][100/204]\tTime 0.504 (0.539)\tData 0.002 (0.037)\tLoss 0.0563 (0.0615)\tSoftmaxLoss 0.0026 (0.0111)\tRankLoss 0.0536 (0.0504)\tPrec@1 100.000 (99.745)\n","Time: Fri Apr  1 00:08:19 2022\n","Step: 1140\t Epoch: [85][120/204]\tTime 0.507 (0.542)\tData 0.003 (0.040)\tLoss 0.0602 (0.0609)\tSoftmaxLoss 0.0102 (0.0105)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (99.780)\n","Time: Fri Apr  1 00:08:29 2022\n","Step: 1160\t Epoch: [85][140/204]\tTime 0.504 (0.537)\tData 0.003 (0.035)\tLoss 0.0515 (0.0598)\tSoftmaxLoss 0.0005 (0.0094)\tRankLoss 0.0510 (0.0504)\tPrec@1 100.000 (99.812)\n","Time: Fri Apr  1 00:08:39 2022\n","Step: 1180\t Epoch: [85][160/204]\tTime 0.506 (0.534)\tData 0.003 (0.032)\tLoss 0.0504 (0.0593)\tSoftmaxLoss 0.0004 (0.0089)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (99.825)\n","Time: Fri Apr  1 00:08:50 2022\n","Step: 1200\t Epoch: [85][180/204]\tTime 0.508 (0.533)\tData 0.003 (0.031)\tLoss 0.0516 (0.0587)\tSoftmaxLoss 0.0016 (0.0082)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (99.845)\n","Time: Fri Apr  1 00:09:01 2022\n","Step: 1220\t Epoch: [85][200/204]\tTime 0.505 (0.533)\tData 0.003 (0.030)\tLoss 0.1113 (0.0598)\tSoftmaxLoss 0.0607 (0.0093)\tRankLoss 0.0506 (0.0505)\tPrec@1 98.438 (99.810)\n","Time: Fri Apr  1 00:09:04 2022\n","Test: [0/14]\tTime 1.737 (1.737)\tSoftmaxLoss 0.4639 (0.4639)\tPrec@1 84.375 (84.375)\n"," * Prec@1 88.143\n","Time: Fri Apr  1 00:09:20 2022\n","Step: 1224\t Epoch: [86][0/204]\tTime 1.924 (1.924)\tData 1.404 (1.404)\tLoss 0.0519 (0.0519)\tSoftmaxLoss 0.0014 (0.0014)\tRankLoss 0.0505 (0.0505)\tPrec@1 100.000 (100.000)\n","Time: Fri Apr  1 00:09:31 2022\n","Step: 1244\t Epoch: [86][20/204]\tTime 0.513 (0.607)\tData 0.002 (0.104)\tLoss 0.0549 (0.0600)\tSoftmaxLoss 0.0048 (0.0096)\tRankLoss 0.0502 (0.0505)\tPrec@1 100.000 (99.963)\n","Time: Fri Apr  1 00:09:42 2022\n","Step: 1264\t Epoch: [86][40/204]\tTime 0.502 (0.580)\tData 0.003 (0.078)\tLoss 0.0520 (0.0651)\tSoftmaxLoss 0.0016 (0.0145)\tRankLoss 0.0504 (0.0506)\tPrec@1 100.000 (99.562)\n","Time: Fri Apr  1 00:09:52 2022\n","Step: 1284\t Epoch: [86][60/204]\tTime 0.501 (0.561)\tData 0.002 (0.059)\tLoss 0.0510 (0.0630)\tSoftmaxLoss 0.0001 (0.0124)\tRankLoss 0.0509 (0.0506)\tPrec@1 100.000 (99.693)\n","Time: Fri Apr  1 00:10:02 2022\n","Step: 1304\t Epoch: [86][80/204]\tTime 0.506 (0.548)\tData 0.003 (0.046)\tLoss 0.0560 (0.0610)\tSoftmaxLoss 0.0042 (0.0103)\tRankLoss 0.0518 (0.0506)\tPrec@1 100.000 (99.769)\n","Time: Fri Apr  1 00:10:13 2022\n","Step: 1324\t Epoch: [86][100/204]\tTime 0.506 (0.545)\tData 0.002 (0.043)\tLoss 0.0572 (0.0796)\tSoftmaxLoss 0.0071 (0.0289)\tRankLoss 0.0502 (0.0507)\tPrec@1 100.000 (99.304)\n","Time: Fri Apr  1 00:10:23 2022\n","Step: 1344\t Epoch: [86][120/204]\tTime 0.518 (0.539)\tData 0.003 (0.038)\tLoss 0.1253 (0.0881)\tSoftmaxLoss 0.0736 (0.0374)\tRankLoss 0.0517 (0.0507)\tPrec@1 98.438 (98.915)\n","Time: Fri Apr  1 00:10:34 2022\n","Step: 1364\t Epoch: [86][140/204]\tTime 0.506 (0.541)\tData 0.002 (0.040)\tLoss 0.0709 (0.0941)\tSoftmaxLoss 0.0206 (0.0434)\tRankLoss 0.0503 (0.0507)\tPrec@1 100.000 (98.631)\n","Time: Fri Apr  1 00:10:45 2022\n","Step: 1384\t Epoch: [86][160/204]\tTime 0.689 (0.540)\tData 0.188 (0.039)\tLoss 0.0841 (0.0972)\tSoftmaxLoss 0.0341 (0.0465)\tRankLoss 0.0500 (0.0507)\tPrec@1 100.000 (98.510)\n","Time: Fri Apr  1 00:10:55 2022\n","Step: 1404\t Epoch: [86][180/204]\tTime 0.504 (0.537)\tData 0.002 (0.035)\tLoss 0.0549 (0.0948)\tSoftmaxLoss 0.0048 (0.0442)\tRankLoss 0.0501 (0.0506)\tPrec@1 100.000 (98.550)\n","Time: Fri Apr  1 00:11:05 2022\n","Step: 1424\t Epoch: [86][200/204]\tTime 0.502 (0.534)\tData 0.003 (0.032)\tLoss 0.1043 (0.0945)\tSoftmaxLoss 0.0543 (0.0438)\tRankLoss 0.0500 (0.0506)\tPrec@1 100.000 (98.585)\n","Time: Fri Apr  1 00:11:09 2022\n","Test: [0/14]\tTime 1.725 (1.725)\tSoftmaxLoss 0.4847 (0.4847)\tPrec@1 87.500 (87.500)\n"," * Prec@1 86.801\n","Time: Fri Apr  1 00:11:25 2022\n","Step: 1428\t Epoch: [87][0/204]\tTime 1.446 (1.446)\tData 0.938 (0.938)\tLoss 0.0606 (0.0606)\tSoftmaxLoss 0.0106 (0.0106)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n","Time: Fri Apr  1 00:11:36 2022\n","Step: 1448\t Epoch: [87][20/204]\tTime 0.509 (0.591)\tData 0.003 (0.089)\tLoss 0.1110 (0.1913)\tSoftmaxLoss 0.0602 (0.1407)\tRankLoss 0.0508 (0.0506)\tPrec@1 100.000 (94.680)\n","Time: Fri Apr  1 00:11:47 2022\n","Step: 1468\t Epoch: [87][40/204]\tTime 0.504 (0.565)\tData 0.003 (0.062)\tLoss 0.0744 (0.1982)\tSoftmaxLoss 0.0243 (0.1476)\tRankLoss 0.0500 (0.0506)\tPrec@1 100.000 (94.874)\n","Time: Fri Apr  1 00:11:57 2022\n","Step: 1488\t Epoch: [87][60/204]\tTime 0.500 (0.548)\tData 0.003 (0.046)\tLoss 0.0857 (0.1741)\tSoftmaxLoss 0.0357 (0.1235)\tRankLoss 0.0500 (0.0506)\tPrec@1 99.219 (95.556)\n","Time: Fri Apr  1 00:12:08 2022\n","Step: 1508\t Epoch: [87][80/204]\tTime 0.501 (0.546)\tData 0.002 (0.043)\tLoss 0.0869 (0.1689)\tSoftmaxLoss 0.0362 (0.1184)\tRankLoss 0.0506 (0.0505)\tPrec@1 100.000 (95.467)\n","Time: Fri Apr  1 00:12:19 2022\n","Step: 1528\t Epoch: [87][100/204]\tTime 0.510 (0.540)\tData 0.002 (0.038)\tLoss 0.0597 (0.1614)\tSoftmaxLoss 0.0095 (0.1110)\tRankLoss 0.0503 (0.0504)\tPrec@1 100.000 (95.753)\n","Time: Fri Apr  1 00:12:29 2022\n","Step: 1548\t Epoch: [87][120/204]\tTime 0.503 (0.538)\tData 0.003 (0.036)\tLoss 0.0610 (0.1569)\tSoftmaxLoss 0.0096 (0.1065)\tRankLoss 0.0514 (0.0504)\tPrec@1 100.000 (95.952)\n","Time: Fri Apr  1 00:12:39 2022\n","Step: 1568\t Epoch: [87][140/204]\tTime 0.495 (0.534)\tData 0.003 (0.032)\tLoss 0.0653 (0.1494)\tSoftmaxLoss 0.0145 (0.0990)\tRankLoss 0.0508 (0.0504)\tPrec@1 100.000 (96.243)\n","Time: Fri Apr  1 00:12:50 2022\n","Step: 1588\t Epoch: [87][160/204]\tTime 0.508 (0.534)\tData 0.003 (0.032)\tLoss 0.0665 (0.1444)\tSoftmaxLoss 0.0156 (0.0940)\tRankLoss 0.0509 (0.0504)\tPrec@1 100.000 (96.370)\n","Time: Fri Apr  1 00:13:00 2022\n","Step: 1608\t Epoch: [87][180/204]\tTime 0.505 (0.532)\tData 0.002 (0.030)\tLoss 0.0572 (0.1398)\tSoftmaxLoss 0.0072 (0.0894)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (96.577)\n","Time: Fri Apr  1 00:13:10 2022\n","Step: 1628\t Epoch: [87][200/204]\tTime 0.495 (0.530)\tData 0.001 (0.028)\tLoss 0.0793 (0.1341)\tSoftmaxLoss 0.0270 (0.0836)\tRankLoss 0.0523 (0.0504)\tPrec@1 100.000 (96.797)\n","Time: Fri Apr  1 00:13:14 2022\n","Test: [0/14]\tTime 1.703 (1.703)\tSoftmaxLoss 0.5182 (0.5182)\tPrec@1 84.375 (84.375)\n"," * Prec@1 88.143\n","Time: Fri Apr  1 00:13:31 2022\n","Step: 1632\t Epoch: [88][0/204]\tTime 2.648 (2.648)\tData 2.129 (2.129)\tLoss 0.0629 (0.0629)\tSoftmaxLoss 0.0116 (0.0116)\tRankLoss 0.0513 (0.0513)\tPrec@1 100.000 (100.000)\n","Time: Fri Apr  1 00:13:43 2022\n","Step: 1652\t Epoch: [88][20/204]\tTime 0.649 (0.678)\tData 0.137 (0.172)\tLoss 0.0536 (0.0912)\tSoftmaxLoss 0.0034 (0.0408)\tRankLoss 0.0502 (0.0504)\tPrec@1 100.000 (99.256)\n","Time: Fri Apr  1 00:13:53 2022\n","Step: 1672\t Epoch: [88][40/204]\tTime 0.498 (0.609)\tData 0.002 (0.105)\tLoss 0.0574 (0.0864)\tSoftmaxLoss 0.0073 (0.0361)\tRankLoss 0.0501 (0.0503)\tPrec@1 100.000 (99.104)\n","Time: Fri Apr  1 00:14:04 2022\n","Step: 1692\t Epoch: [88][60/204]\tTime 0.508 (0.587)\tData 0.010 (0.084)\tLoss 0.0511 (0.0776)\tSoftmaxLoss 0.0007 (0.0272)\tRankLoss 0.0504 (0.0504)\tPrec@1 100.000 (99.398)\n","Time: Fri Apr  1 00:14:15 2022\n","Step: 1712\t Epoch: [88][80/204]\tTime 0.505 (0.571)\tData 0.003 (0.069)\tLoss 0.0948 (0.0832)\tSoftmaxLoss 0.0448 (0.0328)\tRankLoss 0.0500 (0.0504)\tPrec@1 99.219 (99.257)\n","Time: Fri Apr  1 00:14:25 2022\n","Step: 1732\t Epoch: [88][100/204]\tTime 0.502 (0.565)\tData 0.003 (0.063)\tLoss 0.0523 (0.0814)\tSoftmaxLoss 0.0023 (0.0311)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (99.250)\n","Time: Fri Apr  1 00:14:36 2022\n","Step: 1752\t Epoch: [88][120/204]\tTime 0.512 (0.560)\tData 0.002 (0.058)\tLoss 0.0747 (0.0807)\tSoftmaxLoss 0.0238 (0.0303)\tRankLoss 0.0509 (0.0504)\tPrec@1 100.000 (99.212)\n","Time: Fri Apr  1 00:14:47 2022\n","Step: 1772\t Epoch: [88][140/204]\tTime 0.500 (0.558)\tData 0.002 (0.056)\tLoss 0.0547 (0.0782)\tSoftmaxLoss 0.0033 (0.0278)\tRankLoss 0.0515 (0.0504)\tPrec@1 100.000 (99.285)\n","Time: Fri Apr  1 00:14:57 2022\n","Step: 1792\t Epoch: [88][160/204]\tTime 0.498 (0.552)\tData 0.002 (0.051)\tLoss 0.0577 (0.0791)\tSoftmaxLoss 0.0069 (0.0287)\tRankLoss 0.0507 (0.0504)\tPrec@1 100.000 (99.224)\n","Time: Fri Apr  1 00:15:07 2022\n","Step: 1812\t Epoch: [88][180/204]\tTime 0.506 (0.547)\tData 0.003 (0.046)\tLoss 0.0540 (0.0796)\tSoftmaxLoss 0.0035 (0.0291)\tRankLoss 0.0505 (0.0504)\tPrec@1 100.000 (99.150)\n","Time: Fri Apr  1 00:15:18 2022\n","Step: 1832\t Epoch: [88][200/204]\tTime 0.497 (0.544)\tData 0.002 (0.043)\tLoss 0.0975 (0.0789)\tSoftmaxLoss 0.0473 (0.0284)\tRankLoss 0.0503 (0.0505)\tPrec@1 100.000 (99.180)\n","Time: Fri Apr  1 00:15:21 2022\n","Test: [0/14]\tTime 1.658 (1.658)\tSoftmaxLoss 0.4365 (0.4365)\tPrec@1 90.625 (90.625)\n"," * Prec@1 87.248\n","Time: Fri Apr  1 00:15:37 2022\n","Step: 1836\t Epoch: [89][0/204]\tTime 1.501 (1.501)\tData 0.985 (0.985)\tLoss 0.0641 (0.0641)\tSoftmaxLoss 0.0141 (0.0141)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n","Time: Fri Apr  1 00:15:47 2022\n","Step: 1856\t Epoch: [89][20/204]\tTime 0.680 (0.572)\tData 0.175 (0.067)\tLoss 0.0575 (0.0814)\tSoftmaxLoss 0.0066 (0.0310)\tRankLoss 0.0510 (0.0504)\tPrec@1 100.000 (99.070)\n","Time: Fri Apr  1 00:15:58 2022\n","Step: 1876\t Epoch: [89][40/204]\tTime 0.495 (0.560)\tData 0.002 (0.056)\tLoss 0.0528 (0.0833)\tSoftmaxLoss 0.0007 (0.0327)\tRankLoss 0.0521 (0.0506)\tPrec@1 100.000 (98.895)\n","Time: Fri Apr  1 00:16:09 2022\n","Step: 1896\t Epoch: [89][60/204]\tTime 0.509 (0.550)\tData 0.003 (0.046)\tLoss 0.0531 (0.0795)\tSoftmaxLoss 0.0029 (0.0287)\tRankLoss 0.0502 (0.0508)\tPrec@1 100.000 (98.975)\n","Time: Fri Apr  1 00:16:20 2022\n","Step: 1916\t Epoch: [89][80/204]\tTime 0.626 (0.546)\tData 0.129 (0.043)\tLoss 0.0502 (0.0778)\tSoftmaxLoss 0.0000 (0.0270)\tRankLoss 0.0502 (0.0507)\tPrec@1 100.000 (98.968)\n","Time: Fri Apr  1 00:16:30 2022\n","Step: 1936\t Epoch: [89][100/204]\tTime 0.504 (0.538)\tData 0.003 (0.035)\tLoss 0.0519 (0.0793)\tSoftmaxLoss 0.0016 (0.0286)\tRankLoss 0.0503 (0.0507)\tPrec@1 100.000 (98.925)\n","Time: Fri Apr  1 00:16:40 2022\n","Step: 1956\t Epoch: [89][120/204]\tTime 0.500 (0.536)\tData 0.002 (0.033)\tLoss 0.0719 (0.0836)\tSoftmaxLoss 0.0205 (0.0329)\tRankLoss 0.0514 (0.0507)\tPrec@1 100.000 (98.754)\n","Time: Fri Apr  1 00:16:50 2022\n","Step: 1976\t Epoch: [89][140/204]\tTime 0.504 (0.532)\tData 0.003 (0.029)\tLoss 0.0665 (0.0935)\tSoftmaxLoss 0.0162 (0.0428)\tRankLoss 0.0503 (0.0507)\tPrec@1 100.000 (98.421)\n","Time: Fri Apr  1 00:17:01 2022\n","Step: 1996\t Epoch: [89][160/204]\tTime 0.497 (0.529)\tData 0.003 (0.027)\tLoss 1.2947 (0.1185)\tSoftmaxLoss 1.2435 (0.0678)\tRankLoss 0.0512 (0.0507)\tPrec@1 78.906 (97.938)\n","Time: Fri Apr  1 00:17:12 2022\n","Step: 2016\t Epoch: [89][180/204]\tTime 0.514 (0.533)\tData 0.002 (0.030)\tLoss 0.2399 (0.1232)\tSoftmaxLoss 0.1898 (0.0726)\tRankLoss 0.0500 (0.0507)\tPrec@1 85.156 (97.669)\n","Time: Fri Apr  1 00:17:22 2022\n","Step: 2036\t Epoch: [89][200/204]\tTime 0.504 (0.530)\tData 0.003 (0.028)\tLoss 0.0749 (0.1244)\tSoftmaxLoss 0.0240 (0.0737)\tRankLoss 0.0509 (0.0507)\tPrec@1 100.000 (97.648)\n","Time: Fri Apr  1 00:17:25 2022\n","Test: [0/14]\tTime 1.658 (1.658)\tSoftmaxLoss 0.4937 (0.4937)\tPrec@1 87.500 (87.500)\n"," * Prec@1 86.801\n","Time: Fri Apr  1 00:17:41 2022\n","Step: 2040\t Epoch: [90][0/204]\tTime 1.519 (1.519)\tData 1.000 (1.000)\tLoss 0.0890 (0.0890)\tSoftmaxLoss 0.0388 (0.0388)\tRankLoss 0.0503 (0.0503)\tPrec@1 100.000 (100.000)\n","Time: Fri Apr  1 00:17:52 2022\n","Step: 2060\t Epoch: [90][20/204]\tTime 0.591 (0.564)\tData 0.095 (0.061)\tLoss 0.1756 (0.1122)\tSoftmaxLoss 0.1256 (0.0618)\tRankLoss 0.0500 (0.0503)\tPrec@1 89.844 (97.879)\n","Time: Fri Apr  1 00:18:03 2022\n","Step: 2080\t Epoch: [90][40/204]\tTime 0.497 (0.573)\tData 0.002 (0.069)\tLoss 0.0577 (0.0999)\tSoftmaxLoss 0.0076 (0.0497)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (98.285)\n","Time: Fri Apr  1 00:18:15 2022\n","Step: 2100\t Epoch: [90][60/204]\tTime 0.503 (0.576)\tData 0.002 (0.073)\tLoss 0.4034 (0.1203)\tSoftmaxLoss 0.3532 (0.0700)\tRankLoss 0.0502 (0.0503)\tPrec@1 84.375 (97.631)\n","Time: Fri Apr  1 00:18:25 2022\n","Step: 2120\t Epoch: [90][80/204]\tTime 0.503 (0.559)\tData 0.002 (0.057)\tLoss 0.2042 (0.1203)\tSoftmaxLoss 0.1541 (0.0700)\tRankLoss 0.0501 (0.0503)\tPrec@1 92.969 (97.425)\n","Time: Fri Apr  1 00:18:35 2022\n","Step: 2140\t Epoch: [90][100/204]\tTime 0.509 (0.551)\tData 0.002 (0.049)\tLoss 0.0563 (0.1160)\tSoftmaxLoss 0.0063 (0.0658)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (97.679)\n","Time: Fri Apr  1 00:18:45 2022\n","Step: 2160\t Epoch: [90][120/204]\tTime 0.513 (0.543)\tData 0.004 (0.041)\tLoss 0.3931 (0.1199)\tSoftmaxLoss 0.3422 (0.0695)\tRankLoss 0.0509 (0.0503)\tPrec@1 85.938 (97.450)\n","Time: Fri Apr  1 00:18:56 2022\n","Step: 2180\t Epoch: [90][140/204]\tTime 0.502 (0.539)\tData 0.003 (0.037)\tLoss 0.0547 (0.1248)\tSoftmaxLoss 0.0046 (0.0744)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (97.207)\n","Time: Fri Apr  1 00:19:06 2022\n","Step: 2200\t Epoch: [90][160/204]\tTime 0.512 (0.539)\tData 0.003 (0.037)\tLoss 0.5121 (0.1504)\tSoftmaxLoss 0.4620 (0.1001)\tRankLoss 0.0502 (0.0503)\tPrec@1 84.375 (96.642)\n","Time: Fri Apr  1 00:19:17 2022\n","Step: 2220\t Epoch: [90][180/204]\tTime 0.508 (0.535)\tData 0.004 (0.033)\tLoss 0.1632 (0.1717)\tSoftmaxLoss 0.1124 (0.1214)\tRankLoss 0.0508 (0.0503)\tPrec@1 94.531 (96.046)\n","Time: Fri Apr  1 00:19:27 2022\n","Step: 2240\t Epoch: [90][200/204]\tTime 0.507 (0.534)\tData 0.003 (0.033)\tLoss 0.1458 (0.1735)\tSoftmaxLoss 0.0957 (0.1232)\tRankLoss 0.0501 (0.0503)\tPrec@1 100.000 (95.857)\n","Time: Fri Apr  1 00:19:30 2022\n","Test: [0/14]\tTime 1.683 (1.683)\tSoftmaxLoss 0.4305 (0.4305)\tPrec@1 84.375 (84.375)\n"," * Prec@1 87.248\n","Time: Fri Apr  1 00:19:46 2022\n","Step: 2244\t Epoch: [91][0/204]\tTime 1.460 (1.460)\tData 0.953 (0.953)\tLoss 0.0976 (0.0976)\tSoftmaxLoss 0.0475 (0.0475)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n","Time: Fri Apr  1 00:19:58 2022\n","Step: 2264\t Epoch: [91][20/204]\tTime 0.525 (0.626)\tData 0.007 (0.122)\tLoss 0.1228 (0.1298)\tSoftmaxLoss 0.0720 (0.0796)\tRankLoss 0.0508 (0.0502)\tPrec@1 100.000 (97.061)\n","Time: Fri Apr  1 00:20:09 2022\n","Step: 2284\t Epoch: [91][40/204]\tTime 0.506 (0.591)\tData 0.003 (0.088)\tLoss 0.1920 (0.1355)\tSoftmaxLoss 0.1419 (0.0854)\tRankLoss 0.0500 (0.0502)\tPrec@1 92.188 (96.589)\n","Time: Fri Apr  1 00:20:19 2022\n","Step: 2304\t Epoch: [91][60/204]\tTime 0.506 (0.565)\tData 0.003 (0.063)\tLoss 0.1044 (0.1442)\tSoftmaxLoss 0.0544 (0.0940)\tRankLoss 0.0501 (0.0502)\tPrec@1 98.438 (96.657)\n","Time: Fri Apr  1 00:20:30 2022\n","Step: 2324\t Epoch: [91][80/204]\tTime 0.512 (0.556)\tData 0.003 (0.053)\tLoss 0.0675 (0.1367)\tSoftmaxLoss 0.0175 (0.0865)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (96.807)\n","Time: Fri Apr  1 00:20:40 2022\n","Step: 2344\t Epoch: [91][100/204]\tTime 0.498 (0.549)\tData 0.002 (0.047)\tLoss 0.0575 (0.1262)\tSoftmaxLoss 0.0074 (0.0760)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (97.184)\n","Time: Fri Apr  1 00:20:51 2022\n","Step: 2364\t Epoch: [91][120/204]\tTime 0.657 (0.545)\tData 0.156 (0.043)\tLoss 0.0661 (0.1213)\tSoftmaxLoss 0.0160 (0.0711)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (97.372)\n","Time: Fri Apr  1 00:21:02 2022\n","Step: 2384\t Epoch: [91][140/204]\tTime 0.866 (0.545)\tData 0.354 (0.043)\tLoss 0.0506 (0.1160)\tSoftmaxLoss 0.0006 (0.0658)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (97.645)\n","Time: Fri Apr  1 00:21:12 2022\n","Step: 2404\t Epoch: [91][160/204]\tTime 0.508 (0.542)\tData 0.003 (0.040)\tLoss 0.0608 (0.1104)\tSoftmaxLoss 0.0108 (0.0602)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (97.923)\n","Time: Fri Apr  1 00:21:22 2022\n","Step: 2424\t Epoch: [91][180/204]\tTime 0.503 (0.540)\tData 0.003 (0.038)\tLoss 0.0522 (0.1060)\tSoftmaxLoss 0.0022 (0.0559)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (98.135)\n","Time: Fri Apr  1 00:21:33 2022\n","Step: 2444\t Epoch: [91][200/204]\tTime 0.495 (0.539)\tData 0.002 (0.037)\tLoss 0.0523 (0.1020)\tSoftmaxLoss 0.0011 (0.0519)\tRankLoss 0.0512 (0.0501)\tPrec@1 100.000 (98.243)\n","Time: Fri Apr  1 00:21:36 2022\n","Test: [0/14]\tTime 1.667 (1.667)\tSoftmaxLoss 0.3039 (0.3039)\tPrec@1 90.625 (90.625)\n"," * Prec@1 89.262\n","Time: Fri Apr  1 00:21:52 2022\n","Step: 2448\t Epoch: [92][0/204]\tTime 1.440 (1.440)\tData 0.927 (0.927)\tLoss 0.0899 (0.0899)\tSoftmaxLoss 0.0384 (0.0384)\tRankLoss 0.0515 (0.0515)\tPrec@1 100.000 (100.000)\n","Time: Fri Apr  1 00:22:03 2022\n","Step: 2468\t Epoch: [92][20/204]\tTime 0.507 (0.600)\tData 0.003 (0.095)\tLoss 0.0515 (0.0707)\tSoftmaxLoss 0.0015 (0.0204)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (99.330)\n","Time: Fri Apr  1 00:22:15 2022\n","Step: 2488\t Epoch: [92][40/204]\tTime 0.501 (0.582)\tData 0.002 (0.079)\tLoss 0.1152 (0.0713)\tSoftmaxLoss 0.0652 (0.0210)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (99.352)\n","Time: Fri Apr  1 00:22:25 2022\n","Step: 2508\t Epoch: [92][60/204]\tTime 0.502 (0.559)\tData 0.003 (0.056)\tLoss 0.1728 (0.0746)\tSoftmaxLoss 0.1228 (0.0243)\tRankLoss 0.0500 (0.0502)\tPrec@1 92.188 (99.180)\n","Time: Fri Apr  1 00:22:35 2022\n","Step: 2528\t Epoch: [92][80/204]\tTime 0.698 (0.548)\tData 0.200 (0.046)\tLoss 0.0565 (0.0753)\tSoftmaxLoss 0.0065 (0.0250)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (99.228)\n","Time: Fri Apr  1 00:22:46 2022\n","Step: 2548\t Epoch: [92][100/204]\tTime 0.499 (0.544)\tData 0.002 (0.043)\tLoss 0.0603 (0.0757)\tSoftmaxLoss 0.0084 (0.0253)\tRankLoss 0.0519 (0.0504)\tPrec@1 100.000 (99.219)\n","Time: Fri Apr  1 00:22:57 2022\n","Step: 2568\t Epoch: [92][120/204]\tTime 0.498 (0.545)\tData 0.002 (0.044)\tLoss 0.0559 (0.0749)\tSoftmaxLoss 0.0029 (0.0246)\tRankLoss 0.0530 (0.0504)\tPrec@1 100.000 (99.322)\n","Time: Fri Apr  1 00:23:07 2022\n","Step: 2588\t Epoch: [92][140/204]\tTime 0.508 (0.541)\tData 0.002 (0.040)\tLoss 0.0709 (0.0726)\tSoftmaxLoss 0.0203 (0.0222)\tRankLoss 0.0506 (0.0503)\tPrec@1 100.000 (99.407)\n","Time: Fri Apr  1 00:23:17 2022\n","Step: 2608\t Epoch: [92][160/204]\tTime 0.504 (0.537)\tData 0.003 (0.036)\tLoss 0.0532 (0.0727)\tSoftmaxLoss 0.0027 (0.0224)\tRankLoss 0.0505 (0.0503)\tPrec@1 100.000 (99.359)\n","Time: Fri Apr  1 00:23:28 2022\n","Step: 2628\t Epoch: [92][180/204]\tTime 0.500 (0.538)\tData 0.002 (0.037)\tLoss 0.0539 (0.0724)\tSoftmaxLoss 0.0039 (0.0221)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (99.357)\n","Time: Fri Apr  1 00:23:39 2022\n","Step: 2648\t Epoch: [92][200/204]\tTime 0.776 (0.539)\tData 0.272 (0.038)\tLoss 0.0561 (0.0719)\tSoftmaxLoss 0.0058 (0.0215)\tRankLoss 0.0503 (0.0504)\tPrec@1 100.000 (99.366)\n","Time: Fri Apr  1 00:23:42 2022\n","Test: [0/14]\tTime 1.758 (1.758)\tSoftmaxLoss 0.3062 (0.3062)\tPrec@1 87.500 (87.500)\n"," * Prec@1 87.696\n","Time: Fri Apr  1 00:23:59 2022\n","Step: 2652\t Epoch: [93][0/204]\tTime 1.769 (1.769)\tData 1.241 (1.241)\tLoss 0.1259 (0.1259)\tSoftmaxLoss 0.0759 (0.0759)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n","Time: Fri Apr  1 00:24:10 2022\n","Step: 2672\t Epoch: [93][20/204]\tTime 0.507 (0.606)\tData 0.002 (0.103)\tLoss 0.0549 (0.0834)\tSoftmaxLoss 0.0046 (0.0331)\tRankLoss 0.0503 (0.0503)\tPrec@1 100.000 (98.661)\n","Time: Fri Apr  1 00:24:21 2022\n","Step: 2692\t Epoch: [93][40/204]\tTime 0.501 (0.594)\tData 0.003 (0.093)\tLoss 0.0501 (0.0703)\tSoftmaxLoss 0.0001 (0.0199)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (99.295)\n","Time: Fri Apr  1 00:24:32 2022\n","Step: 2712\t Epoch: [93][60/204]\tTime 0.504 (0.567)\tData 0.003 (0.065)\tLoss 0.0974 (0.0690)\tSoftmaxLoss 0.0474 (0.0185)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (99.334)\n","Time: Fri Apr  1 00:24:42 2022\n","Step: 2732\t Epoch: [93][80/204]\tTime 0.506 (0.555)\tData 0.003 (0.053)\tLoss 0.0506 (0.0717)\tSoftmaxLoss 0.0004 (0.0213)\tRankLoss 0.0502 (0.0504)\tPrec@1 100.000 (99.190)\n","Time: Fri Apr  1 00:24:52 2022\n","Step: 2752\t Epoch: [93][100/204]\tTime 0.506 (0.547)\tData 0.003 (0.045)\tLoss 0.0580 (0.0704)\tSoftmaxLoss 0.0056 (0.0199)\tRankLoss 0.0524 (0.0506)\tPrec@1 100.000 (99.250)\n","Time: Fri Apr  1 00:25:02 2022\n","Step: 2772\t Epoch: [93][120/204]\tTime 0.500 (0.540)\tData 0.002 (0.038)\tLoss 0.0677 (0.0684)\tSoftmaxLoss 0.0177 (0.0178)\tRankLoss 0.0500 (0.0506)\tPrec@1 100.000 (99.367)\n","Time: Fri Apr  1 00:25:13 2022\n","Step: 2792\t Epoch: [93][140/204]\tTime 0.511 (0.538)\tData 0.003 (0.036)\tLoss 0.0576 (0.0665)\tSoftmaxLoss 0.0076 (0.0159)\tRankLoss 0.0500 (0.0506)\tPrec@1 100.000 (99.457)\n","Time: Fri Apr  1 00:25:23 2022\n","Step: 2812\t Epoch: [93][160/204]\tTime 0.497 (0.535)\tData 0.002 (0.033)\tLoss 0.0513 (0.0660)\tSoftmaxLoss 0.0002 (0.0154)\tRankLoss 0.0511 (0.0506)\tPrec@1 100.000 (99.461)\n","Time: Fri Apr  1 00:25:33 2022\n","Step: 2832\t Epoch: [93][180/204]\tTime 0.508 (0.532)\tData 0.003 (0.031)\tLoss 0.0674 (0.0656)\tSoftmaxLoss 0.0174 (0.0150)\tRankLoss 0.0500 (0.0506)\tPrec@1 100.000 (99.486)\n","Time: Fri Apr  1 00:25:44 2022\n","Step: 2852\t Epoch: [93][200/204]\tTime 0.500 (0.531)\tData 0.002 (0.029)\tLoss 0.0515 (0.0643)\tSoftmaxLoss 0.0015 (0.0137)\tRankLoss 0.0500 (0.0506)\tPrec@1 100.000 (99.537)\n","Time: Fri Apr  1 00:25:47 2022\n","Test: [0/14]\tTime 1.697 (1.697)\tSoftmaxLoss 0.1705 (0.1705)\tPrec@1 93.750 (93.750)\n"," * Prec@1 88.143\n","Time: Fri Apr  1 00:26:03 2022\n","Step: 2856\t Epoch: [94][0/204]\tTime 2.101 (2.101)\tData 1.583 (1.583)\tLoss 0.0540 (0.0540)\tSoftmaxLoss 0.0040 (0.0040)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n","Time: Fri Apr  1 00:26:15 2022\n","Step: 2876\t Epoch: [94][20/204]\tTime 0.995 (0.646)\tData 0.485 (0.141)\tLoss 0.0504 (0.0531)\tSoftmaxLoss 0.0001 (0.0028)\tRankLoss 0.0502 (0.0503)\tPrec@1 100.000 (100.000)\n","Time: Fri Apr  1 00:26:26 2022\n","Step: 2896\t Epoch: [94][40/204]\tTime 0.511 (0.591)\tData 0.003 (0.087)\tLoss 0.0586 (0.0557)\tSoftmaxLoss 0.0061 (0.0051)\tRankLoss 0.0525 (0.0505)\tPrec@1 100.000 (99.829)\n","Time: Fri Apr  1 00:26:36 2022\n","Step: 2916\t Epoch: [94][60/204]\tTime 0.499 (0.569)\tData 0.002 (0.066)\tLoss 0.0508 (0.0551)\tSoftmaxLoss 0.0008 (0.0047)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (99.885)\n","Time: Fri Apr  1 00:26:47 2022\n","Step: 2936\t Epoch: [94][80/204]\tTime 0.497 (0.560)\tData 0.002 (0.058)\tLoss 0.0672 (0.0562)\tSoftmaxLoss 0.0166 (0.0057)\tRankLoss 0.0506 (0.0505)\tPrec@1 100.000 (99.894)\n","Time: Fri Apr  1 00:26:57 2022\n","Step: 2956\t Epoch: [94][100/204]\tTime 0.501 (0.555)\tData 0.003 (0.052)\tLoss 0.0512 (0.0596)\tSoftmaxLoss 0.0012 (0.0091)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (99.760)\n","Time: Fri Apr  1 00:27:07 2022\n","Step: 2976\t Epoch: [94][120/204]\tTime 0.507 (0.547)\tData 0.002 (0.044)\tLoss 0.0859 (0.0607)\tSoftmaxLoss 0.0358 (0.0102)\tRankLoss 0.0501 (0.0505)\tPrec@1 100.000 (99.729)\n","Time: Fri Apr  1 00:27:18 2022\n","Step: 2996\t Epoch: [94][140/204]\tTime 0.503 (0.542)\tData 0.003 (0.040)\tLoss 0.0526 (0.0600)\tSoftmaxLoss 0.0017 (0.0095)\tRankLoss 0.0509 (0.0505)\tPrec@1 100.000 (99.767)\n","Time: Fri Apr  1 00:27:29 2022\n","Step: 3016\t Epoch: [94][160/204]\tTime 0.504 (0.542)\tData 0.002 (0.040)\tLoss 0.0502 (0.0593)\tSoftmaxLoss 0.0002 (0.0087)\tRankLoss 0.0500 (0.0506)\tPrec@1 100.000 (99.796)\n","Time: Fri Apr  1 00:27:39 2022\n","Step: 3036\t Epoch: [94][180/204]\tTime 0.505 (0.541)\tData 0.002 (0.039)\tLoss 0.0518 (0.0596)\tSoftmaxLoss 0.0002 (0.0091)\tRankLoss 0.0516 (0.0505)\tPrec@1 100.000 (99.754)\n","Time: Fri Apr  1 00:27:50 2022\n","Step: 3056\t Epoch: [94][200/204]\tTime 0.502 (0.540)\tData 0.001 (0.039)\tLoss 0.0520 (0.0588)\tSoftmaxLoss 0.0011 (0.0083)\tRankLoss 0.0509 (0.0505)\tPrec@1 100.000 (99.778)\n","Time: Fri Apr  1 00:27:53 2022\n","Test: [0/14]\tTime 1.658 (1.658)\tSoftmaxLoss 0.5538 (0.5538)\tPrec@1 87.500 (87.500)\n"," * Prec@1 88.143\n","Time: Fri Apr  1 00:28:10 2022\n","Step: 3060\t Epoch: [95][0/204]\tTime 1.549 (1.549)\tData 1.030 (1.030)\tLoss 0.0525 (0.0525)\tSoftmaxLoss 0.0023 (0.0023)\tRankLoss 0.0502 (0.0502)\tPrec@1 100.000 (100.000)\n","Time: Fri Apr  1 00:28:20 2022\n","Step: 3080\t Epoch: [95][20/204]\tTime 0.577 (0.571)\tData 0.075 (0.068)\tLoss 0.0507 (0.0627)\tSoftmaxLoss 0.0007 (0.0121)\tRankLoss 0.0500 (0.0506)\tPrec@1 100.000 (99.442)\n","Time: Fri Apr  1 00:28:32 2022\n","Step: 3100\t Epoch: [95][40/204]\tTime 0.505 (0.576)\tData 0.003 (0.072)\tLoss 0.0537 (0.0739)\tSoftmaxLoss 0.0036 (0.0234)\tRankLoss 0.0501 (0.0505)\tPrec@1 100.000 (99.143)\n","Time: Fri Apr  1 00:28:42 2022\n","Step: 3120\t Epoch: [95][60/204]\tTime 0.501 (0.559)\tData 0.003 (0.056)\tLoss 0.1283 (0.0857)\tSoftmaxLoss 0.0775 (0.0351)\tRankLoss 0.0508 (0.0506)\tPrec@1 99.219 (98.809)\n","Time: Fri Apr  1 00:28:53 2022\n","Step: 3140\t Epoch: [95][80/204]\tTime 0.628 (0.553)\tData 0.127 (0.050)\tLoss 0.1726 (0.0843)\tSoftmaxLoss 0.1226 (0.0337)\tRankLoss 0.0500 (0.0505)\tPrec@1 92.969 (98.814)\n","Time: Fri Apr  1 00:29:03 2022\n","Step: 3160\t Epoch: [95][100/204]\tTime 0.503 (0.544)\tData 0.003 (0.041)\tLoss 0.3041 (0.0908)\tSoftmaxLoss 0.2516 (0.0402)\tRankLoss 0.0524 (0.0506)\tPrec@1 85.156 (98.391)\n","Time: Fri Apr  1 00:29:13 2022\n","Step: 3180\t Epoch: [95][120/204]\tTime 0.543 (0.538)\tData 0.031 (0.036)\tLoss 0.0937 (0.0948)\tSoftmaxLoss 0.0429 (0.0441)\tRankLoss 0.0508 (0.0507)\tPrec@1 99.219 (98.237)\n","Time: Fri Apr  1 00:29:25 2022\n","Step: 3200\t Epoch: [95][140/204]\tTime 0.496 (0.540)\tData 0.002 (0.038)\tLoss 0.0501 (0.1031)\tSoftmaxLoss 0.0001 (0.0524)\tRankLoss 0.0500 (0.0507)\tPrec@1 100.000 (98.127)\n","Time: Fri Apr  1 00:29:35 2022\n","Step: 3220\t Epoch: [95][160/204]\tTime 0.507 (0.537)\tData 0.002 (0.035)\tLoss 0.1059 (0.1075)\tSoftmaxLoss 0.0551 (0.0568)\tRankLoss 0.0507 (0.0507)\tPrec@1 99.219 (97.986)\n","Time: Fri Apr  1 00:29:45 2022\n","Step: 3240\t Epoch: [95][180/204]\tTime 0.511 (0.536)\tData 0.003 (0.033)\tLoss 0.0763 (0.1236)\tSoftmaxLoss 0.0245 (0.0730)\tRankLoss 0.0518 (0.0506)\tPrec@1 100.000 (97.328)\n","Time: Fri Apr  1 00:29:56 2022\n","Step: 3260\t Epoch: [95][200/204]\tTime 0.506 (0.535)\tData 0.003 (0.033)\tLoss 0.0546 (0.1289)\tSoftmaxLoss 0.0027 (0.0783)\tRankLoss 0.0519 (0.0506)\tPrec@1 100.000 (97.093)\n","Time: Fri Apr  1 00:29:59 2022\n","Test: [0/14]\tTime 1.658 (1.658)\tSoftmaxLoss 0.1456 (0.1456)\tPrec@1 93.750 (93.750)\n"," * Prec@1 86.577\n","Time: Fri Apr  1 00:30:15 2022\n","Step: 3264\t Epoch: [96][0/204]\tTime 1.419 (1.419)\tData 0.899 (0.899)\tLoss 0.2910 (0.2910)\tSoftmaxLoss 0.2410 (0.2410)\tRankLoss 0.0500 (0.0500)\tPrec@1 87.500 (87.500)\n","Time: Fri Apr  1 00:30:27 2022\n","Step: 3284\t Epoch: [96][20/204]\tTime 0.942 (0.621)\tData 0.430 (0.116)\tLoss 0.1162 (0.2817)\tSoftmaxLoss 0.0661 (0.2315)\tRankLoss 0.0501 (0.0501)\tPrec@1 100.000 (93.304)\n","Time: Fri Apr  1 00:30:38 2022\n","Step: 3304\t Epoch: [96][40/204]\tTime 0.505 (0.599)\tData 0.003 (0.095)\tLoss 0.0673 (0.2237)\tSoftmaxLoss 0.0166 (0.1736)\tRankLoss 0.0507 (0.0502)\tPrec@1 100.000 (94.703)\n","Time: Fri Apr  1 00:30:48 2022\n","Step: 3324\t Epoch: [96][60/204]\tTime 0.499 (0.570)\tData 0.002 (0.067)\tLoss 0.0985 (0.2074)\tSoftmaxLoss 0.0485 (0.1573)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (94.954)\n","Time: Fri Apr  1 00:30:58 2022\n","Step: 3344\t Epoch: [96][80/204]\tTime 0.501 (0.554)\tData 0.003 (0.052)\tLoss 0.0561 (0.1872)\tSoftmaxLoss 0.0061 (0.1371)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (95.457)\n","Time: Fri Apr  1 00:31:09 2022\n","Step: 3364\t Epoch: [96][100/204]\tTime 0.506 (0.547)\tData 0.003 (0.045)\tLoss 0.1090 (0.1817)\tSoftmaxLoss 0.0580 (0.1316)\tRankLoss 0.0510 (0.0502)\tPrec@1 99.219 (95.575)\n","Time: Fri Apr  1 00:31:20 2022\n","Step: 3384\t Epoch: [96][120/204]\tTime 0.504 (0.547)\tData 0.003 (0.045)\tLoss 0.0576 (0.1686)\tSoftmaxLoss 0.0076 (0.1184)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (96.010)\n","Time: Fri Apr  1 00:31:30 2022\n","Step: 3404\t Epoch: [96][140/204]\tTime 0.501 (0.541)\tData 0.002 (0.039)\tLoss 0.0948 (0.1606)\tSoftmaxLoss 0.0448 (0.1105)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (96.271)\n","Time: Fri Apr  1 00:31:40 2022\n","Step: 3424\t Epoch: [96][160/204]\tTime 0.505 (0.539)\tData 0.002 (0.038)\tLoss 0.0528 (0.1499)\tSoftmaxLoss 0.0028 (0.0998)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (96.661)\n","Time: Fri Apr  1 00:31:51 2022\n","Step: 3444\t Epoch: [96][180/204]\tTime 0.659 (0.539)\tData 0.157 (0.037)\tLoss 0.1847 (0.1452)\tSoftmaxLoss 0.1347 (0.0950)\tRankLoss 0.0500 (0.0501)\tPrec@1 91.406 (96.702)\n","Time: Fri Apr  1 00:32:02 2022\n","Step: 3464\t Epoch: [96][200/204]\tTime 0.514 (0.538)\tData 0.012 (0.036)\tLoss 0.1040 (0.1444)\tSoftmaxLoss 0.0540 (0.0943)\tRankLoss 0.0501 (0.0501)\tPrec@1 98.438 (96.661)\n","Time: Fri Apr  1 00:32:05 2022\n","Test: [0/14]\tTime 1.660 (1.660)\tSoftmaxLoss 0.1681 (0.1681)\tPrec@1 93.750 (93.750)\n"," * Prec@1 87.248\n","Time: Fri Apr  1 00:32:21 2022\n","Step: 3468\t Epoch: [97][0/204]\tTime 1.231 (1.231)\tData 0.717 (0.717)\tLoss 0.3048 (0.3048)\tSoftmaxLoss 0.2548 (0.2548)\tRankLoss 0.0500 (0.0500)\tPrec@1 87.500 (87.500)\n","Time: Fri Apr  1 00:32:32 2022\n","Step: 3488\t Epoch: [97][20/204]\tTime 1.116 (0.603)\tData 0.608 (0.098)\tLoss 0.0591 (0.0877)\tSoftmaxLoss 0.0080 (0.0375)\tRankLoss 0.0511 (0.0502)\tPrec@1 100.000 (98.698)\n","Time: Fri Apr  1 00:32:43 2022\n","Step: 3508\t Epoch: [97][40/204]\tTime 0.501 (0.569)\tData 0.002 (0.065)\tLoss 0.0567 (0.0828)\tSoftmaxLoss 0.0066 (0.0326)\tRankLoss 0.0501 (0.0502)\tPrec@1 100.000 (98.857)\n","Time: Fri Apr  1 00:32:53 2022\n","Step: 3528\t Epoch: [97][60/204]\tTime 0.505 (0.548)\tData 0.003 (0.045)\tLoss 0.0767 (0.0796)\tSoftmaxLoss 0.0261 (0.0294)\tRankLoss 0.0506 (0.0502)\tPrec@1 100.000 (98.975)\n","Time: Fri Apr  1 00:33:03 2022\n","Step: 3548\t Epoch: [97][80/204]\tTime 0.495 (0.542)\tData 0.002 (0.039)\tLoss 0.0647 (0.0757)\tSoftmaxLoss 0.0144 (0.0255)\tRankLoss 0.0504 (0.0502)\tPrec@1 100.000 (99.228)\n","Time: Fri Apr  1 00:33:14 2022\n","Step: 3568\t Epoch: [97][100/204]\tTime 0.506 (0.542)\tData 0.003 (0.040)\tLoss 0.0559 (0.0733)\tSoftmaxLoss 0.0058 (0.0231)\tRankLoss 0.0501 (0.0502)\tPrec@1 100.000 (99.350)\n","Time: Fri Apr  1 00:33:24 2022\n","Step: 3588\t Epoch: [97][120/204]\tTime 0.510 (0.536)\tData 0.003 (0.034)\tLoss 0.0525 (0.0706)\tSoftmaxLoss 0.0018 (0.0204)\tRankLoss 0.0506 (0.0502)\tPrec@1 100.000 (99.458)\n","Time: Fri Apr  1 00:33:35 2022\n","Step: 3608\t Epoch: [97][140/204]\tTime 0.505 (0.535)\tData 0.003 (0.034)\tLoss 0.0501 (0.0681)\tSoftmaxLoss 0.0001 (0.0179)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (99.535)\n","Time: Fri Apr  1 00:33:46 2022\n","Step: 3628\t Epoch: [97][160/204]\tTime 0.505 (0.538)\tData 0.003 (0.037)\tLoss 0.0508 (0.0669)\tSoftmaxLoss 0.0005 (0.0167)\tRankLoss 0.0503 (0.0502)\tPrec@1 100.000 (99.583)\n","Time: Fri Apr  1 00:33:57 2022\n","Step: 3648\t Epoch: [97][180/204]\tTime 0.505 (0.537)\tData 0.004 (0.036)\tLoss 0.0543 (0.0676)\tSoftmaxLoss 0.0033 (0.0173)\tRankLoss 0.0510 (0.0503)\tPrec@1 100.000 (99.504)\n","Time: Fri Apr  1 00:34:07 2022\n","Step: 3668\t Epoch: [97][200/204]\tTime 0.500 (0.534)\tData 0.001 (0.033)\tLoss 0.0503 (0.0679)\tSoftmaxLoss 0.0003 (0.0176)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (99.487)\n","Time: Fri Apr  1 00:34:10 2022\n","Test: [0/14]\tTime 1.697 (1.697)\tSoftmaxLoss 0.1349 (0.1349)\tPrec@1 96.875 (96.875)\n"," * Prec@1 88.591\n","Time: Fri Apr  1 00:34:27 2022\n","Step: 3672\t Epoch: [98][0/204]\tTime 1.581 (1.581)\tData 1.063 (1.063)\tLoss 0.0550 (0.0550)\tSoftmaxLoss 0.0044 (0.0044)\tRankLoss 0.0506 (0.0506)\tPrec@1 100.000 (100.000)\n","Time: Fri Apr  1 00:34:39 2022\n","Step: 3692\t Epoch: [98][20/204]\tTime 1.132 (0.647)\tData 0.602 (0.141)\tLoss 0.0506 (0.0647)\tSoftmaxLoss 0.0006 (0.0144)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (99.405)\n","Time: Fri Apr  1 00:34:50 2022\n","Step: 3712\t Epoch: [98][40/204]\tTime 0.510 (0.585)\tData 0.002 (0.080)\tLoss 0.0900 (0.0617)\tSoftmaxLoss 0.0400 (0.0114)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (99.695)\n","Time: Fri Apr  1 00:35:00 2022\n","Step: 3732\t Epoch: [98][60/204]\tTime 0.508 (0.568)\tData 0.002 (0.065)\tLoss 0.0508 (0.0620)\tSoftmaxLoss 0.0008 (0.0116)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (99.705)\n","Time: Fri Apr  1 00:35:10 2022\n","Step: 3752\t Epoch: [98][80/204]\tTime 0.502 (0.553)\tData 0.003 (0.050)\tLoss 0.0632 (0.0607)\tSoftmaxLoss 0.0122 (0.0103)\tRankLoss 0.0510 (0.0504)\tPrec@1 100.000 (99.778)\n","Time: Fri Apr  1 00:35:21 2022\n","Step: 3772\t Epoch: [98][100/204]\tTime 0.504 (0.545)\tData 0.002 (0.043)\tLoss 0.0512 (0.0605)\tSoftmaxLoss 0.0004 (0.0101)\tRankLoss 0.0508 (0.0504)\tPrec@1 100.000 (99.807)\n","Time: Fri Apr  1 00:35:31 2022\n","Step: 3792\t Epoch: [98][120/204]\tTime 0.502 (0.539)\tData 0.002 (0.037)\tLoss 0.0536 (0.0594)\tSoftmaxLoss 0.0035 (0.0090)\tRankLoss 0.0501 (0.0504)\tPrec@1 100.000 (99.839)\n","Time: Fri Apr  1 00:35:42 2022\n","Step: 3812\t Epoch: [98][140/204]\tTime 0.911 (0.542)\tData 0.415 (0.040)\tLoss 0.0516 (0.0605)\tSoftmaxLoss 0.0012 (0.0100)\tRankLoss 0.0504 (0.0504)\tPrec@1 100.000 (99.717)\n","Time: Fri Apr  1 00:35:53 2022\n","Step: 3832\t Epoch: [98][160/204]\tTime 0.501 (0.540)\tData 0.003 (0.038)\tLoss 0.0525 (0.0614)\tSoftmaxLoss 0.0015 (0.0110)\tRankLoss 0.0510 (0.0504)\tPrec@1 100.000 (99.680)\n","Time: Fri Apr  1 00:36:03 2022\n","Step: 3852\t Epoch: [98][180/204]\tTime 0.507 (0.541)\tData 0.002 (0.039)\tLoss 0.0527 (0.0629)\tSoftmaxLoss 0.0023 (0.0124)\tRankLoss 0.0504 (0.0505)\tPrec@1 100.000 (99.629)\n","Time: Fri Apr  1 00:36:14 2022\n","Step: 3872\t Epoch: [98][200/204]\tTime 0.495 (0.538)\tData 0.001 (0.036)\tLoss 0.0533 (0.0674)\tSoftmaxLoss 0.0032 (0.0169)\tRankLoss 0.0501 (0.0505)\tPrec@1 100.000 (99.444)\n","Time: Fri Apr  1 00:36:17 2022\n","Test: [0/14]\tTime 1.667 (1.667)\tSoftmaxLoss 0.1243 (0.1243)\tPrec@1 93.750 (93.750)\n"," * Prec@1 85.682\n","Time: Fri Apr  1 00:36:33 2022\n","Step: 3876\t Epoch: [99][0/204]\tTime 1.462 (1.462)\tData 0.943 (0.943)\tLoss 0.0508 (0.0508)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0505 (0.0505)\tPrec@1 100.000 (100.000)\n","Time: Fri Apr  1 00:36:44 2022\n","Step: 3896\t Epoch: [99][20/204]\tTime 0.512 (0.610)\tData 0.003 (0.104)\tLoss 0.0648 (0.0710)\tSoftmaxLoss 0.0147 (0.0205)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (99.070)\n","Time: Fri Apr  1 00:36:55 2022\n","Step: 3916\t Epoch: [99][40/204]\tTime 0.497 (0.583)\tData 0.002 (0.079)\tLoss 0.2597 (0.0698)\tSoftmaxLoss 0.2093 (0.0193)\tRankLoss 0.0504 (0.0505)\tPrec@1 84.375 (99.143)\n","Time: Fri Apr  1 00:37:06 2022\n","Step: 3936\t Epoch: [99][60/204]\tTime 0.496 (0.566)\tData 0.002 (0.063)\tLoss 0.1098 (0.0723)\tSoftmaxLoss 0.0598 (0.0218)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (99.296)\n","Time: Fri Apr  1 00:37:16 2022\n","Step: 3956\t Epoch: [99][80/204]\tTime 0.504 (0.555)\tData 0.002 (0.053)\tLoss 0.0526 (0.0700)\tSoftmaxLoss 0.0023 (0.0195)\tRankLoss 0.0503 (0.0505)\tPrec@1 100.000 (99.441)\n","Time: Fri Apr  1 00:37:27 2022\n","Step: 3976\t Epoch: [99][100/204]\tTime 0.515 (0.550)\tData 0.003 (0.048)\tLoss 0.0531 (0.0717)\tSoftmaxLoss 0.0027 (0.0212)\tRankLoss 0.0503 (0.0505)\tPrec@1 100.000 (99.312)\n","Time: Fri Apr  1 00:37:37 2022\n","Step: 3996\t Epoch: [99][120/204]\tTime 0.517 (0.545)\tData 0.003 (0.042)\tLoss 0.0843 (0.0689)\tSoftmaxLoss 0.0343 (0.0184)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (99.425)\n","Time: Fri Apr  1 00:37:47 2022\n","Step: 4016\t Epoch: [99][140/204]\tTime 0.495 (0.541)\tData 0.002 (0.038)\tLoss 0.0572 (0.0784)\tSoftmaxLoss 0.0072 (0.0279)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (99.086)\n","Time: Fri Apr  1 00:37:59 2022\n","Step: 4036\t Epoch: [99][160/204]\tTime 0.504 (0.544)\tData 0.003 (0.042)\tLoss 0.0685 (0.0858)\tSoftmaxLoss 0.0181 (0.0353)\tRankLoss 0.0505 (0.0505)\tPrec@1 100.000 (98.627)\n","Time: Fri Apr  1 00:38:10 2022\n","Step: 4056\t Epoch: [99][180/204]\tTime 0.505 (0.546)\tData 0.003 (0.044)\tLoss 0.8412 (0.0944)\tSoftmaxLoss 0.7912 (0.0439)\tRankLoss 0.0500 (0.0505)\tPrec@1 70.312 (98.243)\n","Time: Fri Apr  1 00:38:21 2022\n","Step: 4076\t Epoch: [99][200/204]\tTime 0.520 (0.547)\tData 0.002 (0.045)\tLoss 0.1333 (0.1068)\tSoftmaxLoss 0.0826 (0.0563)\tRankLoss 0.0507 (0.0505)\tPrec@1 96.094 (97.874)\n","Time: Fri Apr  1 00:38:24 2022\n","Test: [0/14]\tTime 1.705 (1.705)\tSoftmaxLoss 0.9907 (0.9907)\tPrec@1 78.125 (78.125)\n"," * Prec@1 81.879\n"]}]},{"cell_type":"markdown","source":["# From here all cells mean for testing phase."],"metadata":{"id":"j3IoZ-KnNJFw"}},{"cell_type":"code","source":["# this class is just like RandomDataset above but we copied that with path to test dataset images.\n","class RandomDatasetTest(Dataset):\n","    def __init__(self, transform=None, dataloader=default_loader):\n","        self.transform = transform\n","        self.dataloader = dataloader\n","\n","        with open('/content/drive/MyDrive/LV_data/main_closure_hardware/test/test.txt', 'r') as fid:\n","            self.imglist = fid.readlines()\n","\n","        self.labels = []\n","        for line in self.imglist:\n","            #print(i)\n","            image_path, label = line.strip().split()\n","            self.labels.append(int(label))\n","        self.labels = np.array(self.labels)\n","        self.labels = torch.LongTensor(self.labels)\n","\n","    def __getitem__(self, index):\n","        image_name, label = self.imglist[index].strip().split()\n","        image_path = image_name\n","        img = self.dataloader(image_path)\n","        img = self.transform(img)\n","        label = int(label)\n","        label = torch.LongTensor([label])\n","\n","        return img, label\n","\n","    def __len__(self):\n","        return len(self.imglist)"],"metadata":{"id":"tRaRx9jktHGE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# loading trained weight and model\n","model = API_Net()\n","model = model.to(device)\n","model.conv = nn.DataParallel(model.conv)\n","checkpoint = torch.load('/content/drive/MyDrive/main-zipper-pull_model_best.pth.tar')\n","\n","model.load_state_dict(checkpoint['state_dict'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4OJOeKPItL07","executionInfo":{"status":"ok","timestamp":1648776003300,"user_tz":240,"elapsed":2470,"user":{"displayName":"Reza hojjaty saeedy","userId":"18162372094943119950"}},"outputId":"1415a261-9764-4219-fa6c-2b9cd5599389"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# This function calculates the metrics TP, FP, TN, and FN\n","def confusion(scores, targets):\n","    \"\"\" Returns the confusion matrix for the values in the `prediction` and `truth`\n","    tensors, i.e. the amount of positions where the values of `prediction`\n","    and `truth` are\n","    - 1 and 1 (True Positive)\n","    - 1 and 0 (False Positive)\n","    - 0 and 0 (True Negative)\n","    - 0 and 1 (False Negative)\n","    \"\"\"\n","\n","    _, ind = scores.topk(1, 1, True, True)\n","    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n","    prediction = correct.view(-1).float()\n","    #correct_total = correct.view(-1).float().sum()  # 0D tensor\n","    confusion_vector = prediction / targets\n","    # Element-wise division of the 2 tensors returns a new tensor which holds a\n","    # unique value for each case:\n","    #   1     where prediction and truth are 1 (True Positive)\n","    #   inf   where prediction is 1 and truth is 0 (False Positive)\n","    #   nan   where prediction and truth are 0 (True Negative)\n","    #   0     where prediction is 0 and truth is 1 (False Negative)\n","\n","    true_positives = torch.sum(confusion_vector == 1).item()\n","    false_positives = torch.sum(confusion_vector == float('inf')).item()\n","    true_negatives = torch.sum(torch.isnan(confusion_vector)).item()\n","    false_negatives = torch.sum(confusion_vector == 0).item()\n","\n","    return true_positives, false_positives, true_negatives, false_negatives"],"metadata":{"id":"0NwoRm3UtTrL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calculating specificity and sensitivity on our test dataset\n","test_dataset = RandomDatasetTest(transform=transforms.Compose([\n","                transforms.Resize([512, 512]),\n","                transforms.CenterCrop([448, 448]),\n","                transforms.ToTensor(),\n","                transforms.Normalize(\n","                    mean=(0.485, 0.456, 0.406),\n","                    std=(0.229, 0.224, 0.225)\n","                )]))\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=6, shuffle=False,\n","                num_workers=workers, pin_memory=True)\n","\n","criterion = nn.CrossEntropyLoss().to(device)\n","\n","TP=0\n","TN=0\n","FP=0\n","FN=0\n","for input, target in test_loader:\n","  input_var = input.to(device)\n","  target_var = target.to(device).squeeze()\n","\n","  logits = model(input_var, targets=None, flag='val')\n","  tp, fp, tn, fn = confusion(logits, target_var)\n","\n","  TP += tp\n","  FP += fp\n","  TN += tn\n","  FN += fn\n","sp, se = TN / (TN+FP), TP / (TP+FN)\n","print('specificity: {}\\t sensitivity: {}'.format(sp, se))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ki5yURXttZ44","executionInfo":{"status":"ok","timestamp":1648776109595,"user_tz":240,"elapsed":76753,"user":{"displayName":"Reza hojjaty saeedy","userId":"18162372094943119950"}},"outputId":"c69bc459-27a0-4993-8f9c-87e662d03c8f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["specificity: 0.5406976744186046\t sensitivity: 0.5247524752475248\n"]}]}]}