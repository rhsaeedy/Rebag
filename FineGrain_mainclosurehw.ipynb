{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FineGrain-mainclosurehw.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#This notebook Implements the API-Net for Fine-Grained image classification. For\n",
        "# more details please refere to the notebook *FineGrain-zipperpull*"
      ],
      "metadata": {
        "id": "dtUxERjlo30P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-35ANnJgo1dv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import shutil\n",
        "\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best, filename='checkpoint_main-closure.pth.tar'):\n",
        "    torch.save(state, '/content/drive/MyDrive/'+filename)\n",
        "    if is_best:\n",
        "        shutil.copyfile('/content/drive/MyDrive/'+filename, '/content/drive/MyDrive/main-closure_model_best.pth.tar')\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Keeps track of most recent, average, sum, and count of a metric.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def accuracy(scores, targets, k):\n",
        "    \"\"\"\n",
        "    Computes top-k accuracy, from predicted and true labels.\n",
        "\n",
        "    :param scores: scores from the model\n",
        "    :param targets: true labels\n",
        "    :param k: k in top-k accuracy\n",
        "    :return: top-k accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = targets.size(0)\n",
        "    _, ind = scores.topk(k, 1, True, True)\n",
        "    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n",
        "    correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
        "    return correct_total.item() * (100.0 / batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required packages.\n",
        "from torch import nn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.utils.data\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.sampler import BatchSampler\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "import torch.backends.cudnn as cudnn"
      ],
      "metadata": {
        "id": "bCrKfZ4U1fGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def default_loader(path):\n",
        "    try:\n",
        "        img = Image.open(path).convert('RGB')\n",
        "    except:\n",
        "        with open('read_error.txt', 'a') as fid:\n",
        "            fid.write(path + '\\n')\n",
        "        return Image.new('RGB', (224, 224), 'white')\n",
        "    return img\n",
        "\n",
        "    \n",
        "class RandomDataset(Dataset):\n",
        "    def __init__(self, transform=None, dataloader=default_loader):\n",
        "        self.transform = transform\n",
        "        self.dataloader = dataloader\n",
        "\n",
        "        with open('/content/drive/MyDrive/LV_data/main_closure_hardware/val/val.txt', 'r') as fid:\n",
        "            self.imglist = fid.readlines()\n",
        "\n",
        "        self.labels = []\n",
        "        for line in self.imglist:\n",
        "            image_path, label = line.strip().split()\n",
        "            self.labels.append(int(label))\n",
        "        self.labels = np.array(self.labels)\n",
        "        self.labels = torch.LongTensor(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_name, label = self.imglist[index].strip().split()\n",
        "        image_path = image_name\n",
        "        img = self.dataloader(image_path)\n",
        "        img = self.transform(img)\n",
        "        label = int(label)\n",
        "        label = torch.LongTensor([label])\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imglist)\n",
        "\n",
        "\n",
        "class BatchDataset(Dataset):\n",
        "    def __init__(self, transform=None, dataloader=default_loader):\n",
        "        self.transform = transform\n",
        "        self.dataloader = dataloader\n",
        "\n",
        "        with open('/content/drive/MyDrive/LV_data/main_closure_hardware/train/train.txt', 'r') as fid:\n",
        "            self.imglist = fid.readlines()\n",
        "\n",
        "        self.labels = []\n",
        "        for line in self.imglist:\n",
        "            #print(i)\n",
        "            image_path, label = line.strip().split()\n",
        "            self.labels.append(int(label))\n",
        "        self.labels = np.array(self.labels)\n",
        "        self.labels = torch.LongTensor(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_name, label = self.imglist[index].strip().split()\n",
        "        image_path = image_name\n",
        "        img = self.dataloader(image_path)\n",
        "        img = self.transform(img)\n",
        "        label = int(label)\n",
        "        label = torch.LongTensor([label])\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imglist)\n",
        "\n",
        "\n",
        "class BalancedBatchSampler(BatchSampler):\n",
        "    def __init__(self, dataset, n_classes, n_samples):\n",
        "        self.labels = dataset.labels\n",
        "        self.labels_set = list(set(self.labels.numpy()))\n",
        "        self.label_to_indices = {label: np.where(self.labels.numpy() == label)[0]\n",
        "                                 for label in self.labels_set}\n",
        "        for l in self.labels_set:\n",
        "            np.random.shuffle(self.label_to_indices[l])\n",
        "        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n",
        "        self.count = 0\n",
        "        self.n_classes = n_classes\n",
        "        self.n_samples = n_samples\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = self.n_samples * self.n_classes\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.count = 0\n",
        "        while self.count + self.batch_size < len(self.dataset):\n",
        "            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n",
        "            indices = []\n",
        "            for class_ in classes:\n",
        "                indices.extend(self.label_to_indices[class_][\n",
        "                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n",
        "                                                                         class_] + self.n_samples])\n",
        "                self.used_label_indices_count[class_] += self.n_samples\n",
        "                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n",
        "                    np.random.shuffle(self.label_to_indices[class_])\n",
        "                    self.used_label_indices_count[class_] = 0\n",
        "            yield indices\n",
        "            self.count += self.n_classes * self.n_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset) // self.batch_size\n"
      ],
      "metadata": {
        "id": "IGS34l3h1nh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def pdist(vectors):\n",
        "    distance_matrix = -2 * vectors.mm(torch.t(vectors)) + vectors.pow(2).sum(dim=1).view(1, -1) + vectors.pow(2).sum(\n",
        "        dim=1).view(-1, 1)\n",
        "    return distance_matrix\n",
        "\n",
        "class API_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(API_Net, self).__init__()\n",
        "\n",
        "        resnet101 = models.resnet101(pretrained=True)\n",
        "        layers = list(resnet101.children())[:-2]\n",
        "\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "        self.avg = nn.AvgPool2d(kernel_size=14, stride=1)\n",
        "        self.map1 = nn.Linear(2048 * 2, 512)\n",
        "        self.map2 = nn.Linear(512, 2048)\n",
        "        self.fc = nn.Linear(2048, 2)\n",
        "        self.drop = nn.Dropout(p=0.5)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, images, targets=None, flag='train'):\n",
        "        conv_out = self.conv(images)\n",
        "        pool_out = self.avg(conv_out).squeeze()\n",
        "\n",
        "        if flag == 'train':\n",
        "            intra_pairs, inter_pairs, \\\n",
        "                    intra_labels, inter_labels = self.get_pairs(pool_out, targets)\n",
        "\n",
        "            features1 = torch.cat([pool_out[intra_pairs[:, 0]], pool_out[inter_pairs[:, 0]]], dim=0)\n",
        "            features2 = torch.cat([pool_out[intra_pairs[:, 1]], pool_out[inter_pairs[:, 1]]], dim=0)\n",
        "            labels1 = torch.cat([intra_labels[:, 0], inter_labels[:, 0]], dim=0)\n",
        "            labels2 = torch.cat([intra_labels[:, 1], inter_labels[:, 1]], dim=0)\n",
        "\n",
        "\n",
        "            mutual_features = torch.cat([features1, features2], dim=1)\n",
        "            map1_out = self.map1(mutual_features)\n",
        "            map2_out = self.drop(map1_out)\n",
        "            map2_out = self.map2(map2_out)\n",
        "\n",
        "\n",
        "            gate1 = torch.mul(map2_out, features1)\n",
        "            gate1 = self.sigmoid(gate1)\n",
        "\n",
        "            gate2 = torch.mul(map2_out, features2)\n",
        "            gate2 = self.sigmoid(gate2)\n",
        "\n",
        "            features1_self = torch.mul(gate1, features1) + features1\n",
        "            features1_other = torch.mul(gate2, features1) + features1\n",
        "\n",
        "            features2_self = torch.mul(gate2, features2) + features2\n",
        "            features2_other = torch.mul(gate1, features2) + features2\n",
        "\n",
        "            logit1_self = self.fc(self.drop(features1_self))\n",
        "            logit1_other = self.fc(self.drop(features1_other))\n",
        "            logit2_self = self.fc(self.drop(features2_self))\n",
        "            logit2_other = self.fc(self.drop(features2_other))\n",
        "\n",
        "            return logit1_self, logit1_other, logit2_self, logit2_other, labels1, labels2\n",
        "\n",
        "        elif flag == 'val':\n",
        "            return self.fc(pool_out)\n",
        "\n",
        "\n",
        "    def get_pairs(self, embeddings, labels):\n",
        "        distance_matrix = pdist(embeddings).detach().cpu().numpy()\n",
        "\n",
        "        labels = labels.detach().cpu().numpy().reshape(-1,1)\n",
        "        num = labels.shape[0]\n",
        "        dia_inds = np.diag_indices(num)\n",
        "        lb_eqs = (labels == labels.T)\n",
        "        lb_eqs[dia_inds] = False\n",
        "        dist_same = distance_matrix.copy()\n",
        "        dist_same[lb_eqs == False] = np.inf\n",
        "        intra_idxs = np.argmin(dist_same, axis=1)\n",
        "\n",
        "        dist_diff = distance_matrix.copy()\n",
        "        lb_eqs[dia_inds] = True\n",
        "        dist_diff[lb_eqs == True] = np.inf\n",
        "        inter_idxs = np.argmin(dist_diff, axis=1)\n",
        "\n",
        "        intra_pairs = np.zeros([embeddings.shape[0], 2])\n",
        "        inter_pairs  = np.zeros([embeddings.shape[0], 2])\n",
        "        intra_labels = np.zeros([embeddings.shape[0], 2])\n",
        "        inter_labels = np.zeros([embeddings.shape[0], 2])\n",
        "        for i in range(embeddings.shape[0]):\n",
        "            intra_labels[i, 0] = labels[i]\n",
        "            intra_labels[i, 1] = labels[intra_idxs[i]]\n",
        "            intra_pairs[i, 0] = i\n",
        "            intra_pairs[i, 1] = intra_idxs[i]\n",
        "\n",
        "            inter_labels[i, 0] = labels[i]\n",
        "            inter_labels[i, 1] = labels[inter_idxs[i]]\n",
        "            inter_pairs[i, 0] = i\n",
        "            inter_pairs[i, 1] = inter_idxs[i]\n",
        "\n",
        "        intra_labels = torch.from_numpy(intra_labels).long().to(device)\n",
        "        intra_pairs = torch.from_numpy(intra_pairs).long().to(device)\n",
        "        inter_labels = torch.from_numpy(inter_labels).long().to(device)\n",
        "        inter_pairs = torch.from_numpy(inter_pairs).long().to(device)\n",
        "\n",
        "        return intra_pairs, inter_pairs, intra_labels, inter_labels\n"
      ],
      "metadata": {
        "id": "CkxIieUH2FPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "workers=2\n",
        "epochs=80\n",
        "start_epoch=0\n",
        "#batch_size=32\n",
        "lr=0.01\n",
        "momentum=0.9\n",
        "weight_decay=5e-4\n",
        "print_freq=20\n",
        "evaluate_freq=20\n",
        "resume='/content/drive/MyDrive/checkpoint_main-closure.pth.tar'\n",
        "n_classes=2\n",
        "n_samples=8\n"
      ],
      "metadata": {
        "id": "9KLKwmHz2IlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, model, criterion, optimizer_conv, scheduler_conv, optimizer_fc, scheduler_fc, epoch, step):\n",
        "    best_prec1 = 0\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    softmax_losses = AverageMeter()\n",
        "    rank_losses = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    \n",
        "    # switch to train mode\n",
        "    end = time.time()\n",
        "    rank_criterion = nn.MarginRankingLoss(margin=0.05)\n",
        "    softmax_layer = nn.Softmax(dim=0).to(device)\n",
        "\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "        model.train()\n",
        "\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "        input_var = input.to(device)\n",
        "        target_var = target.to(device).squeeze()\n",
        "\n",
        "        # compute output\n",
        "        logit1_self, logit1_other, logit2_self, logit2_other, labels1, labels2 = model(input_var, target_var,flag='train')\n",
        "        batch_size = logit1_self.shape[0]\n",
        "        labels1 = labels1.to(device)\n",
        "        labels2 = labels2.to(device)\n",
        "\n",
        "        self_logits = torch.zeros(2 * batch_size, 2).to(device)\n",
        "        other_logits = torch.zeros(2 * batch_size, 2).to(device)\n",
        "        self_logits[:batch_size] = logit1_self\n",
        "        self_logits[batch_size:] = logit2_self\n",
        "        other_logits[:batch_size] = logit1_other\n",
        "        other_logits[batch_size:] = logit2_other\n",
        "\n",
        "        # compute loss\n",
        "        logits = torch.cat([self_logits, other_logits], dim=0)\n",
        "        targets = torch.cat([labels1, labels2, labels1, labels2], dim=0)\n",
        "    \n",
        "        softmax_loss = criterion(logits, targets)\n",
        "\n",
        "        self_scores = softmax_layer(self_logits)[torch.arange(2 * batch_size).to(device).long(),\n",
        "                                                 torch.cat([labels1, labels2], dim=0)]\n",
        "        other_scores = softmax_layer(other_logits)[torch.arange(2 * batch_size).to(device).long(),\n",
        "                                                   torch.cat([labels1, labels2], dim=0)]\n",
        "        flag = torch.ones([2 * batch_size, ]).to(device)\n",
        "        rank_loss = rank_criterion(self_scores, other_scores, flag)\n",
        "\n",
        "        loss = softmax_loss + rank_loss\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        prec1 = accuracy(logits, targets, 1)\n",
        "        #prec2 = accuracy(logits, targets, 2)\n",
        "        losses.update(loss.item(), 2 * batch_size)\n",
        "        softmax_losses.update(softmax_loss.item(), 4 * batch_size)\n",
        "        rank_losses.update(rank_loss.item(), 2 * batch_size)\n",
        "        top1.update(prec1, 4 * batch_size)\n",
        "        #top2.update(prec2, 4 * batch_size)\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer_conv.zero_grad()\n",
        "        optimizer_fc.zero_grad()\n",
        "        loss.backward()\n",
        "        if epoch >= 8:\n",
        "            optimizer_conv.step()\n",
        "        optimizer_fc.step()\n",
        "        scheduler_conv.step()\n",
        "        scheduler_fc.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % print_freq == 0:\n",
        "            print('Time: {time}\\nStep: {step}\\t Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'SoftmaxLoss {softmax_loss.val:.4f} ({softmax_loss.avg:.4f})\\t'\n",
        "                  'RankLoss {rank_loss.val:.4f} ({rank_loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                epoch, i, len(train_loader), batch_time=batch_time,\n",
        "                data_time=data_time, loss=losses, softmax_loss=softmax_losses, rank_loss=rank_losses,\n",
        "                top1=top1, step=step, time=time.asctime(time.localtime(time.time()))))\n",
        "\n",
        "        if i == len(train_loader) - 1:\n",
        "            val_dataset = RandomDataset(transform=transforms.Compose([\n",
        "                transforms.Resize([512, 512]),\n",
        "                transforms.CenterCrop([448, 448]),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=(0.485, 0.456, 0.406),\n",
        "                    std=(0.229, 0.224, 0.225)\n",
        "                )]))\n",
        "            val_loader = torch.utils.data.DataLoader(\n",
        "                val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                num_workers=workers, pin_memory=True)\n",
        "            prec1 = validate(val_loader, model, criterion)\n",
        "\n",
        "            # remember best prec@1 and save checkpoint\n",
        "            is_best = prec1 > best_prec1\n",
        "            best_prec1 = max(prec1, best_prec1)\n",
        "            save_checkpoint({\n",
        "                'epoch': epoch + 1,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'best_prec1': best_prec1,\n",
        "                'optimizer_conv': optimizer_conv.state_dict(),\n",
        "                'optimizer_fc': optimizer_fc.state_dict(),\n",
        "            }, is_best)\n",
        "\n",
        "        step = step + 1\n",
        "    return step"
      ],
      "metadata": {
        "id": "3Xd5nFR32RTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(val_loader, model, criterion):\n",
        "    batch_time = AverageMeter()\n",
        "    softmax_losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    #top2 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "    end = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (input, target) in enumerate(val_loader):\n",
        "\n",
        "            input_var = input.to(device)\n",
        "            target_var = target.to(device).squeeze()\n",
        "\n",
        "            # compute output\n",
        "            logits = model(input_var, targets=None, flag='val')\n",
        "            #print(logits)\n",
        "            #print(target_var)\n",
        "            softmax_loss = criterion(logits, target_var)\n",
        "            \n",
        "\n",
        "            prec1 = accuracy(logits, target_var, 1)\n",
        "            #prec2 = accuracy(logits, target_var, 2)\n",
        "            softmax_losses.update(softmax_loss.item(), logits.size(0))\n",
        "            top1.update(prec1, logits.size(0))\n",
        "            #top2.update(prec2, logits.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if i % print_freq == 0:\n",
        "                print('Time: {time}\\nTest: [{0}/{1}]\\t'\n",
        "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                      'SoftmaxLoss {softmax_loss.val:.4f} ({softmax_loss.avg:.4f})\\t'\n",
        "                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                    i, len(val_loader), batch_time=batch_time, softmax_loss=softmax_losses,\n",
        "                    top1=top1, time=time.asctime(time.localtime(time.time()))))\n",
        "        print(' * Prec@1 {top1.avg:.3f}'.format(top1=top1))\n",
        "\n",
        "    return top1.avg\n"
      ],
      "metadata": {
        "id": "UXFXQJY92UQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(20)\n",
        "torch.cuda.manual_seed_all(20)\n",
        "np.random.seed(25)\n",
        "epochs=150\n",
        "# create model\n",
        "model = API_Net()\n",
        "model = model.to(device)\n",
        "model.conv = nn.DataParallel(model.conv)\n",
        "\n",
        "# define loss function (criterion) and optimizer\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer_conv = torch.optim.SGD(model.conv.parameters(), lr,\n",
        "                                  momentum=momentum,\n",
        "                                  weight_decay=weight_decay)\n",
        "\n",
        "fc_parameters = [value for name, value in model.named_parameters() if 'conv' not in name]\n",
        "optimizer_fc = torch.optim.SGD(fc_parameters, lr,\n",
        "                                momentum=momentum,\n",
        "                                weight_decay=weight_decay)\n",
        "if resume:\n",
        "    if os.path.isfile(resume):\n",
        "        print('loading checkpoint {}'.format(resume))\n",
        "        checkpoint = torch.load(resume)\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        best_prec1 = checkpoint['best_prec1']\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer_conv.load_state_dict(checkpoint['optimizer_conv'])\n",
        "        optimizer_fc.load_state_dict(checkpoint['optimizer_fc'])\n",
        "        print('loaded checkpoint {}(epoch {})'.format(resume, checkpoint['epoch']))\n",
        "    else:\n",
        "        print('no checkpoint found at {}'.format(resume))\n",
        "\n",
        "cudnn.benchmark = True\n",
        "# Data loading code\n",
        "train_dataset = BatchDataset(transform=transforms.Compose([\n",
        "    transforms.Resize([512, 512]),\n",
        "    transforms.RandomCrop([448, 448]),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=(0.485, 0.456, 0.406),\n",
        "        std=(0.229, 0.224, 0.225)\n",
        "    )]))\n",
        "\n",
        "train_sampler = BalancedBatchSampler(train_dataset, n_classes, n_samples)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=train_sampler, num_workers=workers, pin_memory=True)\n",
        "scheduler_conv = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_conv, 100 * len(train_loader))\n",
        "scheduler_fc = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_fc, 100 * len(train_loader))\n",
        "\n",
        "step = 0\n",
        "print('START TIME:', time.asctime(time.localtime(time.time())))\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    step = train(train_loader, model, criterion, optimizer_conv, scheduler_conv, optimizer_fc, scheduler_fc, epoch, step)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJTVlyY32X_j",
        "outputId": "963695e9-6654-4453-ddad-0ea142f99bde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no checkpoint found at /content/drive/MyDrive/checkpoint_main-closure.pth.tar\n",
            "START TIME: Wed Mar 30 22:34:27 2022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: Wed Mar 30 22:34:29 2022\n",
            "Step: 0\t Epoch: [0][0/149]\tTime 1.646 (1.646)\tData 1.369 (1.369)\tLoss 0.9805 (0.9805)\tSoftmaxLoss 0.9318 (0.9318)\tRankLoss 0.0487 (0.0487)\tPrec@1 32.812 (32.812)\n",
            "Time: Wed Mar 30 22:35:08 2022\n",
            "Step: 20\t Epoch: [0][20/149]\tTime 4.131 (1.955)\tData 3.891 (1.711)\tLoss 1.0989 (1.0345)\tSoftmaxLoss 1.0528 (0.9840)\tRankLoss 0.0461 (0.0505)\tPrec@1 50.000 (54.204)\n",
            "Time: Wed Mar 30 22:36:06 2022\n",
            "Step: 40\t Epoch: [0][40/149]\tTime 4.545 (2.424)\tData 4.320 (2.190)\tLoss 1.0977 (0.9915)\tSoftmaxLoss 1.0470 (0.9404)\tRankLoss 0.0507 (0.0511)\tPrec@1 58.594 (58.213)\n",
            "Time: Wed Mar 30 22:36:50 2022\n",
            "Step: 60\t Epoch: [0][60/149]\tTime 1.777 (2.350)\tData 1.551 (2.118)\tLoss 1.2469 (1.0102)\tSoftmaxLoss 1.1918 (0.9577)\tRankLoss 0.0551 (0.0525)\tPrec@1 70.312 (60.272)\n",
            "Time: Wed Mar 30 22:37:23 2022\n",
            "Step: 80\t Epoch: [0][80/149]\tTime 0.502 (2.174)\tData 0.273 (1.942)\tLoss 1.3154 (1.0357)\tSoftmaxLoss 1.2619 (0.9828)\tRankLoss 0.0535 (0.0530)\tPrec@1 59.375 (61.121)\n",
            "Time: Wed Mar 30 22:37:56 2022\n",
            "Step: 100\t Epoch: [0][100/149]\tTime 0.501 (2.072)\tData 0.002 (1.834)\tLoss 1.7192 (1.1432)\tSoftmaxLoss 1.6614 (1.0896)\tRankLoss 0.0578 (0.0536)\tPrec@1 58.594 (60.992)\n",
            "Time: Wed Mar 30 22:38:29 2022\n",
            "Step: 120\t Epoch: [0][120/149]\tTime 0.705 (1.998)\tData 0.482 (1.761)\tLoss 1.3244 (1.1907)\tSoftmaxLoss 1.2656 (1.1360)\tRankLoss 0.0587 (0.0546)\tPrec@1 60.938 (61.499)\n",
            "Time: Wed Mar 30 22:39:02 2022\n",
            "Step: 140\t Epoch: [0][140/149]\tTime 0.496 (1.952)\tData 0.175 (1.707)\tLoss 1.8991 (1.1950)\tSoftmaxLoss 1.8404 (1.1397)\tRankLoss 0.0586 (0.0553)\tPrec@1 62.500 (62.411)\n",
            "Time: Wed Mar 30 22:39:29 2022\n",
            "Test: [0/11]\tTime 13.799 (13.799)\tSoftmaxLoss 1.6495 (1.6495)\tPrec@1 34.375 (34.375)\n",
            " * Prec@1 49.235\n",
            "Time: Wed Mar 30 22:40:19 2022\n",
            "Step: 149\t Epoch: [1][0/149]\tTime 3.736 (3.736)\tData 3.485 (3.485)\tLoss 1.1760 (1.1760)\tSoftmaxLoss 1.1183 (1.1183)\tRankLoss 0.0577 (0.0577)\tPrec@1 65.625 (65.625)\n",
            "Time: Wed Mar 30 22:40:51 2022\n",
            "Step: 169\t Epoch: [1][20/149]\tTime 2.331 (1.729)\tData 2.103 (1.458)\tLoss 1.1686 (1.1777)\tSoftmaxLoss 1.1095 (1.1218)\tRankLoss 0.0591 (0.0559)\tPrec@1 60.938 (64.993)\n",
            "Time: Wed Mar 30 22:41:23 2022\n",
            "Step: 189\t Epoch: [1][40/149]\tTime 1.697 (1.668)\tData 1.468 (1.418)\tLoss 1.7032 (1.1988)\tSoftmaxLoss 1.6479 (1.1423)\tRankLoss 0.0553 (0.0565)\tPrec@1 62.500 (65.320)\n",
            "Time: Wed Mar 30 22:41:56 2022\n",
            "Step: 209\t Epoch: [1][60/149]\tTime 2.215 (1.663)\tData 1.978 (1.419)\tLoss 0.8362 (1.1773)\tSoftmaxLoss 0.7820 (1.1202)\tRankLoss 0.0542 (0.0571)\tPrec@1 75.000 (65.420)\n",
            "Time: Wed Mar 30 22:42:28 2022\n",
            "Step: 229\t Epoch: [1][80/149]\tTime 0.930 (1.642)\tData 0.681 (1.401)\tLoss 1.2191 (1.2074)\tSoftmaxLoss 1.1674 (1.1507)\tRankLoss 0.0517 (0.0567)\tPrec@1 58.594 (64.670)\n",
            "Time: Wed Mar 30 22:43:01 2022\n",
            "Step: 249\t Epoch: [1][100/149]\tTime 0.927 (1.641)\tData 0.692 (1.399)\tLoss 1.3094 (1.2333)\tSoftmaxLoss 1.2568 (1.1765)\tRankLoss 0.0526 (0.0568)\tPrec@1 62.500 (64.411)\n",
            "Time: Wed Mar 30 22:43:17 2022\n",
            "Step: 269\t Epoch: [1][120/149]\tTime 1.206 (1.507)\tData 0.962 (1.245)\tLoss 0.6286 (1.2504)\tSoftmaxLoss 0.5695 (1.1938)\tRankLoss 0.0591 (0.0567)\tPrec@1 79.688 (64.095)\n",
            "Time: Wed Mar 30 22:43:32 2022\n",
            "Step: 289\t Epoch: [1][140/149]\tTime 1.266 (1.401)\tData 1.038 (1.126)\tLoss 1.0799 (1.2377)\tSoftmaxLoss 1.0185 (1.1810)\tRankLoss 0.0614 (0.0568)\tPrec@1 64.844 (64.328)\n",
            "Time: Wed Mar 30 22:43:41 2022\n",
            "Test: [0/11]\tTime 3.129 (3.129)\tSoftmaxLoss 0.2646 (0.2646)\tPrec@1 87.500 (87.500)\n",
            " * Prec@1 78.593\n",
            "Time: Wed Mar 30 22:43:55 2022\n",
            "Step: 298\t Epoch: [2][0/149]\tTime 1.727 (1.727)\tData 1.481 (1.481)\tLoss 0.4974 (0.4974)\tSoftmaxLoss 0.4425 (0.4425)\tRankLoss 0.0549 (0.0549)\tPrec@1 84.375 (84.375)\n",
            "Time: Wed Mar 30 22:44:10 2022\n",
            "Step: 318\t Epoch: [2][20/149]\tTime 1.006 (0.807)\tData 0.762 (0.552)\tLoss 1.2512 (1.0434)\tSoftmaxLoss 1.1918 (0.9859)\tRankLoss 0.0594 (0.0575)\tPrec@1 65.625 (67.485)\n",
            "Time: Wed Mar 30 22:44:25 2022\n",
            "Step: 338\t Epoch: [2][40/149]\tTime 0.505 (0.783)\tData 0.002 (0.521)\tLoss 1.0482 (1.1401)\tSoftmaxLoss 0.9895 (1.0835)\tRankLoss 0.0587 (0.0566)\tPrec@1 61.719 (65.473)\n",
            "Time: Wed Mar 30 22:44:40 2022\n",
            "Step: 358\t Epoch: [2][60/149]\tTime 0.618 (0.765)\tData 0.381 (0.497)\tLoss 0.8169 (1.1127)\tSoftmaxLoss 0.7610 (1.0564)\tRankLoss 0.0559 (0.0563)\tPrec@1 65.625 (65.587)\n",
            "Time: Wed Mar 30 22:44:55 2022\n",
            "Step: 378\t Epoch: [2][80/149]\tTime 0.496 (0.765)\tData 0.002 (0.477)\tLoss 0.8601 (1.0742)\tSoftmaxLoss 0.8077 (1.0180)\tRankLoss 0.0524 (0.0562)\tPrec@1 71.875 (66.348)\n",
            "Time: Wed Mar 30 22:45:10 2022\n",
            "Step: 398\t Epoch: [2][100/149]\tTime 0.494 (0.760)\tData 0.228 (0.481)\tLoss 0.8371 (1.0436)\tSoftmaxLoss 0.7842 (0.9878)\tRankLoss 0.0528 (0.0558)\tPrec@1 69.531 (66.631)\n",
            "Time: Wed Mar 30 22:45:26 2022\n",
            "Step: 418\t Epoch: [2][120/149]\tTime 1.067 (0.765)\tData 0.816 (0.481)\tLoss 0.9789 (1.0392)\tSoftmaxLoss 0.9219 (0.9835)\tRankLoss 0.0570 (0.0557)\tPrec@1 64.844 (66.342)\n",
            "Time: Wed Mar 30 22:45:41 2022\n",
            "Step: 438\t Epoch: [2][140/149]\tTime 0.607 (0.764)\tData 0.372 (0.469)\tLoss 0.7427 (1.0301)\tSoftmaxLoss 0.6871 (0.9744)\tRankLoss 0.0557 (0.0557)\tPrec@1 75.781 (66.473)\n",
            "Time: Wed Mar 30 22:45:49 2022\n",
            "Test: [0/11]\tTime 3.178 (3.178)\tSoftmaxLoss 0.6500 (0.6500)\tPrec@1 68.750 (68.750)\n",
            " * Prec@1 69.725\n",
            "Time: Wed Mar 30 22:46:03 2022\n",
            "Step: 447\t Epoch: [3][0/149]\tTime 1.650 (1.650)\tData 1.387 (1.387)\tLoss 0.6084 (0.6084)\tSoftmaxLoss 0.5545 (0.5545)\tRankLoss 0.0539 (0.0539)\tPrec@1 74.219 (74.219)\n",
            "Time: Wed Mar 30 22:46:19 2022\n",
            "Step: 467\t Epoch: [3][20/149]\tTime 0.715 (0.827)\tData 0.466 (0.498)\tLoss 1.5086 (0.9106)\tSoftmaxLoss 1.4507 (0.8549)\tRankLoss 0.0579 (0.0557)\tPrec@1 59.375 (66.555)\n",
            "Time: Wed Mar 30 22:46:34 2022\n",
            "Step: 487\t Epoch: [3][40/149]\tTime 0.501 (0.806)\tData 0.218 (0.467)\tLoss 1.4131 (0.9562)\tSoftmaxLoss 1.3613 (0.9007)\tRankLoss 0.0518 (0.0555)\tPrec@1 57.031 (66.025)\n",
            "Time: Wed Mar 30 22:46:49 2022\n",
            "Step: 507\t Epoch: [3][60/149]\tTime 0.773 (0.783)\tData 0.524 (0.447)\tLoss 0.6753 (0.9990)\tSoftmaxLoss 0.6204 (0.9439)\tRankLoss 0.0550 (0.0552)\tPrec@1 77.344 (65.138)\n",
            "Time: Wed Mar 30 22:47:06 2022\n",
            "Step: 527\t Epoch: [3][80/149]\tTime 1.471 (0.797)\tData 1.223 (0.470)\tLoss 0.9360 (1.0348)\tSoftmaxLoss 0.8850 (0.9798)\tRankLoss 0.0510 (0.0550)\tPrec@1 64.844 (64.333)\n",
            "Time: Wed Mar 30 22:47:21 2022\n",
            "Step: 547\t Epoch: [3][100/149]\tTime 1.177 (0.786)\tData 0.941 (0.452)\tLoss 0.9652 (1.0140)\tSoftmaxLoss 0.9122 (0.9593)\tRankLoss 0.0530 (0.0547)\tPrec@1 62.500 (64.867)\n",
            "Time: Wed Mar 30 22:47:35 2022\n",
            "Step: 567\t Epoch: [3][120/149]\tTime 0.936 (0.779)\tData 0.689 (0.441)\tLoss 0.6883 (1.0034)\tSoftmaxLoss 0.6354 (0.9489)\tRankLoss 0.0529 (0.0545)\tPrec@1 71.875 (64.837)\n",
            "Time: Wed Mar 30 22:47:50 2022\n",
            "Step: 587\t Epoch: [3][140/149]\tTime 0.495 (0.773)\tData 0.151 (0.441)\tLoss 1.3800 (0.9917)\tSoftmaxLoss 1.3287 (0.9374)\tRankLoss 0.0513 (0.0543)\tPrec@1 59.375 (64.766)\n",
            "Time: Wed Mar 30 22:47:59 2022\n",
            "Test: [0/11]\tTime 3.196 (3.196)\tSoftmaxLoss 0.1812 (0.1812)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 84.098\n",
            "Time: Wed Mar 30 22:48:13 2022\n",
            "Step: 596\t Epoch: [4][0/149]\tTime 1.675 (1.675)\tData 1.405 (1.405)\tLoss 0.8410 (0.8410)\tSoftmaxLoss 0.7879 (0.7879)\tRankLoss 0.0531 (0.0531)\tPrec@1 67.969 (67.969)\n",
            "Time: Wed Mar 30 22:48:30 2022\n",
            "Step: 616\t Epoch: [4][20/149]\tTime 0.717 (0.877)\tData 0.482 (0.511)\tLoss 0.8499 (0.8839)\tSoftmaxLoss 0.7981 (0.8302)\tRankLoss 0.0518 (0.0537)\tPrec@1 70.312 (66.555)\n",
            "Time: Wed Mar 30 22:48:46 2022\n",
            "Step: 636\t Epoch: [4][40/149]\tTime 1.173 (0.842)\tData 0.945 (0.476)\tLoss 0.9830 (0.8775)\tSoftmaxLoss 0.9289 (0.8241)\tRankLoss 0.0540 (0.0534)\tPrec@1 64.062 (66.197)\n",
            "Time: Wed Mar 30 22:49:01 2022\n",
            "Step: 656\t Epoch: [4][60/149]\tTime 1.588 (0.804)\tData 1.359 (0.439)\tLoss 0.8666 (0.8614)\tSoftmaxLoss 0.8093 (0.8082)\tRankLoss 0.0574 (0.0532)\tPrec@1 61.719 (66.573)\n",
            "Time: Wed Mar 30 22:49:16 2022\n",
            "Step: 676\t Epoch: [4][80/149]\tTime 0.496 (0.796)\tData 0.003 (0.430)\tLoss 1.0449 (0.8448)\tSoftmaxLoss 0.9892 (0.7916)\tRankLoss 0.0557 (0.0532)\tPrec@1 58.594 (66.889)\n",
            "Time: Wed Mar 30 22:49:31 2022\n",
            "Step: 696\t Epoch: [4][100/149]\tTime 0.622 (0.781)\tData 0.377 (0.434)\tLoss 1.1402 (0.8468)\tSoftmaxLoss 1.0858 (0.7938)\tRankLoss 0.0544 (0.0530)\tPrec@1 58.594 (66.507)\n",
            "Time: Wed Mar 30 22:49:46 2022\n",
            "Step: 716\t Epoch: [4][120/149]\tTime 0.508 (0.784)\tData 0.002 (0.441)\tLoss 0.5757 (0.8306)\tSoftmaxLoss 0.5205 (0.7775)\tRankLoss 0.0551 (0.0531)\tPrec@1 76.562 (66.994)\n",
            "Time: Wed Mar 30 22:50:02 2022\n",
            "Step: 736\t Epoch: [4][140/149]\tTime 0.499 (0.780)\tData 0.002 (0.434)\tLoss 0.8399 (0.8333)\tSoftmaxLoss 0.7847 (0.7803)\tRankLoss 0.0553 (0.0531)\tPrec@1 65.625 (66.805)\n",
            "Time: Wed Mar 30 22:50:10 2022\n",
            "Test: [0/11]\tTime 3.091 (3.091)\tSoftmaxLoss 0.2178 (0.2178)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 84.098\n",
            "Time: Wed Mar 30 22:50:24 2022\n",
            "Step: 745\t Epoch: [5][0/149]\tTime 1.368 (1.368)\tData 1.120 (1.120)\tLoss 0.9769 (0.9769)\tSoftmaxLoss 0.9216 (0.9216)\tRankLoss 0.0553 (0.0553)\tPrec@1 64.062 (64.062)\n",
            "Time: Wed Mar 30 22:50:40 2022\n",
            "Step: 765\t Epoch: [5][20/149]\tTime 1.276 (0.828)\tData 1.041 (0.583)\tLoss 0.9840 (0.8901)\tSoftmaxLoss 0.9344 (0.8381)\tRankLoss 0.0495 (0.0520)\tPrec@1 62.500 (63.653)\n",
            "Time: Wed Mar 30 22:50:56 2022\n",
            "Step: 785\t Epoch: [5][40/149]\tTime 1.147 (0.807)\tData 0.915 (0.526)\tLoss 0.4851 (0.8662)\tSoftmaxLoss 0.4332 (0.8136)\tRankLoss 0.0519 (0.0526)\tPrec@1 78.906 (65.492)\n",
            "Time: Wed Mar 30 22:51:12 2022\n",
            "Step: 805\t Epoch: [5][60/149]\tTime 1.219 (0.807)\tData 0.975 (0.497)\tLoss 0.9732 (0.9112)\tSoftmaxLoss 0.9195 (0.8584)\tRankLoss 0.0537 (0.0528)\tPrec@1 63.281 (64.921)\n",
            "Time: Wed Mar 30 22:51:27 2022\n",
            "Step: 825\t Epoch: [5][80/149]\tTime 0.942 (0.790)\tData 0.706 (0.466)\tLoss 0.7315 (0.8961)\tSoftmaxLoss 0.6775 (0.8432)\tRankLoss 0.0540 (0.0529)\tPrec@1 67.188 (65.451)\n",
            "Time: Wed Mar 30 22:51:42 2022\n",
            "Step: 845\t Epoch: [5][100/149]\tTime 0.479 (0.780)\tData 0.002 (0.450)\tLoss 1.2289 (0.8854)\tSoftmaxLoss 1.1744 (0.8325)\tRankLoss 0.0544 (0.0529)\tPrec@1 53.906 (65.540)\n",
            "Time: Wed Mar 30 22:51:58 2022\n",
            "Step: 865\t Epoch: [5][120/149]\tTime 1.314 (0.781)\tData 1.065 (0.453)\tLoss 1.1936 (0.8795)\tSoftmaxLoss 1.1400 (0.8267)\tRankLoss 0.0536 (0.0529)\tPrec@1 57.031 (65.599)\n",
            "Time: Wed Mar 30 22:52:14 2022\n",
            "Step: 885\t Epoch: [5][140/149]\tTime 0.517 (0.786)\tData 0.002 (0.455)\tLoss 1.4897 (0.8709)\tSoftmaxLoss 1.4399 (0.8180)\tRankLoss 0.0498 (0.0529)\tPrec@1 42.188 (65.819)\n",
            "Time: Wed Mar 30 22:52:23 2022\n",
            "Test: [0/11]\tTime 3.114 (3.114)\tSoftmaxLoss 0.2332 (0.2332)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 79.511\n",
            "Time: Wed Mar 30 22:52:37 2022\n",
            "Step: 894\t Epoch: [6][0/149]\tTime 1.552 (1.552)\tData 1.302 (1.302)\tLoss 0.6833 (0.6833)\tSoftmaxLoss 0.6304 (0.6304)\tRankLoss 0.0529 (0.0529)\tPrec@1 69.531 (69.531)\n",
            "Time: Wed Mar 30 22:52:53 2022\n",
            "Step: 914\t Epoch: [6][20/149]\tTime 0.906 (0.824)\tData 0.665 (0.551)\tLoss 1.2339 (0.8451)\tSoftmaxLoss 1.1832 (0.7924)\tRankLoss 0.0507 (0.0527)\tPrec@1 57.031 (66.034)\n",
            "Time: Wed Mar 30 22:53:09 2022\n",
            "Step: 934\t Epoch: [6][40/149]\tTime 0.785 (0.801)\tData 0.541 (0.495)\tLoss 0.6399 (0.8590)\tSoftmaxLoss 0.5899 (0.8065)\tRankLoss 0.0500 (0.0525)\tPrec@1 76.562 (66.730)\n",
            "Time: Wed Mar 30 22:53:23 2022\n",
            "Step: 954\t Epoch: [6][60/149]\tTime 0.496 (0.772)\tData 0.002 (0.458)\tLoss 0.5348 (0.8552)\tSoftmaxLoss 0.4848 (0.8030)\tRankLoss 0.0500 (0.0522)\tPrec@1 77.344 (66.265)\n",
            "Time: Wed Mar 30 22:53:38 2022\n",
            "Step: 974\t Epoch: [6][80/149]\tTime 0.502 (0.770)\tData 0.002 (0.443)\tLoss 0.7922 (0.8482)\tSoftmaxLoss 0.7382 (0.7960)\tRankLoss 0.0540 (0.0522)\tPrec@1 71.875 (66.705)\n",
            "Time: Wed Mar 30 22:53:53 2022\n",
            "Step: 994\t Epoch: [6][100/149]\tTime 0.662 (0.764)\tData 0.430 (0.444)\tLoss 0.9184 (0.8573)\tSoftmaxLoss 0.8622 (0.8050)\tRankLoss 0.0562 (0.0523)\tPrec@1 60.156 (66.476)\n",
            "Time: Wed Mar 30 22:54:08 2022\n",
            "Step: 1014\t Epoch: [6][120/149]\tTime 1.103 (0.759)\tData 0.859 (0.443)\tLoss 1.2826 (0.8786)\tSoftmaxLoss 1.2281 (0.8262)\tRankLoss 0.0545 (0.0524)\tPrec@1 53.125 (65.941)\n",
            "Time: Wed Mar 30 22:54:22 2022\n",
            "Step: 1034\t Epoch: [6][140/149]\tTime 0.498 (0.754)\tData 0.002 (0.435)\tLoss 1.0213 (0.8866)\tSoftmaxLoss 0.9658 (0.8341)\tRankLoss 0.0556 (0.0525)\tPrec@1 63.281 (65.786)\n",
            "Time: Wed Mar 30 22:54:32 2022\n",
            "Test: [0/11]\tTime 3.135 (3.135)\tSoftmaxLoss 1.2505 (1.2505)\tPrec@1 34.375 (34.375)\n",
            " * Prec@1 47.706\n",
            "Time: Wed Mar 30 22:54:47 2022\n",
            "Step: 1043\t Epoch: [7][0/149]\tTime 1.838 (1.838)\tData 1.586 (1.586)\tLoss 1.2283 (1.2283)\tSoftmaxLoss 1.1776 (1.1776)\tRankLoss 0.0507 (0.0507)\tPrec@1 63.281 (63.281)\n",
            "Time: Wed Mar 30 22:55:03 2022\n",
            "Step: 1063\t Epoch: [7][20/149]\tTime 0.996 (0.850)\tData 0.753 (0.515)\tLoss 0.6182 (0.8284)\tSoftmaxLoss 0.5659 (0.7765)\tRankLoss 0.0523 (0.0519)\tPrec@1 77.344 (68.527)\n",
            "Time: Wed Mar 30 22:55:17 2022\n",
            "Step: 1083\t Epoch: [7][40/149]\tTime 0.505 (0.795)\tData 0.002 (0.461)\tLoss 0.5536 (0.8184)\tSoftmaxLoss 0.5002 (0.7660)\tRankLoss 0.0534 (0.0524)\tPrec@1 77.344 (68.369)\n",
            "Time: Wed Mar 30 22:55:33 2022\n",
            "Step: 1103\t Epoch: [7][60/149]\tTime 0.491 (0.791)\tData 0.004 (0.447)\tLoss 0.9477 (0.8115)\tSoftmaxLoss 0.8928 (0.7591)\tRankLoss 0.0549 (0.0523)\tPrec@1 57.812 (68.071)\n",
            "Time: Wed Mar 30 22:55:49 2022\n",
            "Step: 1123\t Epoch: [7][80/149]\tTime 0.503 (0.787)\tData 0.003 (0.438)\tLoss 0.6864 (0.8177)\tSoftmaxLoss 0.6328 (0.7654)\tRankLoss 0.0536 (0.0523)\tPrec@1 66.406 (67.458)\n",
            "Time: Wed Mar 30 22:56:05 2022\n",
            "Step: 1143\t Epoch: [7][100/149]\tTime 0.509 (0.792)\tData 0.002 (0.439)\tLoss 1.1506 (0.8264)\tSoftmaxLoss 1.0963 (0.7741)\tRankLoss 0.0542 (0.0522)\tPrec@1 56.250 (66.994)\n",
            "Time: Wed Mar 30 22:56:20 2022\n",
            "Step: 1163\t Epoch: [7][120/149]\tTime 1.196 (0.785)\tData 0.959 (0.437)\tLoss 1.3791 (0.8380)\tSoftmaxLoss 1.3301 (0.7857)\tRankLoss 0.0490 (0.0522)\tPrec@1 50.781 (66.606)\n",
            "Time: Wed Mar 30 22:56:35 2022\n",
            "Step: 1183\t Epoch: [7][140/149]\tTime 0.509 (0.784)\tData 0.002 (0.439)\tLoss 0.6322 (0.8389)\tSoftmaxLoss 0.5789 (0.7866)\tRankLoss 0.0533 (0.0523)\tPrec@1 72.656 (66.584)\n",
            "Time: Wed Mar 30 22:56:44 2022\n",
            "Test: [0/11]\tTime 3.077 (3.077)\tSoftmaxLoss 0.4796 (0.4796)\tPrec@1 81.250 (81.250)\n",
            " * Prec@1 75.535\n",
            "Time: Wed Mar 30 22:56:58 2022\n",
            "Step: 1192\t Epoch: [8][0/149]\tTime 1.816 (1.816)\tData 1.335 (1.335)\tLoss 0.6891 (0.6891)\tSoftmaxLoss 0.6361 (0.6361)\tRankLoss 0.0530 (0.0530)\tPrec@1 67.969 (67.969)\n",
            "Time: Wed Mar 30 22:57:16 2022\n",
            "Step: 1212\t Epoch: [8][20/149]\tTime 1.417 (0.910)\tData 0.909 (0.401)\tLoss 1.3019 (1.3382)\tSoftmaxLoss 1.2507 (1.2852)\tRankLoss 0.0512 (0.0530)\tPrec@1 54.688 (56.213)\n",
            "Time: Wed Mar 30 22:57:33 2022\n",
            "Step: 1232\t Epoch: [8][40/149]\tTime 1.255 (0.895)\tData 0.756 (0.390)\tLoss 1.0656 (1.2155)\tSoftmaxLoss 1.0168 (1.1641)\tRankLoss 0.0488 (0.0514)\tPrec@1 53.906 (53.944)\n",
            "Time: Wed Mar 30 22:57:49 2022\n",
            "Step: 1252\t Epoch: [8][60/149]\tTime 1.065 (0.860)\tData 0.556 (0.355)\tLoss 0.7930 (1.1034)\tSoftmaxLoss 0.7427 (1.0525)\tRankLoss 0.0503 (0.0509)\tPrec@1 51.562 (52.959)\n",
            "Time: Wed Mar 30 22:58:05 2022\n",
            "Step: 1272\t Epoch: [8][80/149]\tTime 0.505 (0.842)\tData 0.002 (0.337)\tLoss 0.7551 (1.0233)\tSoftmaxLoss 0.7055 (0.9725)\tRankLoss 0.0496 (0.0508)\tPrec@1 47.656 (54.167)\n",
            "Time: Wed Mar 30 22:58:22 2022\n",
            "Step: 1292\t Epoch: [8][100/149]\tTime 0.524 (0.850)\tData 0.010 (0.346)\tLoss 0.7714 (0.9748)\tSoftmaxLoss 0.7212 (0.9241)\tRankLoss 0.0502 (0.0507)\tPrec@1 59.375 (54.154)\n",
            "Time: Wed Mar 30 22:58:38 2022\n",
            "Step: 1312\t Epoch: [8][120/149]\tTime 0.523 (0.838)\tData 0.007 (0.334)\tLoss 0.7849 (0.9401)\tSoftmaxLoss 0.7347 (0.8895)\tRankLoss 0.0502 (0.0506)\tPrec@1 53.906 (55.088)\n",
            "Time: Wed Mar 30 22:58:56 2022\n",
            "Step: 1332\t Epoch: [8][140/149]\tTime 0.499 (0.845)\tData 0.002 (0.342)\tLoss 0.8925 (0.9145)\tSoftmaxLoss 0.8420 (0.8640)\tRankLoss 0.0505 (0.0505)\tPrec@1 38.281 (55.502)\n",
            "Time: Wed Mar 30 22:59:05 2022\n",
            "Test: [0/11]\tTime 3.155 (3.155)\tSoftmaxLoss 2.4083 (2.4083)\tPrec@1 37.500 (37.500)\n",
            " * Prec@1 41.284\n",
            "Time: Wed Mar 30 22:59:21 2022\n",
            "Step: 1341\t Epoch: [9][0/149]\tTime 2.316 (2.316)\tData 1.809 (1.809)\tLoss 0.7212 (0.7212)\tSoftmaxLoss 0.6699 (0.6699)\tRankLoss 0.0512 (0.0512)\tPrec@1 57.031 (57.031)\n",
            "Time: Wed Mar 30 22:59:38 2022\n",
            "Step: 1361\t Epoch: [9][20/149]\tTime 1.351 (0.941)\tData 0.854 (0.435)\tLoss 0.7018 (0.7735)\tSoftmaxLoss 0.6515 (0.7232)\tRankLoss 0.0502 (0.0503)\tPrec@1 58.594 (55.469)\n",
            "Time: Wed Mar 30 22:59:56 2022\n",
            "Step: 1381\t Epoch: [9][40/149]\tTime 1.547 (0.909)\tData 1.045 (0.405)\tLoss 0.7592 (0.7688)\tSoftmaxLoss 0.7093 (0.7186)\tRankLoss 0.0500 (0.0502)\tPrec@1 55.469 (57.088)\n",
            "Time: Wed Mar 30 23:00:12 2022\n",
            "Step: 1401\t Epoch: [9][60/149]\tTime 1.477 (0.870)\tData 0.984 (0.366)\tLoss 0.8352 (0.7689)\tSoftmaxLoss 0.7846 (0.7188)\tRankLoss 0.0506 (0.0502)\tPrec@1 53.125 (58.158)\n",
            "Time: Wed Mar 30 23:00:29 2022\n",
            "Step: 1421\t Epoch: [9][80/149]\tTime 0.855 (0.862)\tData 0.362 (0.359)\tLoss 0.9005 (0.7736)\tSoftmaxLoss 0.8509 (0.7235)\tRankLoss 0.0495 (0.0501)\tPrec@1 47.656 (58.584)\n",
            "Time: Wed Mar 30 23:00:44 2022\n",
            "Step: 1441\t Epoch: [9][100/149]\tTime 1.179 (0.843)\tData 0.674 (0.340)\tLoss 0.6175 (0.7643)\tSoftmaxLoss 0.5672 (0.7142)\tRankLoss 0.0502 (0.0501)\tPrec@1 71.875 (58.702)\n",
            "Time: Wed Mar 30 23:01:00 2022\n",
            "Step: 1461\t Epoch: [9][120/149]\tTime 1.182 (0.835)\tData 0.681 (0.331)\tLoss 0.6132 (0.7542)\tSoftmaxLoss 0.5631 (0.7041)\tRankLoss 0.0501 (0.0501)\tPrec@1 76.562 (59.401)\n",
            "Time: Wed Mar 30 23:01:16 2022\n",
            "Step: 1481\t Epoch: [9][140/149]\tTime 1.422 (0.834)\tData 0.930 (0.330)\tLoss 0.6742 (0.7473)\tSoftmaxLoss 0.6243 (0.6972)\tRankLoss 0.0499 (0.0501)\tPrec@1 70.312 (59.652)\n",
            "Time: Wed Mar 30 23:01:25 2022\n",
            "Test: [0/11]\tTime 3.136 (3.136)\tSoftmaxLoss 0.5608 (0.5608)\tPrec@1 65.625 (65.625)\n",
            " * Prec@1 70.336\n",
            "Time: Wed Mar 30 23:01:40 2022\n",
            "Step: 1490\t Epoch: [10][0/149]\tTime 2.284 (2.284)\tData 1.783 (1.783)\tLoss 0.8124 (0.8124)\tSoftmaxLoss 0.7625 (0.7625)\tRankLoss 0.0499 (0.0499)\tPrec@1 60.938 (60.938)\n",
            "Time: Wed Mar 30 23:01:59 2022\n",
            "Step: 1510\t Epoch: [10][20/149]\tTime 1.008 (0.982)\tData 0.498 (0.473)\tLoss 0.6429 (0.7165)\tSoftmaxLoss 0.5931 (0.6665)\tRankLoss 0.0497 (0.0500)\tPrec@1 74.219 (62.277)\n",
            "Time: Wed Mar 30 23:02:16 2022\n",
            "Step: 1530\t Epoch: [10][40/149]\tTime 1.458 (0.915)\tData 0.960 (0.409)\tLoss 0.6903 (0.6960)\tSoftmaxLoss 0.6399 (0.6459)\tRankLoss 0.0504 (0.0501)\tPrec@1 70.312 (64.139)\n",
            "Time: Wed Mar 30 23:02:32 2022\n",
            "Step: 1550\t Epoch: [10][60/149]\tTime 0.972 (0.884)\tData 0.476 (0.379)\tLoss 0.5068 (0.6869)\tSoftmaxLoss 0.4568 (0.6368)\tRankLoss 0.0500 (0.0501)\tPrec@1 81.250 (64.908)\n",
            "Time: Wed Mar 30 23:02:48 2022\n",
            "Step: 1570\t Epoch: [10][80/149]\tTime 0.596 (0.858)\tData 0.091 (0.354)\tLoss 0.4718 (0.6661)\tSoftmaxLoss 0.4212 (0.6159)\tRankLoss 0.0507 (0.0502)\tPrec@1 80.469 (67.033)\n",
            "Time: Wed Mar 30 23:03:04 2022\n",
            "Step: 1590\t Epoch: [10][100/149]\tTime 0.495 (0.844)\tData 0.002 (0.341)\tLoss 0.5911 (0.6751)\tSoftmaxLoss 0.5407 (0.6248)\tRankLoss 0.0503 (0.0502)\tPrec@1 75.000 (66.654)\n",
            "Time: Wed Mar 30 23:03:20 2022\n",
            "Step: 1610\t Epoch: [10][120/149]\tTime 0.500 (0.838)\tData 0.002 (0.335)\tLoss 0.8028 (0.6748)\tSoftmaxLoss 0.7539 (0.6246)\tRankLoss 0.0489 (0.0502)\tPrec@1 49.219 (66.258)\n",
            "Time: Wed Mar 30 23:03:36 2022\n",
            "Step: 1630\t Epoch: [10][140/149]\tTime 0.515 (0.838)\tData 0.002 (0.336)\tLoss 0.6045 (0.6757)\tSoftmaxLoss 0.5545 (0.6255)\tRankLoss 0.0500 (0.0502)\tPrec@1 64.844 (65.752)\n",
            "Time: Wed Mar 30 23:03:46 2022\n",
            "Test: [0/11]\tTime 3.138 (3.138)\tSoftmaxLoss 0.4474 (0.4474)\tPrec@1 90.625 (90.625)\n",
            " * Prec@1 77.676\n",
            "Time: Wed Mar 30 23:04:02 2022\n",
            "Step: 1639\t Epoch: [11][0/149]\tTime 2.287 (2.287)\tData 1.767 (1.767)\tLoss 0.7054 (0.7054)\tSoftmaxLoss 0.6555 (0.6555)\tRankLoss 0.0499 (0.0499)\tPrec@1 64.844 (64.844)\n",
            "Time: Wed Mar 30 23:04:20 2022\n",
            "Step: 1659\t Epoch: [11][20/149]\tTime 1.179 (0.922)\tData 0.675 (0.412)\tLoss 0.5731 (0.6760)\tSoftmaxLoss 0.5226 (0.6257)\tRankLoss 0.0505 (0.0503)\tPrec@1 76.562 (65.067)\n",
            "Time: Wed Mar 30 23:04:36 2022\n",
            "Step: 1679\t Epoch: [11][40/149]\tTime 0.499 (0.871)\tData 0.002 (0.363)\tLoss 0.5387 (0.6474)\tSoftmaxLoss 0.4885 (0.5972)\tRankLoss 0.0502 (0.0502)\tPrec@1 79.688 (68.388)\n",
            "Time: Wed Mar 30 23:04:52 2022\n",
            "Step: 1699\t Epoch: [11][60/149]\tTime 0.502 (0.854)\tData 0.003 (0.348)\tLoss 0.5433 (0.6551)\tSoftmaxLoss 0.4934 (0.6049)\tRankLoss 0.0499 (0.0502)\tPrec@1 73.438 (67.687)\n",
            "Time: Wed Mar 30 23:05:08 2022\n",
            "Step: 1719\t Epoch: [11][80/149]\tTime 0.518 (0.841)\tData 0.002 (0.336)\tLoss 0.5428 (0.6425)\tSoftmaxLoss 0.4930 (0.5923)\tRankLoss 0.0498 (0.0502)\tPrec@1 81.250 (68.856)\n",
            "Time: Wed Mar 30 23:05:25 2022\n",
            "Step: 1739\t Epoch: [11][100/149]\tTime 1.322 (0.838)\tData 0.825 (0.333)\tLoss 0.5620 (0.6478)\tSoftmaxLoss 0.5114 (0.5976)\tRankLoss 0.0506 (0.0502)\tPrec@1 78.125 (68.920)\n",
            "Time: Wed Mar 30 23:05:41 2022\n",
            "Step: 1759\t Epoch: [11][120/149]\tTime 1.319 (0.833)\tData 0.816 (0.329)\tLoss 0.5161 (0.6379)\tSoftmaxLoss 0.4662 (0.5877)\tRankLoss 0.0499 (0.0502)\tPrec@1 89.062 (69.641)\n",
            "Time: Wed Mar 30 23:05:57 2022\n",
            "Step: 1779\t Epoch: [11][140/149]\tTime 0.504 (0.831)\tData 0.002 (0.328)\tLoss 0.5302 (0.6422)\tSoftmaxLoss 0.4800 (0.5919)\tRankLoss 0.0502 (0.0502)\tPrec@1 78.125 (69.454)\n",
            "Time: Wed Mar 30 23:06:06 2022\n",
            "Test: [0/11]\tTime 3.197 (3.197)\tSoftmaxLoss 0.4400 (0.4400)\tPrec@1 84.375 (84.375)\n",
            " * Prec@1 67.584\n",
            "Time: Wed Mar 30 23:06:23 2022\n",
            "Step: 1788\t Epoch: [12][0/149]\tTime 2.387 (2.387)\tData 1.883 (1.883)\tLoss 0.4966 (0.4966)\tSoftmaxLoss 0.4469 (0.4469)\tRankLoss 0.0497 (0.0497)\tPrec@1 80.469 (80.469)\n",
            "Time: Wed Mar 30 23:06:40 2022\n",
            "Step: 1808\t Epoch: [12][20/149]\tTime 0.975 (0.914)\tData 0.477 (0.408)\tLoss 0.5179 (0.6441)\tSoftmaxLoss 0.4677 (0.5940)\tRankLoss 0.0502 (0.0500)\tPrec@1 77.344 (68.936)\n",
            "Time: Wed Mar 30 23:06:56 2022\n",
            "Step: 1828\t Epoch: [12][40/149]\tTime 0.927 (0.861)\tData 0.418 (0.358)\tLoss 0.6175 (0.6101)\tSoftmaxLoss 0.5677 (0.5600)\tRankLoss 0.0498 (0.0501)\tPrec@1 84.375 (71.456)\n",
            "Time: Wed Mar 30 23:07:13 2022\n",
            "Step: 1848\t Epoch: [12][60/149]\tTime 1.262 (0.867)\tData 0.767 (0.363)\tLoss 0.6077 (0.6184)\tSoftmaxLoss 0.5575 (0.5683)\tRankLoss 0.0502 (0.0501)\tPrec@1 57.812 (70.018)\n",
            "Time: Wed Mar 30 23:07:28 2022\n",
            "Step: 1868\t Epoch: [12][80/149]\tTime 0.516 (0.841)\tData 0.009 (0.338)\tLoss 0.3971 (0.6159)\tSoftmaxLoss 0.3467 (0.5657)\tRankLoss 0.0504 (0.0502)\tPrec@1 96.094 (70.573)\n",
            "Time: Wed Mar 30 23:07:45 2022\n",
            "Step: 1888\t Epoch: [12][100/149]\tTime 0.521 (0.837)\tData 0.007 (0.334)\tLoss 0.6267 (0.6230)\tSoftmaxLoss 0.5762 (0.5729)\tRankLoss 0.0505 (0.0502)\tPrec@1 74.219 (70.026)\n",
            "Time: Wed Mar 30 23:08:01 2022\n",
            "Step: 1908\t Epoch: [12][120/149]\tTime 0.510 (0.828)\tData 0.002 (0.325)\tLoss 0.4850 (0.6246)\tSoftmaxLoss 0.4345 (0.5745)\tRankLoss 0.0505 (0.0502)\tPrec@1 84.375 (69.635)\n",
            "Time: Wed Mar 30 23:08:17 2022\n",
            "Step: 1928\t Epoch: [12][140/149]\tTime 0.690 (0.828)\tData 0.184 (0.325)\tLoss 0.7687 (0.6220)\tSoftmaxLoss 0.7191 (0.5718)\tRankLoss 0.0496 (0.0502)\tPrec@1 54.688 (69.509)\n",
            "Time: Wed Mar 30 23:08:27 2022\n",
            "Test: [0/11]\tTime 3.136 (3.136)\tSoftmaxLoss 0.3987 (0.3987)\tPrec@1 81.250 (81.250)\n",
            " * Prec@1 74.618\n",
            "Time: Wed Mar 30 23:08:43 2022\n",
            "Step: 1937\t Epoch: [13][0/149]\tTime 2.393 (2.393)\tData 1.876 (1.876)\tLoss 0.6187 (0.6187)\tSoftmaxLoss 0.5680 (0.5680)\tRankLoss 0.0508 (0.0508)\tPrec@1 75.781 (75.781)\n",
            "Time: Wed Mar 30 23:09:01 2022\n",
            "Step: 1957\t Epoch: [13][20/149]\tTime 0.618 (0.976)\tData 0.117 (0.468)\tLoss 0.9452 (0.5749)\tSoftmaxLoss 0.8961 (0.5246)\tRankLoss 0.0491 (0.0503)\tPrec@1 46.094 (73.847)\n",
            "Time: Wed Mar 30 23:09:18 2022\n",
            "Step: 1977\t Epoch: [13][40/149]\tTime 1.037 (0.918)\tData 0.534 (0.413)\tLoss 0.5878 (0.5959)\tSoftmaxLoss 0.5371 (0.5457)\tRankLoss 0.0506 (0.0502)\tPrec@1 75.000 (72.275)\n",
            "Time: Wed Mar 30 23:09:33 2022\n",
            "Step: 1997\t Epoch: [13][60/149]\tTime 0.818 (0.866)\tData 0.325 (0.362)\tLoss 0.7045 (0.5990)\tSoftmaxLoss 0.6559 (0.5488)\tRankLoss 0.0486 (0.0502)\tPrec@1 74.219 (72.298)\n",
            "Time: Wed Mar 30 23:09:50 2022\n",
            "Step: 2017\t Epoch: [13][80/149]\tTime 1.207 (0.861)\tData 0.702 (0.357)\tLoss 0.5651 (0.5980)\tSoftmaxLoss 0.5152 (0.5477)\tRankLoss 0.0498 (0.0503)\tPrec@1 67.969 (72.753)\n",
            "Time: Wed Mar 30 23:10:07 2022\n",
            "Step: 2037\t Epoch: [13][100/149]\tTime 0.763 (0.850)\tData 0.255 (0.346)\tLoss 0.6047 (0.5943)\tSoftmaxLoss 0.5538 (0.5440)\tRankLoss 0.0509 (0.0503)\tPrec@1 67.969 (72.401)\n",
            "Time: Wed Mar 30 23:10:23 2022\n",
            "Step: 2057\t Epoch: [13][120/149]\tTime 0.610 (0.844)\tData 0.100 (0.340)\tLoss 0.6823 (0.5905)\tSoftmaxLoss 0.6323 (0.5402)\tRankLoss 0.0500 (0.0503)\tPrec@1 64.062 (72.824)\n",
            "Time: Wed Mar 30 23:10:39 2022\n",
            "Step: 2077\t Epoch: [13][140/149]\tTime 0.775 (0.837)\tData 0.260 (0.333)\tLoss 0.4538 (0.5856)\tSoftmaxLoss 0.4039 (0.5353)\tRankLoss 0.0498 (0.0503)\tPrec@1 73.438 (73.127)\n",
            "Time: Wed Mar 30 23:10:48 2022\n",
            "Test: [0/11]\tTime 3.202 (3.202)\tSoftmaxLoss 0.3606 (0.3606)\tPrec@1 87.500 (87.500)\n",
            " * Prec@1 76.147\n",
            "Time: Wed Mar 30 23:11:05 2022\n",
            "Step: 2086\t Epoch: [14][0/149]\tTime 2.257 (2.257)\tData 1.760 (1.760)\tLoss 0.4808 (0.4808)\tSoftmaxLoss 0.4304 (0.4304)\tRankLoss 0.0504 (0.0504)\tPrec@1 79.688 (79.688)\n",
            "Time: Wed Mar 30 23:11:23 2022\n",
            "Step: 2106\t Epoch: [14][20/149]\tTime 1.080 (0.989)\tData 0.568 (0.482)\tLoss 0.4538 (0.5267)\tSoftmaxLoss 0.4031 (0.4764)\tRankLoss 0.0507 (0.0503)\tPrec@1 82.812 (74.777)\n",
            "Time: Wed Mar 30 23:11:40 2022\n",
            "Step: 2126\t Epoch: [14][40/149]\tTime 1.261 (0.921)\tData 0.752 (0.416)\tLoss 0.3946 (0.5361)\tSoftmaxLoss 0.3445 (0.4858)\tRankLoss 0.0501 (0.0503)\tPrec@1 85.938 (73.800)\n",
            "Time: Wed Mar 30 23:11:57 2022\n",
            "Step: 2146\t Epoch: [14][60/149]\tTime 0.816 (0.888)\tData 0.306 (0.383)\tLoss 0.3298 (0.5475)\tSoftmaxLoss 0.2795 (0.4973)\tRankLoss 0.0503 (0.0502)\tPrec@1 100.000 (74.501)\n",
            "Time: Wed Mar 30 23:12:13 2022\n",
            "Step: 2166\t Epoch: [14][80/149]\tTime 1.400 (0.869)\tData 0.909 (0.365)\tLoss 0.4871 (0.5386)\tSoftmaxLoss 0.4367 (0.4884)\tRankLoss 0.0504 (0.0502)\tPrec@1 81.250 (75.125)\n",
            "Time: Wed Mar 30 23:12:28 2022\n",
            "Step: 2186\t Epoch: [14][100/149]\tTime 0.847 (0.850)\tData 0.337 (0.346)\tLoss 0.6625 (0.5414)\tSoftmaxLoss 0.6122 (0.4911)\tRankLoss 0.0503 (0.0503)\tPrec@1 56.250 (75.116)\n",
            "Time: Wed Mar 30 23:12:45 2022\n",
            "Step: 2206\t Epoch: [14][120/149]\tTime 1.296 (0.846)\tData 0.794 (0.342)\tLoss 0.4989 (0.5409)\tSoftmaxLoss 0.4482 (0.4906)\tRankLoss 0.0508 (0.0503)\tPrec@1 77.344 (75.465)\n",
            "Time: Wed Mar 30 23:13:01 2022\n",
            "Step: 2226\t Epoch: [14][140/149]\tTime 0.505 (0.841)\tData 0.002 (0.338)\tLoss 0.6471 (0.5428)\tSoftmaxLoss 0.5967 (0.4925)\tRankLoss 0.0505 (0.0503)\tPrec@1 82.031 (75.227)\n",
            "Time: Wed Mar 30 23:13:10 2022\n",
            "Test: [0/11]\tTime 3.114 (3.114)\tSoftmaxLoss 0.4040 (0.4040)\tPrec@1 87.500 (87.500)\n",
            " * Prec@1 76.147\n",
            "Time: Wed Mar 30 23:13:26 2022\n",
            "Step: 2235\t Epoch: [15][0/149]\tTime 2.435 (2.435)\tData 1.924 (1.924)\tLoss 0.6930 (0.6930)\tSoftmaxLoss 0.6431 (0.6431)\tRankLoss 0.0499 (0.0499)\tPrec@1 63.281 (63.281)\n",
            "Time: Wed Mar 30 23:13:44 2022\n",
            "Step: 2255\t Epoch: [15][20/149]\tTime 0.505 (0.946)\tData 0.002 (0.438)\tLoss 0.5412 (0.4920)\tSoftmaxLoss 0.4911 (0.4417)\tRankLoss 0.0501 (0.0503)\tPrec@1 85.156 (78.460)\n",
            "Time: Wed Mar 30 23:14:02 2022\n",
            "Step: 2275\t Epoch: [15][40/149]\tTime 1.277 (0.917)\tData 0.763 (0.412)\tLoss 0.3847 (0.4549)\tSoftmaxLoss 0.3349 (0.4046)\tRankLoss 0.0498 (0.0503)\tPrec@1 92.969 (80.316)\n",
            "Time: Wed Mar 30 23:14:17 2022\n",
            "Step: 2295\t Epoch: [15][60/149]\tTime 1.476 (0.865)\tData 0.979 (0.361)\tLoss 0.7014 (0.5117)\tSoftmaxLoss 0.6511 (0.4613)\tRankLoss 0.0503 (0.0503)\tPrec@1 75.000 (78.010)\n",
            "Time: Wed Mar 30 23:14:33 2022\n",
            "Step: 2315\t Epoch: [15][80/149]\tTime 1.044 (0.849)\tData 0.550 (0.345)\tLoss 0.6962 (0.5392)\tSoftmaxLoss 0.6460 (0.4890)\tRankLoss 0.0502 (0.0503)\tPrec@1 56.250 (76.370)\n",
            "Time: Wed Mar 30 23:14:50 2022\n",
            "Step: 2335\t Epoch: [15][100/149]\tTime 1.780 (0.850)\tData 1.279 (0.346)\tLoss 0.6655 (0.5337)\tSoftmaxLoss 0.6151 (0.4835)\tRankLoss 0.0504 (0.0502)\tPrec@1 75.000 (76.833)\n",
            "Time: Wed Mar 30 23:15:06 2022\n",
            "Step: 2355\t Epoch: [15][120/149]\tTime 0.585 (0.840)\tData 0.079 (0.336)\tLoss 0.3427 (0.5385)\tSoftmaxLoss 0.2931 (0.4883)\tRankLoss 0.0496 (0.0503)\tPrec@1 95.312 (76.782)\n",
            "Time: Wed Mar 30 23:15:23 2022\n",
            "Step: 2375\t Epoch: [15][140/149]\tTime 0.509 (0.844)\tData 0.002 (0.341)\tLoss 0.5609 (0.5285)\tSoftmaxLoss 0.5106 (0.4783)\tRankLoss 0.0503 (0.0502)\tPrec@1 67.188 (77.250)\n",
            "Time: Wed Mar 30 23:15:32 2022\n",
            "Test: [0/11]\tTime 3.203 (3.203)\tSoftmaxLoss 0.3439 (0.3439)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 73.394\n",
            "Time: Wed Mar 30 23:15:48 2022\n",
            "Step: 2384\t Epoch: [16][0/149]\tTime 2.483 (2.483)\tData 1.964 (1.964)\tLoss 0.8493 (0.8493)\tSoftmaxLoss 0.7998 (0.7998)\tRankLoss 0.0496 (0.0496)\tPrec@1 53.125 (53.125)\n",
            "Time: Wed Mar 30 23:16:07 2022\n",
            "Step: 2404\t Epoch: [16][20/149]\tTime 0.858 (0.988)\tData 0.349 (0.477)\tLoss 0.6916 (0.4514)\tSoftmaxLoss 0.6408 (0.4011)\tRankLoss 0.0508 (0.0503)\tPrec@1 69.531 (81.622)\n",
            "Time: Wed Mar 30 23:16:23 2022\n",
            "Step: 2424\t Epoch: [16][40/149]\tTime 0.796 (0.893)\tData 0.297 (0.387)\tLoss 0.2619 (0.4516)\tSoftmaxLoss 0.2117 (0.4013)\tRankLoss 0.0502 (0.0503)\tPrec@1 98.438 (81.669)\n",
            "Time: Wed Mar 30 23:16:38 2022\n",
            "Step: 2444\t Epoch: [16][60/149]\tTime 0.505 (0.861)\tData 0.002 (0.355)\tLoss 0.6328 (0.4943)\tSoftmaxLoss 0.5824 (0.4440)\tRankLoss 0.0504 (0.0503)\tPrec@1 77.344 (79.380)\n",
            "Time: Wed Mar 30 23:16:55 2022\n",
            "Step: 2464\t Epoch: [16][80/149]\tTime 1.011 (0.857)\tData 0.518 (0.352)\tLoss 0.5321 (0.4887)\tSoftmaxLoss 0.4820 (0.4384)\tRankLoss 0.0501 (0.0503)\tPrec@1 78.906 (79.774)\n",
            "Time: Wed Mar 30 23:17:11 2022\n",
            "Step: 2484\t Epoch: [16][100/149]\tTime 0.943 (0.839)\tData 0.441 (0.334)\tLoss 0.5029 (0.4873)\tSoftmaxLoss 0.4527 (0.4370)\tRankLoss 0.0502 (0.0503)\tPrec@1 78.906 (79.989)\n",
            "Time: Wed Mar 30 23:17:28 2022\n",
            "Step: 2504\t Epoch: [16][120/149]\tTime 1.116 (0.844)\tData 0.625 (0.340)\tLoss 0.2835 (0.4734)\tSoftmaxLoss 0.2331 (0.4232)\tRankLoss 0.0505 (0.0503)\tPrec@1 99.219 (80.520)\n",
            "Time: Wed Mar 30 23:17:43 2022\n",
            "Step: 2524\t Epoch: [16][140/149]\tTime 0.832 (0.834)\tData 0.333 (0.329)\tLoss 0.2856 (0.4686)\tSoftmaxLoss 0.2351 (0.4183)\tRankLoss 0.0505 (0.0503)\tPrec@1 89.062 (80.873)\n",
            "Time: Wed Mar 30 23:17:53 2022\n",
            "Test: [0/11]\tTime 3.187 (3.187)\tSoftmaxLoss 0.3660 (0.3660)\tPrec@1 87.500 (87.500)\n",
            " * Prec@1 72.171\n",
            "Time: Wed Mar 30 23:18:08 2022\n",
            "Step: 2533\t Epoch: [17][0/149]\tTime 1.630 (1.630)\tData 1.135 (1.135)\tLoss 0.2004 (0.2004)\tSoftmaxLoss 0.1502 (0.1502)\tRankLoss 0.0502 (0.0502)\tPrec@1 94.531 (94.531)\n",
            "Time: Wed Mar 30 23:18:26 2022\n",
            "Step: 2553\t Epoch: [17][20/149]\tTime 1.525 (0.917)\tData 1.014 (0.410)\tLoss 0.5154 (0.4598)\tSoftmaxLoss 0.4650 (0.4094)\tRankLoss 0.0504 (0.0504)\tPrec@1 84.375 (82.999)\n",
            "Time: Wed Mar 30 23:18:41 2022\n",
            "Step: 2573\t Epoch: [17][40/149]\tTime 0.890 (0.845)\tData 0.385 (0.339)\tLoss 0.4962 (0.4888)\tSoftmaxLoss 0.4453 (0.4385)\tRankLoss 0.0509 (0.0503)\tPrec@1 71.875 (80.050)\n",
            "Time: Wed Mar 30 23:18:58 2022\n",
            "Step: 2593\t Epoch: [17][60/149]\tTime 0.914 (0.843)\tData 0.409 (0.337)\tLoss 0.3913 (0.5144)\tSoftmaxLoss 0.3414 (0.4642)\tRankLoss 0.0499 (0.0503)\tPrec@1 88.281 (77.523)\n",
            "Time: Wed Mar 30 23:19:13 2022\n",
            "Step: 2613\t Epoch: [17][80/149]\tTime 0.503 (0.822)\tData 0.007 (0.316)\tLoss 0.3057 (0.4958)\tSoftmaxLoss 0.2554 (0.4455)\tRankLoss 0.0503 (0.0503)\tPrec@1 96.875 (78.954)\n",
            "Time: Wed Mar 30 23:19:31 2022\n",
            "Step: 2633\t Epoch: [17][100/149]\tTime 0.512 (0.831)\tData 0.010 (0.326)\tLoss 0.5462 (0.4899)\tSoftmaxLoss 0.4956 (0.4396)\tRankLoss 0.0506 (0.0503)\tPrec@1 75.781 (79.370)\n",
            "Time: Wed Mar 30 23:19:47 2022\n",
            "Step: 2653\t Epoch: [17][120/149]\tTime 0.628 (0.826)\tData 0.118 (0.322)\tLoss 0.3617 (0.4910)\tSoftmaxLoss 0.3114 (0.4407)\tRankLoss 0.0503 (0.0503)\tPrec@1 81.250 (79.145)\n",
            "Time: Wed Mar 30 23:20:03 2022\n",
            "Step: 2673\t Epoch: [17][140/149]\tTime 1.734 (0.827)\tData 1.242 (0.323)\tLoss 0.5996 (0.4857)\tSoftmaxLoss 0.5500 (0.4354)\tRankLoss 0.0496 (0.0503)\tPrec@1 64.062 (79.095)\n",
            "Time: Wed Mar 30 23:20:13 2022\n",
            "Test: [0/11]\tTime 3.185 (3.185)\tSoftmaxLoss 0.2820 (0.2820)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 78.287\n",
            "Time: Wed Mar 30 23:20:29 2022\n",
            "Step: 2682\t Epoch: [18][0/149]\tTime 2.453 (2.453)\tData 1.933 (1.933)\tLoss 0.2588 (0.2588)\tSoftmaxLoss 0.2084 (0.2084)\tRankLoss 0.0504 (0.0504)\tPrec@1 96.875 (96.875)\n",
            "Time: Wed Mar 30 23:20:48 2022\n",
            "Step: 2702\t Epoch: [18][20/149]\tTime 1.716 (1.000)\tData 1.225 (0.496)\tLoss 0.4372 (0.5595)\tSoftmaxLoss 0.3873 (0.5092)\tRankLoss 0.0499 (0.0503)\tPrec@1 71.094 (74.554)\n",
            "Time: Wed Mar 30 23:21:04 2022\n",
            "Step: 2722\t Epoch: [18][40/149]\tTime 0.955 (0.917)\tData 0.463 (0.413)\tLoss 0.5339 (0.5366)\tSoftmaxLoss 0.4834 (0.4864)\tRankLoss 0.0505 (0.0502)\tPrec@1 75.781 (74.790)\n",
            "Time: Wed Mar 30 23:21:20 2022\n",
            "Step: 2742\t Epoch: [18][60/149]\tTime 0.515 (0.866)\tData 0.005 (0.361)\tLoss 0.4282 (0.5001)\tSoftmaxLoss 0.3784 (0.4499)\tRankLoss 0.0499 (0.0502)\tPrec@1 82.812 (77.100)\n",
            "Time: Wed Mar 30 23:21:36 2022\n",
            "Step: 2762\t Epoch: [18][80/149]\tTime 0.518 (0.851)\tData 0.002 (0.348)\tLoss 0.4313 (0.4648)\tSoftmaxLoss 0.3809 (0.4146)\tRankLoss 0.0504 (0.0502)\tPrec@1 79.688 (79.379)\n",
            "Time: Wed Mar 30 23:21:52 2022\n",
            "Step: 2782\t Epoch: [18][100/149]\tTime 0.712 (0.844)\tData 0.212 (0.341)\tLoss 0.1715 (0.4533)\tSoftmaxLoss 0.1214 (0.4030)\tRankLoss 0.0501 (0.0503)\tPrec@1 100.000 (80.476)\n",
            "Time: Wed Mar 30 23:22:09 2022\n",
            "Step: 2802\t Epoch: [18][120/149]\tTime 0.677 (0.845)\tData 0.177 (0.342)\tLoss 0.4269 (0.4570)\tSoftmaxLoss 0.3767 (0.4066)\tRankLoss 0.0501 (0.0503)\tPrec@1 82.031 (80.495)\n",
            "Time: Wed Mar 30 23:22:26 2022\n",
            "Step: 2822\t Epoch: [18][140/149]\tTime 1.051 (0.843)\tData 0.542 (0.339)\tLoss 0.3808 (0.4502)\tSoftmaxLoss 0.3303 (0.3999)\tRankLoss 0.0505 (0.0504)\tPrec@1 87.500 (81.233)\n",
            "Time: Wed Mar 30 23:22:35 2022\n",
            "Test: [0/11]\tTime 3.205 (3.205)\tSoftmaxLoss 0.1482 (0.1482)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 81.651\n",
            "Time: Wed Mar 30 23:22:51 2022\n",
            "Step: 2831\t Epoch: [19][0/149]\tTime 1.806 (1.806)\tData 1.292 (1.292)\tLoss 0.3588 (0.3588)\tSoftmaxLoss 0.3082 (0.3082)\tRankLoss 0.0506 (0.0506)\tPrec@1 82.031 (82.031)\n",
            "Time: Wed Mar 30 23:23:09 2022\n",
            "Step: 2851\t Epoch: [19][20/149]\tTime 0.938 (0.932)\tData 0.439 (0.424)\tLoss 0.3801 (0.3500)\tSoftmaxLoss 0.3293 (0.2995)\tRankLoss 0.0508 (0.0505)\tPrec@1 79.688 (85.677)\n",
            "Time: Wed Mar 30 23:23:25 2022\n",
            "Step: 2871\t Epoch: [19][40/149]\tTime 1.411 (0.877)\tData 0.907 (0.368)\tLoss 0.7062 (0.4428)\tSoftmaxLoss 0.6552 (0.3922)\tRankLoss 0.0510 (0.0506)\tPrec@1 69.531 (81.345)\n",
            "Time: Wed Mar 30 23:23:42 2022\n",
            "Step: 2891\t Epoch: [19][60/149]\tTime 1.103 (0.862)\tData 0.603 (0.355)\tLoss 0.4325 (0.4444)\tSoftmaxLoss 0.3826 (0.3939)\tRankLoss 0.0499 (0.0505)\tPrec@1 76.562 (81.609)\n",
            "Time: Wed Mar 30 23:23:58 2022\n",
            "Step: 2911\t Epoch: [19][80/149]\tTime 1.405 (0.853)\tData 0.912 (0.347)\tLoss 0.3857 (0.4482)\tSoftmaxLoss 0.3356 (0.3978)\tRankLoss 0.0502 (0.0504)\tPrec@1 85.938 (81.462)\n",
            "Time: Wed Mar 30 23:24:16 2022\n",
            "Step: 2931\t Epoch: [19][100/149]\tTime 1.659 (0.861)\tData 1.158 (0.356)\tLoss 0.5532 (0.4483)\tSoftmaxLoss 0.5029 (0.3979)\tRankLoss 0.0504 (0.0504)\tPrec@1 70.312 (81.412)\n",
            "Time: Wed Mar 30 23:24:31 2022\n",
            "Step: 2951\t Epoch: [19][120/149]\tTime 1.391 (0.845)\tData 0.897 (0.341)\tLoss 0.3186 (0.4481)\tSoftmaxLoss 0.2679 (0.3977)\tRankLoss 0.0507 (0.0505)\tPrec@1 91.406 (81.573)\n",
            "Time: Wed Mar 30 23:24:47 2022\n",
            "Step: 2971\t Epoch: [19][140/149]\tTime 0.961 (0.836)\tData 0.462 (0.332)\tLoss 0.4532 (0.4351)\tSoftmaxLoss 0.4027 (0.3847)\tRankLoss 0.0505 (0.0504)\tPrec@1 79.688 (82.214)\n",
            "Time: Wed Mar 30 23:24:56 2022\n",
            "Test: [0/11]\tTime 3.105 (3.105)\tSoftmaxLoss 0.1710 (0.1710)\tPrec@1 100.000 (100.000)\n",
            " * Prec@1 78.593\n",
            "Time: Wed Mar 30 23:25:12 2022\n",
            "Step: 2980\t Epoch: [20][0/149]\tTime 2.150 (2.150)\tData 1.647 (1.647)\tLoss 0.4969 (0.4969)\tSoftmaxLoss 0.4466 (0.4466)\tRankLoss 0.0503 (0.0503)\tPrec@1 76.562 (76.562)\n",
            "Time: Wed Mar 30 23:25:31 2022\n",
            "Step: 3000\t Epoch: [20][20/149]\tTime 0.786 (0.976)\tData 0.286 (0.470)\tLoss 0.3754 (0.4079)\tSoftmaxLoss 0.3251 (0.3575)\tRankLoss 0.0503 (0.0504)\tPrec@1 87.500 (81.771)\n",
            "Time: Wed Mar 30 23:25:47 2022\n",
            "Step: 3020\t Epoch: [20][40/149]\tTime 1.103 (0.888)\tData 0.598 (0.383)\tLoss 0.4668 (0.4404)\tSoftmaxLoss 0.4165 (0.3900)\tRankLoss 0.0503 (0.0504)\tPrec@1 78.906 (82.222)\n",
            "Time: Wed Mar 30 23:26:04 2022\n",
            "Step: 3040\t Epoch: [20][60/149]\tTime 1.356 (0.876)\tData 0.847 (0.372)\tLoss 0.4561 (0.4410)\tSoftmaxLoss 0.4059 (0.3907)\tRankLoss 0.0502 (0.0503)\tPrec@1 86.719 (82.441)\n",
            "Time: Wed Mar 30 23:26:20 2022\n",
            "Step: 3060\t Epoch: [20][80/149]\tTime 0.880 (0.857)\tData 0.380 (0.353)\tLoss 0.2969 (0.4345)\tSoftmaxLoss 0.2466 (0.3841)\tRankLoss 0.0502 (0.0504)\tPrec@1 84.375 (82.620)\n",
            "Time: Wed Mar 30 23:26:36 2022\n",
            "Step: 3080\t Epoch: [20][100/149]\tTime 0.504 (0.846)\tData 0.004 (0.342)\tLoss 0.4374 (0.4181)\tSoftmaxLoss 0.3872 (0.3678)\tRankLoss 0.0503 (0.0504)\tPrec@1 73.438 (83.431)\n",
            "Time: Wed Mar 30 23:26:52 2022\n",
            "Step: 3100\t Epoch: [20][120/149]\tTime 0.500 (0.840)\tData 0.002 (0.336)\tLoss 0.2188 (0.4087)\tSoftmaxLoss 0.1686 (0.3583)\tRankLoss 0.0502 (0.0504)\tPrec@1 94.531 (83.968)\n",
            "Time: Wed Mar 30 23:27:09 2022\n",
            "Step: 3120\t Epoch: [20][140/149]\tTime 0.505 (0.841)\tData 0.003 (0.337)\tLoss 0.3751 (0.4222)\tSoftmaxLoss 0.3249 (0.3718)\tRankLoss 0.0502 (0.0504)\tPrec@1 85.156 (83.289)\n",
            "Time: Wed Mar 30 23:27:18 2022\n",
            "Test: [0/11]\tTime 3.125 (3.125)\tSoftmaxLoss 0.2873 (0.2873)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 77.064\n",
            "Time: Wed Mar 30 23:27:35 2022\n",
            "Step: 3129\t Epoch: [21][0/149]\tTime 2.710 (2.710)\tData 2.209 (2.209)\tLoss 0.5339 (0.5339)\tSoftmaxLoss 0.4835 (0.4835)\tRankLoss 0.0505 (0.0505)\tPrec@1 78.906 (78.906)\n",
            "Time: Wed Mar 30 23:27:53 2022\n",
            "Step: 3149\t Epoch: [21][20/149]\tTime 1.118 (1.015)\tData 0.626 (0.510)\tLoss 0.4861 (0.4322)\tSoftmaxLoss 0.4359 (0.3819)\tRankLoss 0.0502 (0.0503)\tPrec@1 74.219 (80.357)\n",
            "Time: Wed Mar 30 23:28:09 2022\n",
            "Step: 3169\t Epoch: [21][40/149]\tTime 0.581 (0.896)\tData 0.080 (0.392)\tLoss 0.5824 (0.4175)\tSoftmaxLoss 0.5316 (0.3672)\tRankLoss 0.0508 (0.0503)\tPrec@1 83.594 (83.327)\n",
            "Time: Wed Mar 30 23:28:26 2022\n",
            "Step: 3189\t Epoch: [21][60/149]\tTime 1.301 (0.886)\tData 0.788 (0.378)\tLoss 0.1999 (0.4087)\tSoftmaxLoss 0.1498 (0.3584)\tRankLoss 0.0501 (0.0503)\tPrec@1 100.000 (83.478)\n",
            "Time: Wed Mar 30 23:28:41 2022\n",
            "Step: 3209\t Epoch: [21][80/149]\tTime 0.997 (0.852)\tData 0.486 (0.346)\tLoss 0.3093 (0.3968)\tSoftmaxLoss 0.2584 (0.3464)\tRankLoss 0.0509 (0.0503)\tPrec@1 83.594 (84.028)\n",
            "Time: Wed Mar 30 23:28:58 2022\n",
            "Step: 3229\t Epoch: [21][100/149]\tTime 1.189 (0.849)\tData 0.694 (0.343)\tLoss 0.5469 (0.3999)\tSoftmaxLoss 0.4962 (0.3496)\tRankLoss 0.0507 (0.0503)\tPrec@1 71.094 (83.926)\n",
            "Time: Wed Mar 30 23:29:13 2022\n",
            "Step: 3249\t Epoch: [21][120/149]\tTime 0.513 (0.837)\tData 0.007 (0.332)\tLoss 0.3150 (0.3919)\tSoftmaxLoss 0.2648 (0.3415)\tRankLoss 0.0503 (0.0504)\tPrec@1 86.719 (84.465)\n",
            "Time: Wed Mar 30 23:29:30 2022\n",
            "Step: 3269\t Epoch: [21][140/149]\tTime 0.494 (0.836)\tData 0.002 (0.330)\tLoss 0.3314 (0.3791)\tSoftmaxLoss 0.2805 (0.3287)\tRankLoss 0.0509 (0.0504)\tPrec@1 80.469 (85.289)\n",
            "Time: Wed Mar 30 23:29:39 2022\n",
            "Test: [0/11]\tTime 3.185 (3.185)\tSoftmaxLoss 0.6198 (0.6198)\tPrec@1 84.375 (84.375)\n",
            " * Prec@1 73.394\n",
            "Time: Wed Mar 30 23:29:55 2022\n",
            "Step: 3278\t Epoch: [22][0/149]\tTime 1.948 (1.948)\tData 1.429 (1.429)\tLoss 0.0928 (0.0928)\tSoftmaxLoss 0.0427 (0.0427)\tRankLoss 0.0501 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Wed Mar 30 23:30:12 2022\n",
            "Step: 3298\t Epoch: [22][20/149]\tTime 1.107 (0.890)\tData 0.596 (0.383)\tLoss 0.2109 (0.2971)\tSoftmaxLoss 0.1605 (0.2466)\tRankLoss 0.0504 (0.0505)\tPrec@1 94.531 (88.132)\n",
            "Time: Wed Mar 30 23:30:28 2022\n",
            "Step: 3318\t Epoch: [22][40/149]\tTime 0.508 (0.852)\tData 0.002 (0.348)\tLoss 0.2584 (0.3142)\tSoftmaxLoss 0.2083 (0.2638)\tRankLoss 0.0501 (0.0504)\tPrec@1 90.625 (87.462)\n",
            "Time: Wed Mar 30 23:30:46 2022\n",
            "Step: 3338\t Epoch: [22][60/149]\tTime 0.510 (0.857)\tData 0.003 (0.352)\tLoss 0.3482 (0.3687)\tSoftmaxLoss 0.2977 (0.3181)\tRankLoss 0.0505 (0.0505)\tPrec@1 89.062 (85.643)\n",
            "Time: Wed Mar 30 23:31:02 2022\n",
            "Step: 3358\t Epoch: [22][80/149]\tTime 0.522 (0.843)\tData 0.002 (0.339)\tLoss 0.2600 (0.3994)\tSoftmaxLoss 0.2100 (0.3489)\tRankLoss 0.0500 (0.0505)\tPrec@1 91.406 (84.028)\n",
            "Time: Wed Mar 30 23:31:19 2022\n",
            "Step: 3378\t Epoch: [22][100/149]\tTime 0.496 (0.843)\tData 0.002 (0.339)\tLoss 0.3719 (0.3985)\tSoftmaxLoss 0.3214 (0.3480)\tRankLoss 0.0505 (0.0505)\tPrec@1 81.250 (84.429)\n",
            "Time: Wed Mar 30 23:31:35 2022\n",
            "Step: 3398\t Epoch: [22][120/149]\tTime 0.514 (0.842)\tData 0.007 (0.338)\tLoss 0.3503 (0.3915)\tSoftmaxLoss 0.2998 (0.3411)\tRankLoss 0.0505 (0.0504)\tPrec@1 85.938 (85.059)\n",
            "Time: Wed Mar 30 23:31:52 2022\n",
            "Step: 3418\t Epoch: [22][140/149]\tTime 0.525 (0.841)\tData 0.009 (0.337)\tLoss 0.2792 (0.3937)\tSoftmaxLoss 0.2290 (0.3433)\tRankLoss 0.0502 (0.0504)\tPrec@1 89.062 (84.874)\n",
            "Time: Wed Mar 30 23:32:02 2022\n",
            "Test: [0/11]\tTime 3.222 (3.222)\tSoftmaxLoss 0.2152 (0.2152)\tPrec@1 90.625 (90.625)\n",
            " * Prec@1 76.758\n",
            "Time: Wed Mar 30 23:32:17 2022\n",
            "Step: 3427\t Epoch: [23][0/149]\tTime 1.837 (1.837)\tData 1.333 (1.333)\tLoss 0.4241 (0.4241)\tSoftmaxLoss 0.3735 (0.3735)\tRankLoss 0.0506 (0.0506)\tPrec@1 80.469 (80.469)\n",
            "Time: Wed Mar 30 23:32:34 2022\n",
            "Step: 3447\t Epoch: [23][20/149]\tTime 0.522 (0.896)\tData 0.006 (0.388)\tLoss 0.2673 (0.4921)\tSoftmaxLoss 0.2166 (0.4419)\tRankLoss 0.0507 (0.0502)\tPrec@1 92.188 (78.571)\n",
            "Time: Wed Mar 30 23:32:50 2022\n",
            "Step: 3467\t Epoch: [23][40/149]\tTime 0.507 (0.846)\tData 0.002 (0.341)\tLoss 0.1832 (0.3998)\tSoftmaxLoss 0.1329 (0.3496)\tRankLoss 0.0503 (0.0502)\tPrec@1 100.000 (84.356)\n",
            "Time: Wed Mar 30 23:33:08 2022\n",
            "Step: 3487\t Epoch: [23][60/149]\tTime 0.505 (0.854)\tData 0.002 (0.349)\tLoss 0.1804 (0.3709)\tSoftmaxLoss 0.1293 (0.3206)\tRankLoss 0.0511 (0.0503)\tPrec@1 96.094 (85.963)\n",
            "Time: Wed Mar 30 23:33:24 2022\n",
            "Step: 3507\t Epoch: [23][80/149]\tTime 0.499 (0.839)\tData 0.002 (0.335)\tLoss 0.4203 (0.3676)\tSoftmaxLoss 0.3696 (0.3172)\tRankLoss 0.0507 (0.0504)\tPrec@1 82.031 (85.793)\n",
            "Time: Wed Mar 30 23:33:40 2022\n",
            "Step: 3527\t Epoch: [23][100/149]\tTime 0.497 (0.833)\tData 0.002 (0.330)\tLoss 0.3819 (0.3655)\tSoftmaxLoss 0.3306 (0.3151)\tRankLoss 0.0513 (0.0504)\tPrec@1 78.906 (85.705)\n",
            "Time: Wed Mar 30 23:33:56 2022\n",
            "Step: 3547\t Epoch: [23][120/149]\tTime 0.507 (0.833)\tData 0.002 (0.330)\tLoss 0.2618 (0.3758)\tSoftmaxLoss 0.2117 (0.3254)\tRankLoss 0.0501 (0.0504)\tPrec@1 93.750 (85.105)\n",
            "Time: Wed Mar 30 23:34:13 2022\n",
            "Step: 3567\t Epoch: [23][140/149]\tTime 1.053 (0.832)\tData 0.556 (0.329)\tLoss 0.3078 (0.3726)\tSoftmaxLoss 0.2576 (0.3222)\tRankLoss 0.0503 (0.0504)\tPrec@1 94.531 (85.710)\n",
            "Time: Wed Mar 30 23:34:22 2022\n",
            "Test: [0/11]\tTime 3.258 (3.258)\tSoftmaxLoss 0.4740 (0.4740)\tPrec@1 71.875 (71.875)\n",
            " * Prec@1 71.254\n",
            "Time: Wed Mar 30 23:34:38 2022\n",
            "Step: 3576\t Epoch: [24][0/149]\tTime 1.835 (1.835)\tData 1.322 (1.322)\tLoss 0.2609 (0.2609)\tSoftmaxLoss 0.2106 (0.2106)\tRankLoss 0.0503 (0.0503)\tPrec@1 95.312 (95.312)\n",
            "Time: Wed Mar 30 23:34:57 2022\n",
            "Step: 3596\t Epoch: [24][20/149]\tTime 1.567 (0.955)\tData 1.056 (0.442)\tLoss 0.3794 (0.3101)\tSoftmaxLoss 0.3292 (0.2598)\tRankLoss 0.0502 (0.0503)\tPrec@1 85.156 (88.839)\n",
            "Time: Wed Mar 30 23:35:12 2022\n",
            "Step: 3616\t Epoch: [24][40/149]\tTime 1.131 (0.866)\tData 0.637 (0.356)\tLoss 0.4614 (0.3182)\tSoftmaxLoss 0.4109 (0.2678)\tRankLoss 0.0505 (0.0503)\tPrec@1 80.469 (87.862)\n",
            "Time: Wed Mar 30 23:35:28 2022\n",
            "Step: 3636\t Epoch: [24][60/149]\tTime 0.520 (0.847)\tData 0.008 (0.338)\tLoss 0.4470 (0.3446)\tSoftmaxLoss 0.3965 (0.2942)\tRankLoss 0.0506 (0.0503)\tPrec@1 68.750 (86.245)\n",
            "Time: Wed Mar 30 23:35:45 2022\n",
            "Step: 3656\t Epoch: [24][80/149]\tTime 1.107 (0.851)\tData 0.607 (0.344)\tLoss 0.7734 (0.3478)\tSoftmaxLoss 0.7233 (0.2975)\tRankLoss 0.0501 (0.0504)\tPrec@1 71.094 (86.101)\n",
            "Time: Wed Mar 30 23:36:01 2022\n",
            "Step: 3676\t Epoch: [24][100/149]\tTime 0.992 (0.838)\tData 0.493 (0.332)\tLoss 0.8899 (0.3596)\tSoftmaxLoss 0.8392 (0.3092)\tRankLoss 0.0507 (0.0504)\tPrec@1 72.656 (85.992)\n",
            "Time: Wed Mar 30 23:36:17 2022\n",
            "Step: 3696\t Epoch: [24][120/149]\tTime 1.180 (0.834)\tData 0.680 (0.328)\tLoss 0.2080 (0.3562)\tSoftmaxLoss 0.1577 (0.3058)\tRankLoss 0.0503 (0.0504)\tPrec@1 95.312 (85.873)\n",
            "Time: Wed Mar 30 23:36:34 2022\n",
            "Step: 3716\t Epoch: [24][140/149]\tTime 0.504 (0.830)\tData 0.002 (0.324)\tLoss 0.1862 (0.3433)\tSoftmaxLoss 0.1361 (0.2929)\tRankLoss 0.0500 (0.0504)\tPrec@1 95.312 (86.381)\n",
            "Time: Wed Mar 30 23:36:43 2022\n",
            "Test: [0/11]\tTime 3.126 (3.126)\tSoftmaxLoss 0.2543 (0.2543)\tPrec@1 90.625 (90.625)\n",
            " * Prec@1 77.676\n",
            "Time: Wed Mar 30 23:36:59 2022\n",
            "Step: 3725\t Epoch: [25][0/149]\tTime 2.011 (2.011)\tData 1.513 (1.513)\tLoss 0.3785 (0.3785)\tSoftmaxLoss 0.3283 (0.3283)\tRankLoss 0.0502 (0.0502)\tPrec@1 82.031 (82.031)\n",
            "Time: Wed Mar 30 23:37:17 2022\n",
            "Step: 3745\t Epoch: [25][20/149]\tTime 0.986 (0.956)\tData 0.486 (0.446)\tLoss 0.4017 (0.3689)\tSoftmaxLoss 0.3513 (0.3184)\tRankLoss 0.0504 (0.0505)\tPrec@1 78.125 (84.747)\n",
            "Time: Wed Mar 30 23:37:33 2022\n",
            "Step: 3765\t Epoch: [25][40/149]\tTime 0.805 (0.873)\tData 0.307 (0.367)\tLoss 0.1239 (0.3529)\tSoftmaxLoss 0.0738 (0.3022)\tRankLoss 0.0501 (0.0507)\tPrec@1 99.219 (85.213)\n",
            "Time: Wed Mar 30 23:37:49 2022\n",
            "Step: 3785\t Epoch: [25][60/149]\tTime 1.676 (0.861)\tData 1.171 (0.356)\tLoss 0.5303 (0.3677)\tSoftmaxLoss 0.4802 (0.3172)\tRankLoss 0.0501 (0.0506)\tPrec@1 74.219 (85.272)\n",
            "Time: Wed Mar 30 23:38:06 2022\n",
            "Step: 3805\t Epoch: [25][80/149]\tTime 1.269 (0.859)\tData 0.776 (0.354)\tLoss 0.6719 (0.3648)\tSoftmaxLoss 0.6215 (0.3142)\tRankLoss 0.0503 (0.0506)\tPrec@1 87.500 (85.590)\n",
            "Time: Wed Mar 30 23:38:22 2022\n",
            "Step: 3825\t Epoch: [25][100/149]\tTime 0.903 (0.848)\tData 0.402 (0.343)\tLoss 0.3465 (0.3509)\tSoftmaxLoss 0.2955 (0.3003)\tRankLoss 0.0509 (0.0505)\tPrec@1 87.500 (85.945)\n",
            "Time: Wed Mar 30 23:38:38 2022\n",
            "Step: 3845\t Epoch: [25][120/149]\tTime 1.046 (0.839)\tData 0.541 (0.335)\tLoss 0.5861 (0.3578)\tSoftmaxLoss 0.5354 (0.3073)\tRankLoss 0.0507 (0.0505)\tPrec@1 75.781 (85.938)\n",
            "Time: Wed Mar 30 23:38:55 2022\n",
            "Step: 3865\t Epoch: [25][140/149]\tTime 0.986 (0.838)\tData 0.477 (0.334)\tLoss 0.2496 (0.3481)\tSoftmaxLoss 0.1981 (0.2976)\tRankLoss 0.0515 (0.0505)\tPrec@1 91.406 (86.741)\n",
            "Time: Wed Mar 30 23:39:05 2022\n",
            "Test: [0/11]\tTime 3.220 (3.220)\tSoftmaxLoss 0.4253 (0.4253)\tPrec@1 84.375 (84.375)\n",
            " * Prec@1 68.807\n",
            "Time: Wed Mar 30 23:39:21 2022\n",
            "Step: 3874\t Epoch: [26][0/149]\tTime 2.058 (2.058)\tData 1.559 (1.559)\tLoss 0.2346 (0.2346)\tSoftmaxLoss 0.1831 (0.1831)\tRankLoss 0.0514 (0.0514)\tPrec@1 96.875 (96.875)\n",
            "Time: Wed Mar 30 23:39:39 2022\n",
            "Step: 3894\t Epoch: [26][20/149]\tTime 1.076 (0.956)\tData 0.570 (0.449)\tLoss 0.2743 (0.3432)\tSoftmaxLoss 0.2242 (0.2926)\tRankLoss 0.0502 (0.0506)\tPrec@1 91.406 (87.128)\n",
            "Time: Wed Mar 30 23:39:54 2022\n",
            "Step: 3914\t Epoch: [26][40/149]\tTime 0.916 (0.868)\tData 0.421 (0.363)\tLoss 0.2032 (0.3310)\tSoftmaxLoss 0.1531 (0.2804)\tRankLoss 0.0501 (0.0505)\tPrec@1 99.219 (87.481)\n",
            "Time: Wed Mar 30 23:40:11 2022\n",
            "Step: 3934\t Epoch: [26][60/149]\tTime 1.037 (0.854)\tData 0.531 (0.351)\tLoss 0.2540 (0.3562)\tSoftmaxLoss 0.2038 (0.3056)\tRankLoss 0.0503 (0.0506)\tPrec@1 89.844 (86.360)\n",
            "Time: Wed Mar 30 23:40:28 2022\n",
            "Step: 3954\t Epoch: [26][80/149]\tTime 1.369 (0.857)\tData 0.870 (0.354)\tLoss 0.3600 (0.3564)\tSoftmaxLoss 0.3099 (0.3059)\tRankLoss 0.0501 (0.0505)\tPrec@1 78.906 (85.889)\n",
            "Time: Wed Mar 30 23:40:44 2022\n",
            "Step: 3974\t Epoch: [26][100/149]\tTime 0.997 (0.846)\tData 0.496 (0.343)\tLoss 0.4690 (0.3616)\tSoftmaxLoss 0.4180 (0.3111)\tRankLoss 0.0510 (0.0505)\tPrec@1 79.688 (85.636)\n",
            "Time: Wed Mar 30 23:41:00 2022\n",
            "Step: 3994\t Epoch: [26][120/149]\tTime 0.926 (0.836)\tData 0.420 (0.333)\tLoss 0.2764 (0.3568)\tSoftmaxLoss 0.2260 (0.3063)\tRankLoss 0.0505 (0.0505)\tPrec@1 85.938 (85.996)\n",
            "Time: Wed Mar 30 23:41:17 2022\n",
            "Step: 4014\t Epoch: [26][140/149]\tTime 0.792 (0.839)\tData 0.291 (0.335)\tLoss 0.3456 (0.3483)\tSoftmaxLoss 0.2925 (0.2978)\tRankLoss 0.0531 (0.0505)\tPrec@1 83.594 (86.641)\n",
            "Time: Wed Mar 30 23:41:26 2022\n",
            "Test: [0/11]\tTime 3.324 (3.324)\tSoftmaxLoss 0.0787 (0.0787)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 81.346\n",
            "Time: Wed Mar 30 23:41:41 2022\n",
            "Step: 4023\t Epoch: [27][0/149]\tTime 1.707 (1.707)\tData 1.201 (1.201)\tLoss 0.3382 (0.3382)\tSoftmaxLoss 0.2880 (0.2880)\tRankLoss 0.0503 (0.0503)\tPrec@1 81.250 (81.250)\n",
            "Time: Wed Mar 30 23:42:00 2022\n",
            "Step: 4043\t Epoch: [27][20/149]\tTime 1.659 (0.954)\tData 1.167 (0.448)\tLoss 0.1320 (0.3141)\tSoftmaxLoss 0.0817 (0.2636)\tRankLoss 0.0502 (0.0504)\tPrec@1 100.000 (88.914)\n",
            "Time: Wed Mar 30 23:42:16 2022\n",
            "Step: 4063\t Epoch: [27][40/149]\tTime 1.025 (0.890)\tData 0.521 (0.387)\tLoss 0.2928 (0.2846)\tSoftmaxLoss 0.2426 (0.2342)\tRankLoss 0.0502 (0.0504)\tPrec@1 81.250 (90.187)\n",
            "Time: Wed Mar 30 23:42:31 2022\n",
            "Step: 4083\t Epoch: [27][60/149]\tTime 0.609 (0.847)\tData 0.109 (0.344)\tLoss 0.0959 (0.3059)\tSoftmaxLoss 0.0458 (0.2554)\tRankLoss 0.0501 (0.0505)\tPrec@1 100.000 (89.152)\n",
            "Time: Wed Mar 30 23:42:48 2022\n",
            "Step: 4103\t Epoch: [27][80/149]\tTime 1.205 (0.850)\tData 0.696 (0.347)\tLoss 0.3761 (0.3170)\tSoftmaxLoss 0.3255 (0.2666)\tRankLoss 0.0506 (0.0504)\tPrec@1 81.250 (89.178)\n",
            "Time: Wed Mar 30 23:43:04 2022\n",
            "Step: 4123\t Epoch: [27][100/149]\tTime 1.094 (0.836)\tData 0.589 (0.334)\tLoss 0.1864 (0.3164)\tSoftmaxLoss 0.1363 (0.2660)\tRankLoss 0.0502 (0.0505)\tPrec@1 100.000 (89.364)\n",
            "Time: Wed Mar 30 23:43:21 2022\n",
            "Step: 4143\t Epoch: [27][120/149]\tTime 0.762 (0.840)\tData 0.266 (0.338)\tLoss 0.2598 (0.3273)\tSoftmaxLoss 0.2097 (0.2769)\tRankLoss 0.0501 (0.0504)\tPrec@1 90.625 (88.837)\n",
            "Time: Wed Mar 30 23:43:37 2022\n",
            "Step: 4163\t Epoch: [27][140/149]\tTime 1.062 (0.829)\tData 0.570 (0.327)\tLoss 0.3070 (0.3275)\tSoftmaxLoss 0.2565 (0.2771)\tRankLoss 0.0505 (0.0504)\tPrec@1 92.969 (88.846)\n",
            "Time: Wed Mar 30 23:43:46 2022\n",
            "Test: [0/11]\tTime 3.308 (3.308)\tSoftmaxLoss 0.3355 (0.3355)\tPrec@1 81.250 (81.250)\n",
            " * Prec@1 75.841\n",
            "Time: Wed Mar 30 23:44:03 2022\n",
            "Step: 4172\t Epoch: [28][0/149]\tTime 2.201 (2.201)\tData 1.692 (1.692)\tLoss 0.1903 (0.1903)\tSoftmaxLoss 0.1401 (0.1401)\tRankLoss 0.0502 (0.0502)\tPrec@1 94.531 (94.531)\n",
            "Time: Wed Mar 30 23:44:20 2022\n",
            "Step: 4192\t Epoch: [28][20/149]\tTime 1.157 (0.929)\tData 0.657 (0.422)\tLoss 0.2605 (0.2545)\tSoftmaxLoss 0.2103 (0.2043)\tRankLoss 0.0502 (0.0502)\tPrec@1 84.375 (90.513)\n",
            "Time: Wed Mar 30 23:44:37 2022\n",
            "Step: 4212\t Epoch: [28][40/149]\tTime 0.511 (0.885)\tData 0.002 (0.380)\tLoss 0.2313 (0.2630)\tSoftmaxLoss 0.1774 (0.2125)\tRankLoss 0.0539 (0.0504)\tPrec@1 92.969 (89.958)\n",
            "Time: Wed Mar 30 23:44:53 2022\n",
            "Step: 4232\t Epoch: [28][60/149]\tTime 0.494 (0.862)\tData 0.003 (0.358)\tLoss 0.3902 (0.2681)\tSoftmaxLoss 0.3396 (0.2176)\tRankLoss 0.0507 (0.0505)\tPrec@1 81.250 (89.626)\n",
            "Time: Wed Mar 30 23:45:10 2022\n",
            "Step: 4252\t Epoch: [28][80/149]\tTime 0.515 (0.851)\tData 0.002 (0.346)\tLoss 0.3023 (0.2711)\tSoftmaxLoss 0.2520 (0.2206)\tRankLoss 0.0503 (0.0505)\tPrec@1 87.500 (89.786)\n",
            "Time: Wed Mar 30 23:45:26 2022\n",
            "Step: 4272\t Epoch: [28][100/149]\tTime 0.508 (0.846)\tData 0.002 (0.342)\tLoss 0.2445 (0.2686)\tSoftmaxLoss 0.1940 (0.2181)\tRankLoss 0.0505 (0.0505)\tPrec@1 85.156 (90.068)\n",
            "Time: Wed Mar 30 23:45:43 2022\n",
            "Step: 4292\t Epoch: [28][120/149]\tTime 0.518 (0.844)\tData 0.009 (0.340)\tLoss 0.1629 (0.2841)\tSoftmaxLoss 0.1127 (0.2335)\tRankLoss 0.0502 (0.0506)\tPrec@1 92.969 (89.437)\n",
            "Time: Wed Mar 30 23:45:59 2022\n",
            "Step: 4312\t Epoch: [28][140/149]\tTime 0.512 (0.840)\tData 0.002 (0.337)\tLoss 0.5692 (0.2917)\tSoftmaxLoss 0.5178 (0.2411)\tRankLoss 0.0514 (0.0506)\tPrec@1 85.938 (89.018)\n",
            "Time: Wed Mar 30 23:46:09 2022\n",
            "Test: [0/11]\tTime 3.145 (3.145)\tSoftmaxLoss 0.2212 (0.2212)\tPrec@1 87.500 (87.500)\n",
            " * Prec@1 77.064\n",
            "Time: Wed Mar 30 23:46:25 2022\n",
            "Step: 4321\t Epoch: [29][0/149]\tTime 2.436 (2.436)\tData 1.936 (1.936)\tLoss 0.2728 (0.2728)\tSoftmaxLoss 0.2223 (0.2223)\tRankLoss 0.0505 (0.0505)\tPrec@1 92.188 (92.188)\n",
            "Time: Wed Mar 30 23:46:43 2022\n",
            "Step: 4341\t Epoch: [29][20/149]\tTime 0.948 (0.953)\tData 0.446 (0.448)\tLoss 0.2003 (0.3740)\tSoftmaxLoss 0.1502 (0.3234)\tRankLoss 0.0501 (0.0506)\tPrec@1 98.438 (85.751)\n",
            "Time: Wed Mar 30 23:47:00 2022\n",
            "Step: 4361\t Epoch: [29][40/149]\tTime 1.237 (0.894)\tData 0.726 (0.389)\tLoss 0.4006 (0.3556)\tSoftmaxLoss 0.3499 (0.3051)\tRankLoss 0.0508 (0.0505)\tPrec@1 79.688 (87.100)\n",
            "Time: Wed Mar 30 23:47:16 2022\n",
            "Step: 4381\t Epoch: [29][60/149]\tTime 0.518 (0.866)\tData 0.003 (0.361)\tLoss 0.1470 (0.3059)\tSoftmaxLoss 0.0965 (0.2554)\tRankLoss 0.0504 (0.0505)\tPrec@1 100.000 (89.383)\n",
            "Time: Wed Mar 30 23:47:32 2022\n",
            "Step: 4401\t Epoch: [29][80/149]\tTime 0.514 (0.846)\tData 0.002 (0.341)\tLoss 0.5346 (0.2909)\tSoftmaxLoss 0.4845 (0.2405)\tRankLoss 0.0501 (0.0504)\tPrec@1 82.031 (89.940)\n",
            "Time: Wed Mar 30 23:47:48 2022\n",
            "Step: 4421\t Epoch: [29][100/149]\tTime 0.516 (0.845)\tData 0.008 (0.340)\tLoss 0.1933 (0.2903)\tSoftmaxLoss 0.1432 (0.2398)\tRankLoss 0.0501 (0.0504)\tPrec@1 88.281 (89.797)\n",
            "Time: Wed Mar 30 23:48:04 2022\n",
            "Step: 4441\t Epoch: [29][120/149]\tTime 0.512 (0.834)\tData 0.004 (0.330)\tLoss 0.3547 (0.2824)\tSoftmaxLoss 0.3019 (0.2319)\tRankLoss 0.0529 (0.0504)\tPrec@1 84.375 (90.147)\n",
            "Time: Wed Mar 30 23:48:20 2022\n",
            "Step: 4461\t Epoch: [29][140/149]\tTime 0.990 (0.827)\tData 0.485 (0.323)\tLoss 0.2672 (0.2905)\tSoftmaxLoss 0.2170 (0.2401)\tRankLoss 0.0502 (0.0504)\tPrec@1 92.969 (89.733)\n",
            "Time: Wed Mar 30 23:48:29 2022\n",
            "Test: [0/11]\tTime 3.243 (3.243)\tSoftmaxLoss 0.0539 (0.0539)\tPrec@1 100.000 (100.000)\n",
            " * Prec@1 82.263\n",
            "Time: Wed Mar 30 23:48:45 2022\n",
            "Step: 4470\t Epoch: [30][0/149]\tTime 1.879 (1.879)\tData 1.372 (1.372)\tLoss 0.3152 (0.3152)\tSoftmaxLoss 0.2645 (0.2645)\tRankLoss 0.0508 (0.0508)\tPrec@1 87.500 (87.500)\n",
            "Time: Wed Mar 30 23:49:04 2022\n",
            "Step: 4490\t Epoch: [30][20/149]\tTime 1.416 (0.970)\tData 0.924 (0.462)\tLoss 0.2808 (0.2896)\tSoftmaxLoss 0.2304 (0.2391)\tRankLoss 0.0504 (0.0505)\tPrec@1 90.625 (88.951)\n",
            "Time: Wed Mar 30 23:49:19 2022\n",
            "Step: 4510\t Epoch: [30][40/149]\tTime 1.072 (0.881)\tData 0.566 (0.373)\tLoss 0.4145 (0.2755)\tSoftmaxLoss 0.3640 (0.2250)\tRankLoss 0.0505 (0.0505)\tPrec@1 75.781 (89.596)\n",
            "Time: Wed Mar 30 23:49:36 2022\n",
            "Step: 4530\t Epoch: [30][60/149]\tTime 1.103 (0.858)\tData 0.597 (0.353)\tLoss 0.3021 (0.2704)\tSoftmaxLoss 0.2513 (0.2199)\tRankLoss 0.0508 (0.0506)\tPrec@1 83.594 (90.126)\n",
            "Time: Wed Mar 30 23:49:51 2022\n",
            "Step: 4550\t Epoch: [30][80/149]\tTime 0.657 (0.842)\tData 0.162 (0.337)\tLoss 0.1010 (0.2836)\tSoftmaxLoss 0.0498 (0.2330)\tRankLoss 0.0511 (0.0506)\tPrec@1 100.000 (89.988)\n",
            "Time: Wed Mar 30 23:50:08 2022\n",
            "Step: 4570\t Epoch: [30][100/149]\tTime 0.907 (0.838)\tData 0.410 (0.334)\tLoss 0.3172 (0.2740)\tSoftmaxLoss 0.2663 (0.2234)\tRankLoss 0.0509 (0.0506)\tPrec@1 85.938 (90.795)\n",
            "Time: Wed Mar 30 23:50:24 2022\n",
            "Step: 4590\t Epoch: [30][120/149]\tTime 1.211 (0.829)\tData 0.706 (0.325)\tLoss 0.1795 (0.2578)\tSoftmaxLoss 0.1292 (0.2072)\tRankLoss 0.0503 (0.0505)\tPrec@1 89.844 (91.393)\n",
            "Time: Wed Mar 30 23:50:39 2022\n",
            "Step: 4610\t Epoch: [30][140/149]\tTime 0.725 (0.823)\tData 0.215 (0.318)\tLoss 0.2058 (0.2612)\tSoftmaxLoss 0.1555 (0.2107)\tRankLoss 0.0503 (0.0506)\tPrec@1 93.750 (91.356)\n",
            "Time: Wed Mar 30 23:50:48 2022\n",
            "Test: [0/11]\tTime 3.161 (3.161)\tSoftmaxLoss 0.5011 (0.5011)\tPrec@1 84.375 (84.375)\n",
            " * Prec@1 74.618\n",
            "Time: Wed Mar 30 23:51:05 2022\n",
            "Step: 4619\t Epoch: [31][0/149]\tTime 2.350 (2.350)\tData 1.852 (1.852)\tLoss 0.1365 (0.1365)\tSoftmaxLoss 0.0853 (0.0853)\tRankLoss 0.0512 (0.0512)\tPrec@1 98.438 (98.438)\n",
            "Time: Wed Mar 30 23:51:22 2022\n",
            "Step: 4639\t Epoch: [31][20/149]\tTime 1.167 (0.963)\tData 0.669 (0.456)\tLoss 0.5227 (0.3288)\tSoftmaxLoss 0.4723 (0.2780)\tRankLoss 0.0504 (0.0508)\tPrec@1 78.125 (87.091)\n",
            "Time: Wed Mar 30 23:51:39 2022\n",
            "Step: 4659\t Epoch: [31][40/149]\tTime 1.310 (0.903)\tData 0.813 (0.400)\tLoss 0.3222 (0.3717)\tSoftmaxLoss 0.2718 (0.3210)\tRankLoss 0.0504 (0.0507)\tPrec@1 86.719 (85.899)\n",
            "Time: Wed Mar 30 23:51:55 2022\n",
            "Step: 4679\t Epoch: [31][60/149]\tTime 0.502 (0.871)\tData 0.002 (0.369)\tLoss 0.5487 (0.3736)\tSoftmaxLoss 0.4969 (0.3230)\tRankLoss 0.0518 (0.0506)\tPrec@1 74.219 (85.886)\n",
            "Time: Wed Mar 30 23:52:12 2022\n",
            "Step: 4699\t Epoch: [31][80/149]\tTime 1.337 (0.855)\tData 0.840 (0.351)\tLoss 0.1290 (0.3629)\tSoftmaxLoss 0.0787 (0.3124)\tRankLoss 0.0503 (0.0505)\tPrec@1 100.000 (86.699)\n",
            "Time: Wed Mar 30 23:52:29 2022\n",
            "Step: 4719\t Epoch: [31][100/149]\tTime 1.204 (0.858)\tData 0.710 (0.355)\tLoss 0.2606 (0.3276)\tSoftmaxLoss 0.2101 (0.2771)\tRankLoss 0.0505 (0.0505)\tPrec@1 88.281 (88.544)\n",
            "Time: Wed Mar 30 23:52:45 2022\n",
            "Step: 4739\t Epoch: [31][120/149]\tTime 1.151 (0.851)\tData 0.647 (0.348)\tLoss 0.1457 (0.3191)\tSoftmaxLoss 0.0954 (0.2687)\tRankLoss 0.0503 (0.0505)\tPrec@1 98.438 (89.205)\n",
            "Time: Wed Mar 30 23:53:03 2022\n",
            "Step: 4759\t Epoch: [31][140/149]\tTime 1.511 (0.853)\tData 1.009 (0.350)\tLoss 0.2641 (0.3124)\tSoftmaxLoss 0.2140 (0.2619)\tRankLoss 0.0501 (0.0505)\tPrec@1 89.844 (89.340)\n",
            "Time: Wed Mar 30 23:53:12 2022\n",
            "Test: [0/11]\tTime 3.168 (3.168)\tSoftmaxLoss 0.2715 (0.2715)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 82.263\n",
            "Time: Wed Mar 30 23:53:28 2022\n",
            "Step: 4768\t Epoch: [32][0/149]\tTime 1.958 (1.958)\tData 1.436 (1.436)\tLoss 0.2624 (0.2624)\tSoftmaxLoss 0.2120 (0.2120)\tRankLoss 0.0504 (0.0504)\tPrec@1 85.938 (85.938)\n",
            "Time: Wed Mar 30 23:53:45 2022\n",
            "Step: 4788\t Epoch: [32][20/149]\tTime 0.503 (0.922)\tData 0.006 (0.415)\tLoss 0.0918 (0.2399)\tSoftmaxLoss 0.0417 (0.1896)\tRankLoss 0.0501 (0.0503)\tPrec@1 100.000 (91.853)\n",
            "Time: Wed Mar 30 23:54:02 2022\n",
            "Step: 4808\t Epoch: [32][40/149]\tTime 0.497 (0.867)\tData 0.003 (0.361)\tLoss 0.0900 (0.2488)\tSoftmaxLoss 0.0372 (0.1981)\tRankLoss 0.0528 (0.0507)\tPrec@1 100.000 (92.397)\n",
            "Time: Wed Mar 30 23:54:19 2022\n",
            "Step: 4828\t Epoch: [32][60/149]\tTime 0.502 (0.862)\tData 0.003 (0.357)\tLoss 0.1847 (0.2737)\tSoftmaxLoss 0.1328 (0.2228)\tRankLoss 0.0519 (0.0508)\tPrec@1 99.219 (91.342)\n",
            "Time: Wed Mar 30 23:54:34 2022\n",
            "Step: 4848\t Epoch: [32][80/149]\tTime 0.497 (0.842)\tData 0.002 (0.338)\tLoss 0.1191 (0.2858)\tSoftmaxLoss 0.0688 (0.2351)\tRankLoss 0.0503 (0.0507)\tPrec@1 100.000 (90.721)\n",
            "Time: Wed Mar 30 23:54:50 2022\n",
            "Step: 4868\t Epoch: [32][100/149]\tTime 0.610 (0.830)\tData 0.111 (0.325)\tLoss 0.7237 (0.2835)\tSoftmaxLoss 0.6731 (0.2328)\tRankLoss 0.0506 (0.0507)\tPrec@1 64.844 (90.571)\n",
            "Time: Wed Mar 30 23:55:06 2022\n",
            "Step: 4888\t Epoch: [32][120/149]\tTime 0.506 (0.827)\tData 0.002 (0.322)\tLoss 0.1861 (0.2867)\tSoftmaxLoss 0.1360 (0.2360)\tRankLoss 0.0501 (0.0507)\tPrec@1 100.000 (90.457)\n",
            "Time: Wed Mar 30 23:55:23 2022\n",
            "Step: 4908\t Epoch: [32][140/149]\tTime 0.503 (0.827)\tData 0.002 (0.323)\tLoss 0.6702 (0.2855)\tSoftmaxLoss 0.6198 (0.2348)\tRankLoss 0.0504 (0.0507)\tPrec@1 74.219 (90.392)\n",
            "Time: Wed Mar 30 23:55:32 2022\n",
            "Test: [0/11]\tTime 3.143 (3.143)\tSoftmaxLoss 0.4981 (0.4981)\tPrec@1 81.250 (81.250)\n",
            " * Prec@1 69.725\n",
            "Time: Wed Mar 30 23:55:48 2022\n",
            "Step: 4917\t Epoch: [33][0/149]\tTime 1.706 (1.706)\tData 1.201 (1.201)\tLoss 0.1113 (0.1113)\tSoftmaxLoss 0.0608 (0.0608)\tRankLoss 0.0505 (0.0505)\tPrec@1 97.656 (97.656)\n",
            "Time: Wed Mar 30 23:56:07 2022\n",
            "Step: 4937\t Epoch: [33][20/149]\tTime 1.204 (0.985)\tData 0.698 (0.479)\tLoss 0.4419 (0.2471)\tSoftmaxLoss 0.3914 (0.1965)\tRankLoss 0.0505 (0.0505)\tPrec@1 76.562 (90.290)\n",
            "Time: Wed Mar 30 23:56:24 2022\n",
            "Step: 4957\t Epoch: [33][40/149]\tTime 1.081 (0.913)\tData 0.589 (0.408)\tLoss 0.1655 (0.2350)\tSoftmaxLoss 0.1153 (0.1845)\tRankLoss 0.0502 (0.0505)\tPrec@1 99.219 (91.159)\n",
            "Time: Wed Mar 30 23:56:41 2022\n",
            "Step: 4977\t Epoch: [33][60/149]\tTime 1.389 (0.888)\tData 0.883 (0.382)\tLoss 0.0792 (0.2425)\tSoftmaxLoss 0.0291 (0.1918)\tRankLoss 0.0501 (0.0506)\tPrec@1 100.000 (91.150)\n",
            "Time: Wed Mar 30 23:56:56 2022\n",
            "Step: 4997\t Epoch: [33][80/149]\tTime 0.999 (0.855)\tData 0.475 (0.350)\tLoss 0.3151 (0.2665)\tSoftmaxLoss 0.2649 (0.2158)\tRankLoss 0.0502 (0.0507)\tPrec@1 85.156 (90.432)\n",
            "Time: Wed Mar 30 23:57:12 2022\n",
            "Step: 5017\t Epoch: [33][100/149]\tTime 0.743 (0.848)\tData 0.248 (0.344)\tLoss 0.2268 (0.2755)\tSoftmaxLoss 0.1766 (0.2248)\tRankLoss 0.0502 (0.0507)\tPrec@1 89.844 (90.238)\n",
            "Time: Wed Mar 30 23:57:29 2022\n",
            "Step: 5037\t Epoch: [33][120/149]\tTime 0.968 (0.847)\tData 0.475 (0.343)\tLoss 0.3457 (0.2859)\tSoftmaxLoss 0.2953 (0.2351)\tRankLoss 0.0504 (0.0508)\tPrec@1 85.938 (90.173)\n",
            "Time: Wed Mar 30 23:57:46 2022\n",
            "Step: 5057\t Epoch: [33][140/149]\tTime 0.872 (0.848)\tData 0.366 (0.344)\tLoss 0.1766 (0.2819)\tSoftmaxLoss 0.1265 (0.2312)\tRankLoss 0.0502 (0.0507)\tPrec@1 94.531 (90.365)\n",
            "Time: Wed Mar 30 23:57:55 2022\n",
            "Test: [0/11]\tTime 3.228 (3.228)\tSoftmaxLoss 0.1755 (0.1755)\tPrec@1 90.625 (90.625)\n",
            " * Prec@1 79.817\n",
            "Time: Wed Mar 30 23:58:12 2022\n",
            "Step: 5066\t Epoch: [34][0/149]\tTime 2.354 (2.354)\tData 1.857 (1.857)\tLoss 0.2789 (0.2789)\tSoftmaxLoss 0.2284 (0.2284)\tRankLoss 0.0505 (0.0505)\tPrec@1 89.844 (89.844)\n",
            "Time: Wed Mar 30 23:58:29 2022\n",
            "Step: 5086\t Epoch: [34][20/149]\tTime 0.807 (0.932)\tData 0.306 (0.423)\tLoss 0.1893 (0.2542)\tSoftmaxLoss 0.1377 (0.2035)\tRankLoss 0.0516 (0.0508)\tPrec@1 93.750 (91.109)\n",
            "Time: Wed Mar 30 23:58:45 2022\n",
            "Step: 5106\t Epoch: [34][40/149]\tTime 1.228 (0.865)\tData 0.732 (0.359)\tLoss 0.4203 (0.2394)\tSoftmaxLoss 0.3694 (0.1887)\tRankLoss 0.0509 (0.0507)\tPrec@1 77.344 (91.768)\n",
            "Time: Wed Mar 30 23:59:01 2022\n",
            "Step: 5126\t Epoch: [34][60/149]\tTime 1.113 (0.845)\tData 0.604 (0.339)\tLoss 0.1133 (0.2284)\tSoftmaxLoss 0.0632 (0.1777)\tRankLoss 0.0501 (0.0507)\tPrec@1 100.000 (91.906)\n",
            "Time: Wed Mar 30 23:59:17 2022\n",
            "Step: 5146\t Epoch: [34][80/149]\tTime 0.515 (0.837)\tData 0.002 (0.331)\tLoss 0.4955 (0.2220)\tSoftmaxLoss 0.4452 (0.1713)\tRankLoss 0.0503 (0.0507)\tPrec@1 86.719 (92.843)\n",
            "Time: Wed Mar 30 23:59:34 2022\n",
            "Step: 5166\t Epoch: [34][100/149]\tTime 0.500 (0.840)\tData 0.002 (0.335)\tLoss 0.9417 (0.2306)\tSoftmaxLoss 0.8916 (0.1799)\tRankLoss 0.0501 (0.0507)\tPrec@1 76.562 (92.536)\n",
            "Time: Wed Mar 30 23:59:50 2022\n",
            "Step: 5186\t Epoch: [34][120/149]\tTime 0.953 (0.829)\tData 0.458 (0.324)\tLoss 0.6544 (0.2357)\tSoftmaxLoss 0.6036 (0.1850)\tRankLoss 0.0508 (0.0507)\tPrec@1 71.094 (92.478)\n",
            "Time: Thu Mar 31 00:00:06 2022\n",
            "Step: 5206\t Epoch: [34][140/149]\tTime 0.898 (0.827)\tData 0.390 (0.322)\tLoss 0.1763 (0.2316)\tSoftmaxLoss 0.1259 (0.1809)\tRankLoss 0.0504 (0.0507)\tPrec@1 100.000 (92.653)\n",
            "Time: Thu Mar 31 00:00:16 2022\n",
            "Test: [0/11]\tTime 3.139 (3.139)\tSoftmaxLoss 0.1123 (0.1123)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 81.651\n",
            "Time: Thu Mar 31 00:00:31 2022\n",
            "Step: 5215\t Epoch: [35][0/149]\tTime 1.599 (1.599)\tData 1.082 (1.082)\tLoss 0.3238 (0.3238)\tSoftmaxLoss 0.2735 (0.2735)\tRankLoss 0.0503 (0.0503)\tPrec@1 85.938 (85.938)\n",
            "Time: Thu Mar 31 00:00:50 2022\n",
            "Step: 5235\t Epoch: [35][20/149]\tTime 1.363 (0.951)\tData 0.844 (0.443)\tLoss 0.2497 (0.3020)\tSoftmaxLoss 0.1994 (0.2509)\tRankLoss 0.0502 (0.0511)\tPrec@1 85.938 (89.286)\n",
            "Time: Thu Mar 31 00:01:06 2022\n",
            "Step: 5255\t Epoch: [35][40/149]\tTime 0.517 (0.880)\tData 0.002 (0.373)\tLoss 0.6970 (0.3035)\tSoftmaxLoss 0.6469 (0.2525)\tRankLoss 0.0501 (0.0509)\tPrec@1 73.438 (88.434)\n",
            "Time: Thu Mar 31 00:01:22 2022\n",
            "Step: 5275\t Epoch: [35][60/149]\tTime 0.510 (0.857)\tData 0.002 (0.350)\tLoss 0.3760 (0.2901)\tSoftmaxLoss 0.3251 (0.2393)\tRankLoss 0.0509 (0.0508)\tPrec@1 84.375 (89.524)\n",
            "Time: Thu Mar 31 00:01:38 2022\n",
            "Step: 5295\t Epoch: [35][80/149]\tTime 0.495 (0.843)\tData 0.002 (0.337)\tLoss 0.2061 (0.2787)\tSoftmaxLoss 0.1551 (0.2280)\tRankLoss 0.0510 (0.0507)\tPrec@1 89.844 (90.268)\n",
            "Time: Thu Mar 31 00:01:55 2022\n",
            "Step: 5315\t Epoch: [35][100/149]\tTime 0.494 (0.847)\tData 0.002 (0.341)\tLoss 0.1062 (0.2603)\tSoftmaxLoss 0.0561 (0.2097)\tRankLoss 0.0501 (0.0506)\tPrec@1 100.000 (91.143)\n",
            "Time: Thu Mar 31 00:02:11 2022\n",
            "Step: 5335\t Epoch: [35][120/149]\tTime 1.146 (0.840)\tData 0.638 (0.335)\tLoss 0.0677 (0.2508)\tSoftmaxLoss 0.0176 (0.2001)\tRankLoss 0.0501 (0.0506)\tPrec@1 100.000 (91.652)\n",
            "Time: Thu Mar 31 00:02:28 2022\n",
            "Step: 5355\t Epoch: [35][140/149]\tTime 1.374 (0.839)\tData 0.874 (0.334)\tLoss 0.8577 (0.2459)\tSoftmaxLoss 0.8075 (0.1953)\tRankLoss 0.0502 (0.0507)\tPrec@1 73.438 (91.955)\n",
            "Time: Thu Mar 31 00:02:37 2022\n",
            "Test: [0/11]\tTime 3.177 (3.177)\tSoftmaxLoss 0.3251 (0.3251)\tPrec@1 84.375 (84.375)\n",
            " * Prec@1 73.700\n",
            "Time: Thu Mar 31 00:02:54 2022\n",
            "Step: 5364\t Epoch: [36][0/149]\tTime 2.056 (2.056)\tData 1.553 (1.553)\tLoss 0.4823 (0.4823)\tSoftmaxLoss 0.4309 (0.4309)\tRankLoss 0.0514 (0.0514)\tPrec@1 84.375 (84.375)\n",
            "Time: Thu Mar 31 00:03:10 2022\n",
            "Step: 5384\t Epoch: [36][20/149]\tTime 0.683 (0.893)\tData 0.173 (0.384)\tLoss 0.2412 (0.2320)\tSoftmaxLoss 0.1910 (0.1812)\tRankLoss 0.0502 (0.0507)\tPrec@1 85.938 (91.518)\n",
            "Time: Thu Mar 31 00:03:27 2022\n",
            "Step: 5404\t Epoch: [36][40/149]\tTime 0.519 (0.856)\tData 0.002 (0.349)\tLoss 0.0772 (0.2364)\tSoftmaxLoss 0.0267 (0.1857)\tRankLoss 0.0505 (0.0507)\tPrec@1 100.000 (91.463)\n",
            "Time: Thu Mar 31 00:03:43 2022\n",
            "Step: 5424\t Epoch: [36][60/149]\tTime 0.689 (0.841)\tData 0.191 (0.335)\tLoss 0.1304 (0.2354)\tSoftmaxLoss 0.0798 (0.1848)\tRankLoss 0.0506 (0.0506)\tPrec@1 98.438 (91.816)\n",
            "Time: Thu Mar 31 00:03:59 2022\n",
            "Step: 5444\t Epoch: [36][80/149]\tTime 0.501 (0.830)\tData 0.002 (0.325)\tLoss 0.1421 (0.2401)\tSoftmaxLoss 0.0914 (0.1895)\tRankLoss 0.0507 (0.0506)\tPrec@1 94.531 (91.782)\n",
            "Time: Thu Mar 31 00:04:16 2022\n",
            "Step: 5464\t Epoch: [36][100/149]\tTime 0.836 (0.834)\tData 0.342 (0.330)\tLoss 0.1810 (0.2364)\tSoftmaxLoss 0.1302 (0.1858)\tRankLoss 0.0508 (0.0506)\tPrec@1 97.656 (92.427)\n",
            "Time: Thu Mar 31 00:04:32 2022\n",
            "Step: 5484\t Epoch: [36][120/149]\tTime 0.600 (0.830)\tData 0.107 (0.326)\tLoss 0.0902 (0.2321)\tSoftmaxLoss 0.0401 (0.1815)\tRankLoss 0.0500 (0.0506)\tPrec@1 100.000 (92.794)\n",
            "Time: Thu Mar 31 00:04:49 2022\n",
            "Step: 5504\t Epoch: [36][140/149]\tTime 1.174 (0.832)\tData 0.667 (0.327)\tLoss 0.4570 (0.2312)\tSoftmaxLoss 0.4061 (0.1806)\tRankLoss 0.0509 (0.0506)\tPrec@1 69.531 (92.753)\n",
            "Time: Thu Mar 31 00:04:57 2022\n",
            "Test: [0/11]\tTime 3.101 (3.101)\tSoftmaxLoss 0.2890 (0.2890)\tPrec@1 90.625 (90.625)\n",
            " * Prec@1 77.064\n",
            "Time: Thu Mar 31 00:05:14 2022\n",
            "Step: 5513\t Epoch: [37][0/149]\tTime 2.354 (2.354)\tData 1.841 (1.841)\tLoss 0.2707 (0.2707)\tSoftmaxLoss 0.2202 (0.2202)\tRankLoss 0.0505 (0.0505)\tPrec@1 92.188 (92.188)\n",
            "Time: Thu Mar 31 00:05:31 2022\n",
            "Step: 5533\t Epoch: [37][20/149]\tTime 1.397 (0.941)\tData 0.895 (0.434)\tLoss 0.1414 (0.1896)\tSoftmaxLoss 0.0912 (0.1389)\tRankLoss 0.0501 (0.0507)\tPrec@1 97.656 (93.973)\n",
            "Time: Thu Mar 31 00:05:47 2022\n",
            "Step: 5553\t Epoch: [37][40/149]\tTime 0.764 (0.874)\tData 0.262 (0.370)\tLoss 0.0630 (0.1703)\tSoftmaxLoss 0.0128 (0.1197)\tRankLoss 0.0503 (0.0506)\tPrec@1 100.000 (94.989)\n",
            "Time: Thu Mar 31 00:06:02 2022\n",
            "Step: 5573\t Epoch: [37][60/149]\tTime 1.042 (0.840)\tData 0.546 (0.335)\tLoss 0.0982 (0.1656)\tSoftmaxLoss 0.0476 (0.1149)\tRankLoss 0.0506 (0.0508)\tPrec@1 100.000 (95.146)\n",
            "Time: Thu Mar 31 00:06:20 2022\n",
            "Step: 5593\t Epoch: [37][80/149]\tTime 0.518 (0.851)\tData 0.006 (0.347)\tLoss 0.2024 (0.1677)\tSoftmaxLoss 0.1516 (0.1169)\tRankLoss 0.0508 (0.0508)\tPrec@1 87.500 (95.284)\n",
            "Time: Thu Mar 31 00:06:37 2022\n",
            "Step: 5613\t Epoch: [37][100/149]\tTime 0.504 (0.850)\tData 0.003 (0.346)\tLoss 0.0617 (0.1697)\tSoftmaxLoss 0.0102 (0.1189)\tRankLoss 0.0516 (0.0508)\tPrec@1 100.000 (95.227)\n",
            "Time: Thu Mar 31 00:06:53 2022\n",
            "Step: 5633\t Epoch: [37][120/149]\tTime 1.675 (0.844)\tData 1.165 (0.339)\tLoss 0.1289 (0.1653)\tSoftmaxLoss 0.0788 (0.1145)\tRankLoss 0.0501 (0.0508)\tPrec@1 96.875 (95.461)\n",
            "Time: Thu Mar 31 00:07:10 2022\n",
            "Step: 5653\t Epoch: [37][140/149]\tTime 1.243 (0.840)\tData 0.742 (0.335)\tLoss 0.2137 (0.1743)\tSoftmaxLoss 0.1620 (0.1235)\tRankLoss 0.0518 (0.0508)\tPrec@1 92.188 (95.030)\n",
            "Time: Thu Mar 31 00:07:19 2022\n",
            "Test: [0/11]\tTime 3.129 (3.129)\tSoftmaxLoss 0.1322 (0.1322)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 74.618\n",
            "Time: Thu Mar 31 00:07:35 2022\n",
            "Step: 5662\t Epoch: [38][0/149]\tTime 2.138 (2.138)\tData 1.625 (1.625)\tLoss 0.1357 (0.1357)\tSoftmaxLoss 0.0850 (0.0850)\tRankLoss 0.0507 (0.0507)\tPrec@1 97.656 (97.656)\n",
            "Time: Thu Mar 31 00:07:53 2022\n",
            "Step: 5682\t Epoch: [38][20/149]\tTime 1.332 (0.959)\tData 0.841 (0.451)\tLoss 0.1339 (0.3175)\tSoftmaxLoss 0.0825 (0.2662)\tRankLoss 0.0514 (0.0513)\tPrec@1 98.438 (91.332)\n",
            "Time: Thu Mar 31 00:08:09 2022\n",
            "Step: 5702\t Epoch: [38][40/149]\tTime 1.027 (0.873)\tData 0.533 (0.368)\tLoss 0.1681 (0.3374)\tSoftmaxLoss 0.1177 (0.2865)\tRankLoss 0.0504 (0.0510)\tPrec@1 98.438 (88.281)\n",
            "Time: Thu Mar 31 00:08:24 2022\n",
            "Step: 5722\t Epoch: [38][60/149]\tTime 0.509 (0.845)\tData 0.002 (0.339)\tLoss 0.1000 (0.2879)\tSoftmaxLoss 0.0499 (0.2371)\tRankLoss 0.0501 (0.0508)\tPrec@1 100.000 (90.407)\n",
            "Time: Thu Mar 31 00:08:41 2022\n",
            "Step: 5742\t Epoch: [38][80/149]\tTime 0.515 (0.842)\tData 0.013 (0.337)\tLoss 0.2343 (0.2585)\tSoftmaxLoss 0.1815 (0.2077)\tRankLoss 0.0528 (0.0508)\tPrec@1 86.719 (91.474)\n",
            "Time: Thu Mar 31 00:08:58 2022\n",
            "Step: 5762\t Epoch: [38][100/149]\tTime 0.512 (0.841)\tData 0.002 (0.337)\tLoss 0.1044 (0.2355)\tSoftmaxLoss 0.0540 (0.1847)\tRankLoss 0.0504 (0.0508)\tPrec@1 97.656 (92.543)\n",
            "Time: Thu Mar 31 00:09:14 2022\n",
            "Step: 5782\t Epoch: [38][120/149]\tTime 0.505 (0.838)\tData 0.007 (0.333)\tLoss 0.2056 (0.2252)\tSoftmaxLoss 0.1549 (0.1744)\tRankLoss 0.0507 (0.0508)\tPrec@1 98.438 (92.891)\n",
            "Time: Thu Mar 31 00:09:30 2022\n",
            "Step: 5802\t Epoch: [38][140/149]\tTime 0.504 (0.833)\tData 0.002 (0.330)\tLoss 0.2071 (0.2173)\tSoftmaxLoss 0.1569 (0.1665)\tRankLoss 0.0502 (0.0508)\tPrec@1 88.281 (93.262)\n",
            "Time: Thu Mar 31 00:09:39 2022\n",
            "Test: [0/11]\tTime 3.197 (3.197)\tSoftmaxLoss 0.0796 (0.0796)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 81.651\n",
            "Time: Thu Mar 31 00:09:55 2022\n",
            "Step: 5811\t Epoch: [39][0/149]\tTime 1.614 (1.614)\tData 1.104 (1.104)\tLoss 0.3218 (0.3218)\tSoftmaxLoss 0.2701 (0.2701)\tRankLoss 0.0516 (0.0516)\tPrec@1 87.500 (87.500)\n",
            "Time: Thu Mar 31 00:10:14 2022\n",
            "Step: 5831\t Epoch: [39][20/149]\tTime 1.519 (0.984)\tData 1.026 (0.477)\tLoss 0.2274 (0.1565)\tSoftmaxLoss 0.1774 (0.1058)\tRankLoss 0.0501 (0.0507)\tPrec@1 95.312 (97.247)\n",
            "Time: Thu Mar 31 00:10:30 2022\n",
            "Step: 5851\t Epoch: [39][40/149]\tTime 0.945 (0.883)\tData 0.450 (0.377)\tLoss 0.0783 (0.1492)\tSoftmaxLoss 0.0279 (0.0985)\tRankLoss 0.0504 (0.0507)\tPrec@1 100.000 (96.837)\n",
            "Time: Thu Mar 31 00:10:47 2022\n",
            "Step: 5871\t Epoch: [39][60/149]\tTime 0.929 (0.866)\tData 0.435 (0.361)\tLoss 0.0872 (0.1450)\tSoftmaxLoss 0.0372 (0.0942)\tRankLoss 0.0500 (0.0508)\tPrec@1 100.000 (96.824)\n",
            "Time: Thu Mar 31 00:11:03 2022\n",
            "Step: 5891\t Epoch: [39][80/149]\tTime 1.020 (0.857)\tData 0.528 (0.353)\tLoss 0.0623 (0.1639)\tSoftmaxLoss 0.0122 (0.1131)\tRankLoss 0.0500 (0.0508)\tPrec@1 100.000 (95.785)\n",
            "Time: Thu Mar 31 00:11:19 2022\n",
            "Step: 5911\t Epoch: [39][100/149]\tTime 0.538 (0.846)\tData 0.036 (0.341)\tLoss 0.6913 (0.1675)\tSoftmaxLoss 0.6370 (0.1166)\tRankLoss 0.0543 (0.0509)\tPrec@1 89.062 (95.931)\n",
            "Time: Thu Mar 31 00:11:35 2022\n",
            "Step: 5931\t Epoch: [39][120/149]\tTime 0.506 (0.840)\tData 0.002 (0.335)\tLoss 0.2091 (0.1737)\tSoftmaxLoss 0.1571 (0.1229)\tRankLoss 0.0520 (0.0508)\tPrec@1 88.281 (95.584)\n",
            "Time: Thu Mar 31 00:11:52 2022\n",
            "Step: 5951\t Epoch: [39][140/149]\tTime 0.512 (0.840)\tData 0.008 (0.335)\tLoss 0.0651 (0.1690)\tSoftmaxLoss 0.0148 (0.1182)\tRankLoss 0.0503 (0.0508)\tPrec@1 100.000 (95.728)\n",
            "Time: Thu Mar 31 00:12:01 2022\n",
            "Test: [0/11]\tTime 3.225 (3.225)\tSoftmaxLoss 0.1028 (0.1028)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 78.287\n",
            "Time: Thu Mar 31 00:12:17 2022\n",
            "Step: 5960\t Epoch: [40][0/149]\tTime 2.015 (2.015)\tData 1.502 (1.502)\tLoss 0.1729 (0.1729)\tSoftmaxLoss 0.1217 (0.1217)\tRankLoss 0.0512 (0.0512)\tPrec@1 89.844 (89.844)\n",
            "Time: Thu Mar 31 00:12:36 2022\n",
            "Step: 5980\t Epoch: [40][20/149]\tTime 0.762 (0.975)\tData 0.250 (0.466)\tLoss 0.1765 (0.2482)\tSoftmaxLoss 0.1264 (0.1974)\tRankLoss 0.0501 (0.0508)\tPrec@1 98.438 (92.857)\n",
            "Time: Thu Mar 31 00:12:52 2022\n",
            "Step: 6000\t Epoch: [40][40/149]\tTime 0.897 (0.902)\tData 0.398 (0.397)\tLoss 0.3121 (0.2892)\tSoftmaxLoss 0.2607 (0.2385)\tRankLoss 0.0514 (0.0506)\tPrec@1 81.250 (89.863)\n",
            "Time: Thu Mar 31 00:13:10 2022\n",
            "Step: 6020\t Epoch: [40][60/149]\tTime 0.518 (0.892)\tData 0.007 (0.387)\tLoss 0.1453 (0.2828)\tSoftmaxLoss 0.0952 (0.2322)\tRankLoss 0.0501 (0.0506)\tPrec@1 95.312 (91.240)\n",
            "Time: Thu Mar 31 00:13:25 2022\n",
            "Step: 6040\t Epoch: [40][80/149]\tTime 1.184 (0.865)\tData 0.673 (0.360)\tLoss 0.3133 (0.2667)\tSoftmaxLoss 0.2630 (0.2161)\tRankLoss 0.0503 (0.0506)\tPrec@1 86.719 (91.667)\n",
            "Time: Thu Mar 31 00:13:42 2022\n",
            "Step: 6060\t Epoch: [40][100/149]\tTime 1.070 (0.860)\tData 0.571 (0.355)\tLoss 0.0918 (0.2472)\tSoftmaxLoss 0.0418 (0.1966)\tRankLoss 0.0500 (0.0506)\tPrec@1 100.000 (92.180)\n",
            "Time: Thu Mar 31 00:13:59 2022\n",
            "Step: 6080\t Epoch: [40][120/149]\tTime 0.876 (0.859)\tData 0.367 (0.355)\tLoss 0.2534 (0.2360)\tSoftmaxLoss 0.2034 (0.1854)\tRankLoss 0.0501 (0.0505)\tPrec@1 86.719 (92.568)\n",
            "Time: Thu Mar 31 00:14:15 2022\n",
            "Step: 6100\t Epoch: [40][140/149]\tTime 0.997 (0.852)\tData 0.485 (0.348)\tLoss 0.1587 (0.2202)\tSoftmaxLoss 0.1083 (0.1697)\tRankLoss 0.0504 (0.0505)\tPrec@1 98.438 (93.279)\n",
            "Time: Thu Mar 31 00:14:25 2022\n",
            "Test: [0/11]\tTime 3.266 (3.266)\tSoftmaxLoss 0.1021 (0.1021)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 82.569\n",
            "Time: Thu Mar 31 00:14:42 2022\n",
            "Step: 6109\t Epoch: [41][0/149]\tTime 2.112 (2.112)\tData 1.593 (1.593)\tLoss 0.1509 (0.1509)\tSoftmaxLoss 0.1008 (0.1008)\tRankLoss 0.0500 (0.0500)\tPrec@1 98.438 (98.438)\n",
            "Time: Thu Mar 31 00:14:58 2022\n",
            "Step: 6129\t Epoch: [41][20/149]\tTime 1.314 (0.867)\tData 0.808 (0.363)\tLoss 0.2142 (0.1536)\tSoftmaxLoss 0.1630 (0.1027)\tRankLoss 0.0512 (0.0508)\tPrec@1 92.969 (94.978)\n",
            "Time: Thu Mar 31 00:15:14 2022\n",
            "Step: 6149\t Epoch: [41][40/149]\tTime 1.492 (0.844)\tData 0.985 (0.340)\tLoss 0.0874 (0.1492)\tSoftmaxLoss 0.0355 (0.0982)\tRankLoss 0.0520 (0.0510)\tPrec@1 100.000 (96.056)\n",
            "Time: Thu Mar 31 00:15:31 2022\n",
            "Step: 6169\t Epoch: [41][60/149]\tTime 1.263 (0.836)\tData 0.756 (0.332)\tLoss 0.1441 (0.1561)\tSoftmaxLoss 0.0940 (0.1052)\tRankLoss 0.0501 (0.0509)\tPrec@1 100.000 (95.825)\n",
            "Time: Thu Mar 31 00:15:46 2022\n",
            "Step: 6189\t Epoch: [41][80/149]\tTime 1.213 (0.822)\tData 0.707 (0.318)\tLoss 0.0874 (0.1739)\tSoftmaxLoss 0.0361 (0.1229)\tRankLoss 0.0512 (0.0510)\tPrec@1 100.000 (94.599)\n",
            "Time: Thu Mar 31 00:16:02 2022\n",
            "Step: 6209\t Epoch: [41][100/149]\tTime 1.173 (0.814)\tData 0.674 (0.311)\tLoss 0.1310 (0.1771)\tSoftmaxLoss 0.0805 (0.1261)\tRankLoss 0.0505 (0.0510)\tPrec@1 97.656 (94.995)\n",
            "Time: Thu Mar 31 00:16:18 2022\n",
            "Step: 6229\t Epoch: [41][120/149]\tTime 1.253 (0.816)\tData 0.743 (0.312)\tLoss 0.6351 (0.1916)\tSoftmaxLoss 0.5832 (0.1406)\tRankLoss 0.0519 (0.0510)\tPrec@1 94.531 (94.331)\n",
            "Time: Thu Mar 31 00:16:35 2022\n",
            "Step: 6249\t Epoch: [41][140/149]\tTime 1.418 (0.818)\tData 0.923 (0.314)\tLoss 0.1967 (0.1978)\tSoftmaxLoss 0.1462 (0.1468)\tRankLoss 0.0505 (0.0510)\tPrec@1 89.844 (94.227)\n",
            "Time: Thu Mar 31 00:16:43 2022\n",
            "Test: [0/11]\tTime 3.138 (3.138)\tSoftmaxLoss 0.0932 (0.0932)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 81.346\n",
            "Time: Thu Mar 31 00:16:59 2022\n",
            "Step: 6258\t Epoch: [42][0/149]\tTime 2.070 (2.070)\tData 1.570 (1.570)\tLoss 0.1197 (0.1197)\tSoftmaxLoss 0.0684 (0.0684)\tRankLoss 0.0513 (0.0513)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:17:18 2022\n",
            "Step: 6278\t Epoch: [42][20/149]\tTime 1.110 (0.980)\tData 0.604 (0.476)\tLoss 0.0924 (0.1404)\tSoftmaxLoss 0.0381 (0.0896)\tRankLoss 0.0542 (0.0508)\tPrec@1 100.000 (97.917)\n",
            "Time: Thu Mar 31 00:17:35 2022\n",
            "Step: 6298\t Epoch: [42][40/149]\tTime 1.011 (0.909)\tData 0.509 (0.405)\tLoss 0.1611 (0.1455)\tSoftmaxLoss 0.1109 (0.0948)\tRankLoss 0.0501 (0.0508)\tPrec@1 93.750 (96.951)\n",
            "Time: Thu Mar 31 00:17:51 2022\n",
            "Step: 6318\t Epoch: [42][60/149]\tTime 1.023 (0.882)\tData 0.526 (0.379)\tLoss 0.0660 (0.1485)\tSoftmaxLoss 0.0160 (0.0978)\tRankLoss 0.0500 (0.0507)\tPrec@1 100.000 (96.516)\n",
            "Time: Thu Mar 31 00:18:08 2022\n",
            "Step: 6338\t Epoch: [42][80/149]\tTime 1.089 (0.873)\tData 0.591 (0.370)\tLoss 0.1824 (0.1612)\tSoftmaxLoss 0.1323 (0.1104)\tRankLoss 0.0501 (0.0507)\tPrec@1 93.750 (95.583)\n",
            "Time: Thu Mar 31 00:18:23 2022\n",
            "Step: 6358\t Epoch: [42][100/149]\tTime 0.962 (0.849)\tData 0.469 (0.346)\tLoss 0.0599 (0.1621)\tSoftmaxLoss 0.0099 (0.1113)\tRankLoss 0.0500 (0.0508)\tPrec@1 100.000 (95.312)\n",
            "Time: Thu Mar 31 00:18:40 2022\n",
            "Step: 6378\t Epoch: [42][120/149]\tTime 0.508 (0.845)\tData 0.002 (0.341)\tLoss 0.0568 (0.1574)\tSoftmaxLoss 0.0067 (0.1066)\tRankLoss 0.0500 (0.0508)\tPrec@1 100.000 (95.810)\n",
            "Time: Thu Mar 31 00:18:56 2022\n",
            "Step: 6398\t Epoch: [42][140/149]\tTime 0.496 (0.840)\tData 0.002 (0.336)\tLoss 0.3030 (0.1517)\tSoftmaxLoss 0.2530 (0.1009)\tRankLoss 0.0500 (0.0508)\tPrec@1 84.375 (96.055)\n",
            "Time: Thu Mar 31 00:19:05 2022\n",
            "Test: [0/11]\tTime 3.201 (3.201)\tSoftmaxLoss 0.4410 (0.4410)\tPrec@1 84.375 (84.375)\n",
            " * Prec@1 76.147\n",
            "Time: Thu Mar 31 00:19:22 2022\n",
            "Step: 6407\t Epoch: [43][0/149]\tTime 2.461 (2.461)\tData 1.954 (1.954)\tLoss 0.0901 (0.0901)\tSoftmaxLoss 0.0393 (0.0393)\tRankLoss 0.0508 (0.0508)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:19:39 2022\n",
            "Step: 6427\t Epoch: [43][20/149]\tTime 0.701 (0.950)\tData 0.187 (0.442)\tLoss 0.0544 (0.1612)\tSoftmaxLoss 0.0042 (0.1103)\tRankLoss 0.0503 (0.0509)\tPrec@1 100.000 (95.647)\n",
            "Time: Thu Mar 31 00:19:55 2022\n",
            "Step: 6447\t Epoch: [43][40/149]\tTime 0.668 (0.863)\tData 0.156 (0.356)\tLoss 0.1005 (0.2080)\tSoftmaxLoss 0.0502 (0.1573)\tRankLoss 0.0503 (0.0507)\tPrec@1 100.000 (95.598)\n",
            "Time: Thu Mar 31 00:20:11 2022\n",
            "Step: 6467\t Epoch: [43][60/149]\tTime 1.305 (0.848)\tData 0.808 (0.342)\tLoss 0.0539 (0.1981)\tSoftmaxLoss 0.0039 (0.1473)\tRankLoss 0.0500 (0.0508)\tPrec@1 100.000 (95.710)\n",
            "Time: Thu Mar 31 00:20:27 2022\n",
            "Step: 6487\t Epoch: [43][80/149]\tTime 1.291 (0.839)\tData 0.779 (0.334)\tLoss 0.5288 (0.2015)\tSoftmaxLoss 0.4783 (0.1508)\tRankLoss 0.0505 (0.0507)\tPrec@1 80.469 (95.023)\n",
            "Time: Thu Mar 31 00:20:43 2022\n",
            "Step: 6507\t Epoch: [43][100/149]\tTime 0.856 (0.829)\tData 0.353 (0.323)\tLoss 0.0829 (0.1917)\tSoftmaxLoss 0.0310 (0.1410)\tRankLoss 0.0519 (0.0507)\tPrec@1 99.219 (95.104)\n",
            "Time: Thu Mar 31 00:20:58 2022\n",
            "Step: 6527\t Epoch: [43][120/149]\tTime 0.707 (0.818)\tData 0.194 (0.313)\tLoss 0.0636 (0.1825)\tSoftmaxLoss 0.0131 (0.1318)\tRankLoss 0.0505 (0.0507)\tPrec@1 100.000 (95.338)\n",
            "Time: Thu Mar 31 00:21:15 2022\n",
            "Step: 6547\t Epoch: [43][140/149]\tTime 1.445 (0.820)\tData 0.935 (0.314)\tLoss 0.0903 (0.1728)\tSoftmaxLoss 0.0393 (0.1221)\tRankLoss 0.0510 (0.0507)\tPrec@1 100.000 (95.706)\n",
            "Time: Thu Mar 31 00:21:24 2022\n",
            "Test: [0/11]\tTime 3.185 (3.185)\tSoftmaxLoss 0.0647 (0.0647)\tPrec@1 100.000 (100.000)\n",
            " * Prec@1 81.651\n",
            "Time: Thu Mar 31 00:21:40 2022\n",
            "Step: 6556\t Epoch: [44][0/149]\tTime 1.768 (1.768)\tData 1.262 (1.262)\tLoss 0.1403 (0.1403)\tSoftmaxLoss 0.0902 (0.0902)\tRankLoss 0.0501 (0.0501)\tPrec@1 98.438 (98.438)\n",
            "Time: Thu Mar 31 00:21:57 2022\n",
            "Step: 6576\t Epoch: [44][20/149]\tTime 1.124 (0.923)\tData 0.630 (0.415)\tLoss 0.0759 (0.1919)\tSoftmaxLoss 0.0252 (0.1412)\tRankLoss 0.0507 (0.0507)\tPrec@1 100.000 (94.792)\n",
            "Time: Thu Mar 31 00:22:14 2022\n",
            "Step: 6596\t Epoch: [44][40/149]\tTime 0.499 (0.868)\tData 0.002 (0.362)\tLoss 0.1075 (0.2190)\tSoftmaxLoss 0.0565 (0.1683)\tRankLoss 0.0510 (0.0507)\tPrec@1 99.219 (94.055)\n",
            "Time: Thu Mar 31 00:22:29 2022\n",
            "Step: 6616\t Epoch: [44][60/149]\tTime 0.515 (0.839)\tData 0.017 (0.334)\tLoss 0.0630 (0.2142)\tSoftmaxLoss 0.0127 (0.1634)\tRankLoss 0.0503 (0.0508)\tPrec@1 100.000 (94.057)\n",
            "Time: Thu Mar 31 00:22:46 2022\n",
            "Step: 6636\t Epoch: [44][80/149]\tTime 0.507 (0.843)\tData 0.002 (0.339)\tLoss 0.0694 (0.1937)\tSoftmaxLoss 0.0189 (0.1429)\tRankLoss 0.0505 (0.0508)\tPrec@1 100.000 (94.695)\n",
            "Time: Thu Mar 31 00:23:03 2022\n",
            "Step: 6656\t Epoch: [44][100/149]\tTime 0.501 (0.840)\tData 0.002 (0.335)\tLoss 0.2777 (0.1856)\tSoftmaxLoss 0.2275 (0.1349)\tRankLoss 0.0503 (0.0507)\tPrec@1 85.938 (94.802)\n",
            "Time: Thu Mar 31 00:23:19 2022\n",
            "Step: 6676\t Epoch: [44][120/149]\tTime 0.515 (0.835)\tData 0.002 (0.331)\tLoss 0.0533 (0.1832)\tSoftmaxLoss 0.0033 (0.1325)\tRankLoss 0.0500 (0.0507)\tPrec@1 100.000 (94.880)\n",
            "Time: Thu Mar 31 00:23:36 2022\n",
            "Step: 6696\t Epoch: [44][140/149]\tTime 0.515 (0.835)\tData 0.002 (0.331)\tLoss 0.0720 (0.1766)\tSoftmaxLoss 0.0196 (0.1258)\tRankLoss 0.0524 (0.0508)\tPrec@1 100.000 (95.047)\n",
            "Time: Thu Mar 31 00:23:45 2022\n",
            "Test: [0/11]\tTime 3.120 (3.120)\tSoftmaxLoss 0.1623 (0.1623)\tPrec@1 87.500 (87.500)\n",
            " * Prec@1 81.040\n",
            "Time: Thu Mar 31 00:24:02 2022\n",
            "Step: 6705\t Epoch: [45][0/149]\tTime 1.838 (1.838)\tData 1.329 (1.329)\tLoss 0.4828 (0.4828)\tSoftmaxLoss 0.4312 (0.4312)\tRankLoss 0.0516 (0.0516)\tPrec@1 74.219 (74.219)\n",
            "Time: Thu Mar 31 00:24:20 2022\n",
            "Step: 6725\t Epoch: [45][20/149]\tTime 1.465 (0.967)\tData 0.970 (0.460)\tLoss 0.0953 (0.1934)\tSoftmaxLoss 0.0446 (0.1425)\tRankLoss 0.0507 (0.0510)\tPrec@1 100.000 (93.192)\n",
            "Time: Thu Mar 31 00:24:35 2022\n",
            "Step: 6745\t Epoch: [45][40/149]\tTime 0.913 (0.870)\tData 0.398 (0.364)\tLoss 0.0914 (0.1997)\tSoftmaxLoss 0.0405 (0.1485)\tRankLoss 0.0508 (0.0512)\tPrec@1 100.000 (94.379)\n",
            "Time: Thu Mar 31 00:24:53 2022\n",
            "Step: 6765\t Epoch: [45][60/149]\tTime 1.092 (0.870)\tData 0.593 (0.366)\tLoss 0.4153 (0.2002)\tSoftmaxLoss 0.3649 (0.1490)\tRankLoss 0.0503 (0.0511)\tPrec@1 84.375 (94.314)\n",
            "Time: Thu Mar 31 00:25:08 2022\n",
            "Step: 6785\t Epoch: [45][80/149]\tTime 0.779 (0.849)\tData 0.283 (0.346)\tLoss 0.1383 (0.1938)\tSoftmaxLoss 0.0878 (0.1428)\tRankLoss 0.0504 (0.0510)\tPrec@1 99.219 (94.647)\n",
            "Time: Thu Mar 31 00:25:25 2022\n",
            "Step: 6805\t Epoch: [45][100/149]\tTime 1.313 (0.846)\tData 0.815 (0.343)\tLoss 0.0822 (0.1876)\tSoftmaxLoss 0.0317 (0.1365)\tRankLoss 0.0505 (0.0510)\tPrec@1 100.000 (95.011)\n",
            "Time: Thu Mar 31 00:25:41 2022\n",
            "Step: 6825\t Epoch: [45][120/149]\tTime 0.511 (0.837)\tData 0.002 (0.334)\tLoss 0.1534 (0.1888)\tSoftmaxLoss 0.1032 (0.1378)\tRankLoss 0.0501 (0.0510)\tPrec@1 92.969 (95.054)\n",
            "Time: Thu Mar 31 00:25:58 2022\n",
            "Step: 6845\t Epoch: [45][140/149]\tTime 0.512 (0.837)\tData 0.006 (0.334)\tLoss 0.3505 (0.1907)\tSoftmaxLoss 0.2998 (0.1398)\tRankLoss 0.0507 (0.0509)\tPrec@1 75.000 (94.853)\n",
            "Time: Thu Mar 31 00:26:07 2022\n",
            "Test: [0/11]\tTime 3.190 (3.190)\tSoftmaxLoss 0.6320 (0.6320)\tPrec@1 78.125 (78.125)\n",
            " * Prec@1 75.535\n",
            "Time: Thu Mar 31 00:26:23 2022\n",
            "Step: 6854\t Epoch: [46][0/149]\tTime 1.645 (1.645)\tData 1.117 (1.117)\tLoss 0.0696 (0.0696)\tSoftmaxLoss 0.0192 (0.0192)\tRankLoss 0.0503 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:26:41 2022\n",
            "Step: 6874\t Epoch: [46][20/149]\tTime 0.505 (0.958)\tData 0.002 (0.450)\tLoss 0.0744 (0.2287)\tSoftmaxLoss 0.0239 (0.1782)\tRankLoss 0.0506 (0.0505)\tPrec@1 100.000 (92.225)\n",
            "Time: Thu Mar 31 00:26:57 2022\n",
            "Step: 6894\t Epoch: [46][40/149]\tTime 1.336 (0.875)\tData 0.841 (0.366)\tLoss 0.0842 (0.1849)\tSoftmaxLoss 0.0341 (0.1343)\tRankLoss 0.0502 (0.0506)\tPrec@1 100.000 (94.512)\n",
            "Time: Thu Mar 31 00:27:14 2022\n",
            "Step: 6914\t Epoch: [46][60/149]\tTime 1.218 (0.866)\tData 0.705 (0.359)\tLoss 0.0944 (0.1612)\tSoftmaxLoss 0.0422 (0.1106)\tRankLoss 0.0522 (0.0507)\tPrec@1 100.000 (95.722)\n",
            "Time: Thu Mar 31 00:27:29 2022\n",
            "Step: 6934\t Epoch: [46][80/149]\tTime 1.423 (0.844)\tData 0.919 (0.338)\tLoss 0.0576 (0.1479)\tSoftmaxLoss 0.0067 (0.0972)\tRankLoss 0.0508 (0.0507)\tPrec@1 100.000 (96.161)\n",
            "Time: Thu Mar 31 00:27:45 2022\n",
            "Step: 6954\t Epoch: [46][100/149]\tTime 1.290 (0.834)\tData 0.796 (0.328)\tLoss 0.2337 (0.1373)\tSoftmaxLoss 0.1827 (0.0865)\tRankLoss 0.0511 (0.0507)\tPrec@1 86.719 (96.597)\n",
            "Time: Thu Mar 31 00:28:02 2022\n",
            "Step: 6974\t Epoch: [46][120/149]\tTime 1.169 (0.831)\tData 0.662 (0.326)\tLoss 0.0632 (0.1332)\tSoftmaxLoss 0.0114 (0.0825)\tRankLoss 0.0518 (0.0507)\tPrec@1 100.000 (96.985)\n",
            "Time: Thu Mar 31 00:28:18 2022\n",
            "Step: 6994\t Epoch: [46][140/149]\tTime 0.697 (0.828)\tData 0.197 (0.323)\tLoss 0.2350 (0.1302)\tSoftmaxLoss 0.1816 (0.0794)\tRankLoss 0.0534 (0.0508)\tPrec@1 85.156 (96.964)\n",
            "Time: Thu Mar 31 00:28:28 2022\n",
            "Test: [0/11]\tTime 3.139 (3.139)\tSoftmaxLoss 0.0135 (0.0135)\tPrec@1 100.000 (100.000)\n",
            " * Prec@1 81.040\n",
            "Time: Thu Mar 31 00:28:44 2022\n",
            "Step: 7003\t Epoch: [47][0/149]\tTime 2.051 (2.051)\tData 1.542 (1.542)\tLoss 0.0623 (0.0623)\tSoftmaxLoss 0.0108 (0.0108)\tRankLoss 0.0515 (0.0515)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:29:02 2022\n",
            "Step: 7023\t Epoch: [47][20/149]\tTime 0.945 (0.964)\tData 0.449 (0.455)\tLoss 0.0505 (0.0925)\tSoftmaxLoss 0.0002 (0.0413)\tRankLoss 0.0503 (0.0513)\tPrec@1 100.000 (98.251)\n",
            "Time: Thu Mar 31 00:29:18 2022\n",
            "Step: 7043\t Epoch: [47][40/149]\tTime 0.519 (0.866)\tData 0.009 (0.360)\tLoss 0.1031 (0.1097)\tSoftmaxLoss 0.0530 (0.0586)\tRankLoss 0.0501 (0.0510)\tPrec@1 100.000 (97.085)\n",
            "Time: Thu Mar 31 00:29:33 2022\n",
            "Step: 7063\t Epoch: [47][60/149]\tTime 0.501 (0.837)\tData 0.003 (0.332)\tLoss 0.0822 (0.1371)\tSoftmaxLoss 0.0302 (0.0860)\tRankLoss 0.0520 (0.0511)\tPrec@1 100.000 (96.529)\n",
            "Time: Thu Mar 31 00:29:50 2022\n",
            "Step: 7083\t Epoch: [47][80/149]\tTime 0.504 (0.834)\tData 0.003 (0.331)\tLoss 0.1925 (0.1586)\tSoftmaxLoss 0.1422 (0.1075)\tRankLoss 0.0503 (0.0511)\tPrec@1 96.875 (96.123)\n",
            "Time: Thu Mar 31 00:30:06 2022\n",
            "Step: 7103\t Epoch: [47][100/149]\tTime 0.763 (0.835)\tData 0.255 (0.331)\tLoss 0.2181 (0.1582)\tSoftmaxLoss 0.1680 (0.1072)\tRankLoss 0.0500 (0.0511)\tPrec@1 89.062 (95.985)\n",
            "Time: Thu Mar 31 00:30:23 2022\n",
            "Step: 7123\t Epoch: [47][120/149]\tTime 0.782 (0.831)\tData 0.281 (0.327)\tLoss 0.0605 (0.1553)\tSoftmaxLoss 0.0103 (0.1043)\tRankLoss 0.0501 (0.0510)\tPrec@1 100.000 (95.952)\n",
            "Time: Thu Mar 31 00:30:39 2022\n",
            "Step: 7143\t Epoch: [47][140/149]\tTime 0.687 (0.832)\tData 0.191 (0.329)\tLoss 0.1251 (0.1590)\tSoftmaxLoss 0.0751 (0.1080)\tRankLoss 0.0500 (0.0510)\tPrec@1 98.438 (95.739)\n",
            "Time: Thu Mar 31 00:30:49 2022\n",
            "Test: [0/11]\tTime 3.214 (3.214)\tSoftmaxLoss 0.4127 (0.4127)\tPrec@1 84.375 (84.375)\n",
            " * Prec@1 74.312\n",
            "Time: Thu Mar 31 00:31:05 2022\n",
            "Step: 7152\t Epoch: [48][0/149]\tTime 2.297 (2.297)\tData 1.787 (1.787)\tLoss 0.1157 (0.1157)\tSoftmaxLoss 0.0643 (0.0643)\tRankLoss 0.0514 (0.0514)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:31:22 2022\n",
            "Step: 7172\t Epoch: [48][20/149]\tTime 1.138 (0.910)\tData 0.633 (0.403)\tLoss 0.0517 (0.2142)\tSoftmaxLoss 0.0017 (0.1633)\tRankLoss 0.0500 (0.0509)\tPrec@1 100.000 (92.932)\n",
            "Time: Thu Mar 31 00:31:38 2022\n",
            "Step: 7192\t Epoch: [48][40/149]\tTime 0.495 (0.856)\tData 0.002 (0.351)\tLoss 0.1038 (0.1794)\tSoftmaxLoss 0.0535 (0.1287)\tRankLoss 0.0503 (0.0507)\tPrec@1 99.219 (94.779)\n",
            "Time: Thu Mar 31 00:31:54 2022\n",
            "Step: 7212\t Epoch: [48][60/149]\tTime 1.168 (0.837)\tData 0.661 (0.332)\tLoss 0.0580 (0.1537)\tSoftmaxLoss 0.0079 (0.1029)\tRankLoss 0.0501 (0.0508)\tPrec@1 100.000 (96.043)\n",
            "Time: Thu Mar 31 00:32:09 2022\n",
            "Step: 7232\t Epoch: [48][80/149]\tTime 0.623 (0.817)\tData 0.121 (0.313)\tLoss 0.2081 (0.1497)\tSoftmaxLoss 0.1580 (0.0989)\tRankLoss 0.0501 (0.0508)\tPrec@1 87.500 (96.209)\n",
            "Time: Thu Mar 31 00:32:27 2022\n",
            "Step: 7252\t Epoch: [48][100/149]\tTime 0.639 (0.828)\tData 0.134 (0.324)\tLoss 0.0538 (0.1416)\tSoftmaxLoss 0.0038 (0.0908)\tRankLoss 0.0500 (0.0508)\tPrec@1 100.000 (96.658)\n",
            "Time: Thu Mar 31 00:32:44 2022\n",
            "Step: 7272\t Epoch: [48][120/149]\tTime 1.220 (0.837)\tData 0.719 (0.333)\tLoss 0.1620 (0.1339)\tSoftmaxLoss 0.1109 (0.0832)\tRankLoss 0.0511 (0.0507)\tPrec@1 96.094 (96.894)\n",
            "Time: Thu Mar 31 00:33:00 2022\n",
            "Step: 7292\t Epoch: [48][140/149]\tTime 0.509 (0.830)\tData 0.002 (0.327)\tLoss 0.0615 (0.1298)\tSoftmaxLoss 0.0104 (0.0790)\tRankLoss 0.0512 (0.0508)\tPrec@1 100.000 (97.130)\n",
            "Time: Thu Mar 31 00:33:09 2022\n",
            "Test: [0/11]\tTime 3.149 (3.149)\tSoftmaxLoss 0.4782 (0.4782)\tPrec@1 84.375 (84.375)\n",
            " * Prec@1 81.651\n",
            "Time: Thu Mar 31 00:33:26 2022\n",
            "Step: 7301\t Epoch: [49][0/149]\tTime 2.859 (2.859)\tData 2.357 (2.357)\tLoss 0.1543 (0.1543)\tSoftmaxLoss 0.1042 (0.1042)\tRankLoss 0.0501 (0.0501)\tPrec@1 95.312 (95.312)\n",
            "Time: Thu Mar 31 00:33:43 2022\n",
            "Step: 7321\t Epoch: [49][20/149]\tTime 0.583 (0.940)\tData 0.075 (0.433)\tLoss 0.0830 (0.0922)\tSoftmaxLoss 0.0329 (0.0412)\tRankLoss 0.0500 (0.0510)\tPrec@1 100.000 (98.475)\n",
            "Time: Thu Mar 31 00:33:59 2022\n",
            "Step: 7341\t Epoch: [49][40/149]\tTime 0.929 (0.868)\tData 0.423 (0.363)\tLoss 0.0907 (0.0911)\tSoftmaxLoss 0.0394 (0.0400)\tRankLoss 0.0513 (0.0511)\tPrec@1 97.656 (98.742)\n",
            "Time: Thu Mar 31 00:34:16 2022\n",
            "Step: 7361\t Epoch: [49][60/149]\tTime 1.426 (0.864)\tData 0.933 (0.361)\tLoss 0.0530 (0.0889)\tSoftmaxLoss 0.0025 (0.0377)\tRankLoss 0.0505 (0.0512)\tPrec@1 100.000 (98.796)\n",
            "Time: Thu Mar 31 00:34:31 2022\n",
            "Step: 7381\t Epoch: [49][80/149]\tTime 0.911 (0.837)\tData 0.411 (0.334)\tLoss 0.0534 (0.0871)\tSoftmaxLoss 0.0012 (0.0358)\tRankLoss 0.0522 (0.0512)\tPrec@1 100.000 (98.794)\n",
            "Time: Thu Mar 31 00:34:48 2022\n",
            "Step: 7401\t Epoch: [49][100/149]\tTime 0.916 (0.837)\tData 0.413 (0.334)\tLoss 0.0508 (0.0855)\tSoftmaxLoss 0.0007 (0.0343)\tRankLoss 0.0501 (0.0512)\tPrec@1 100.000 (98.770)\n",
            "Time: Thu Mar 31 00:35:04 2022\n",
            "Step: 7421\t Epoch: [49][120/149]\tTime 1.498 (0.830)\tData 0.994 (0.327)\tLoss 0.0508 (0.0851)\tSoftmaxLoss 0.0004 (0.0339)\tRankLoss 0.0504 (0.0512)\tPrec@1 100.000 (98.773)\n",
            "Time: Thu Mar 31 00:35:20 2022\n",
            "Step: 7441\t Epoch: [49][140/149]\tTime 0.846 (0.826)\tData 0.337 (0.323)\tLoss 0.0887 (0.0880)\tSoftmaxLoss 0.0387 (0.0367)\tRankLoss 0.0500 (0.0513)\tPrec@1 99.219 (98.487)\n",
            "Time: Thu Mar 31 00:35:29 2022\n",
            "Test: [0/11]\tTime 3.154 (3.154)\tSoftmaxLoss 0.2470 (0.2470)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 84.404\n",
            "Time: Thu Mar 31 00:35:46 2022\n",
            "Step: 7450\t Epoch: [50][0/149]\tTime 2.498 (2.498)\tData 1.983 (1.983)\tLoss 0.0874 (0.0874)\tSoftmaxLoss 0.0362 (0.0362)\tRankLoss 0.0512 (0.0512)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:36:03 2022\n",
            "Step: 7470\t Epoch: [50][20/149]\tTime 1.072 (0.939)\tData 0.563 (0.430)\tLoss 0.0571 (0.1014)\tSoftmaxLoss 0.0005 (0.0497)\tRankLoss 0.0566 (0.0517)\tPrec@1 100.000 (98.400)\n",
            "Time: Thu Mar 31 00:36:19 2022\n",
            "Step: 7490\t Epoch: [50][40/149]\tTime 0.504 (0.868)\tData 0.004 (0.362)\tLoss 0.0668 (0.0918)\tSoftmaxLoss 0.0145 (0.0402)\tRankLoss 0.0523 (0.0517)\tPrec@1 100.000 (98.838)\n",
            "Time: Thu Mar 31 00:36:34 2022\n",
            "Step: 7510\t Epoch: [50][60/149]\tTime 0.523 (0.840)\tData 0.003 (0.334)\tLoss 0.0512 (0.1007)\tSoftmaxLoss 0.0010 (0.0492)\tRankLoss 0.0502 (0.0516)\tPrec@1 100.000 (98.130)\n",
            "Time: Thu Mar 31 00:36:51 2022\n",
            "Step: 7530\t Epoch: [50][80/149]\tTime 0.498 (0.833)\tData 0.003 (0.329)\tLoss 0.0700 (0.1159)\tSoftmaxLoss 0.0194 (0.0644)\tRankLoss 0.0507 (0.0515)\tPrec@1 100.000 (97.704)\n",
            "Time: Thu Mar 31 00:37:06 2022\n",
            "Step: 7550\t Epoch: [50][100/149]\tTime 0.514 (0.823)\tData 0.002 (0.319)\tLoss 0.0950 (0.1253)\tSoftmaxLoss 0.0438 (0.0739)\tRankLoss 0.0512 (0.0514)\tPrec@1 99.219 (97.324)\n",
            "Time: Thu Mar 31 00:37:24 2022\n",
            "Step: 7570\t Epoch: [50][120/149]\tTime 0.499 (0.830)\tData 0.002 (0.327)\tLoss 0.0998 (0.1280)\tSoftmaxLoss 0.0497 (0.0767)\tRankLoss 0.0501 (0.0513)\tPrec@1 99.219 (97.198)\n",
            "Time: Thu Mar 31 00:37:40 2022\n",
            "Step: 7590\t Epoch: [50][140/149]\tTime 0.506 (0.828)\tData 0.002 (0.324)\tLoss 0.2648 (0.1269)\tSoftmaxLoss 0.2145 (0.0757)\tRankLoss 0.0504 (0.0512)\tPrec@1 86.719 (97.257)\n",
            "Time: Thu Mar 31 00:37:50 2022\n",
            "Test: [0/11]\tTime 3.222 (3.222)\tSoftmaxLoss 0.1801 (0.1801)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.122\n",
            "Time: Thu Mar 31 00:38:06 2022\n",
            "Step: 7599\t Epoch: [51][0/149]\tTime 1.829 (1.829)\tData 1.332 (1.332)\tLoss 0.2218 (0.2218)\tSoftmaxLoss 0.1718 (0.1718)\tRankLoss 0.0500 (0.0500)\tPrec@1 89.062 (89.062)\n",
            "Time: Thu Mar 31 00:38:24 2022\n",
            "Step: 7619\t Epoch: [51][20/149]\tTime 0.503 (0.913)\tData 0.002 (0.403)\tLoss 0.1848 (0.1301)\tSoftmaxLoss 0.1319 (0.0788)\tRankLoss 0.0529 (0.0513)\tPrec@1 92.188 (96.094)\n",
            "Time: Thu Mar 31 00:38:40 2022\n",
            "Step: 7639\t Epoch: [51][40/149]\tTime 1.134 (0.863)\tData 0.642 (0.357)\tLoss 0.0568 (0.1249)\tSoftmaxLoss 0.0037 (0.0736)\tRankLoss 0.0531 (0.0512)\tPrec@1 100.000 (96.303)\n",
            "Time: Thu Mar 31 00:38:57 2022\n",
            "Step: 7659\t Epoch: [51][60/149]\tTime 1.290 (0.867)\tData 0.783 (0.362)\tLoss 0.0570 (0.1230)\tSoftmaxLoss 0.0069 (0.0719)\tRankLoss 0.0500 (0.0511)\tPrec@1 100.000 (96.773)\n",
            "Time: Thu Mar 31 00:39:13 2022\n",
            "Step: 7679\t Epoch: [51][80/149]\tTime 1.203 (0.849)\tData 0.710 (0.346)\tLoss 0.0592 (0.1281)\tSoftmaxLoss 0.0082 (0.0770)\tRankLoss 0.0510 (0.0511)\tPrec@1 100.000 (96.923)\n",
            "Time: Thu Mar 31 00:39:29 2022\n",
            "Step: 7699\t Epoch: [51][100/149]\tTime 1.258 (0.841)\tData 0.737 (0.337)\tLoss 0.0533 (0.1179)\tSoftmaxLoss 0.0019 (0.0667)\tRankLoss 0.0514 (0.0511)\tPrec@1 100.000 (97.463)\n",
            "Time: Thu Mar 31 00:39:45 2022\n",
            "Step: 7719\t Epoch: [51][120/149]\tTime 1.026 (0.833)\tData 0.522 (0.330)\tLoss 0.0552 (0.1107)\tSoftmaxLoss 0.0048 (0.0594)\tRankLoss 0.0504 (0.0512)\tPrec@1 100.000 (97.805)\n",
            "Time: Thu Mar 31 00:40:02 2022\n",
            "Step: 7739\t Epoch: [51][140/149]\tTime 0.948 (0.831)\tData 0.444 (0.327)\tLoss 0.0533 (0.1066)\tSoftmaxLoss 0.0018 (0.0553)\tRankLoss 0.0514 (0.0512)\tPrec@1 100.000 (97.900)\n",
            "Time: Thu Mar 31 00:40:11 2022\n",
            "Test: [0/11]\tTime 3.085 (3.085)\tSoftmaxLoss 0.1468 (0.1468)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 79.817\n",
            "Time: Thu Mar 31 00:40:28 2022\n",
            "Step: 7748\t Epoch: [52][0/149]\tTime 2.140 (2.140)\tData 1.644 (1.644)\tLoss 0.0693 (0.0693)\tSoftmaxLoss 0.0155 (0.0155)\tRankLoss 0.0538 (0.0538)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:40:46 2022\n",
            "Step: 7768\t Epoch: [52][20/149]\tTime 0.824 (0.953)\tData 0.312 (0.445)\tLoss 0.0715 (0.0944)\tSoftmaxLoss 0.0181 (0.0427)\tRankLoss 0.0534 (0.0517)\tPrec@1 100.000 (98.326)\n",
            "Time: Thu Mar 31 00:41:01 2022\n",
            "Step: 7788\t Epoch: [52][40/149]\tTime 1.113 (0.870)\tData 0.603 (0.365)\tLoss 0.0536 (0.0908)\tSoftmaxLoss 0.0019 (0.0392)\tRankLoss 0.0517 (0.0516)\tPrec@1 100.000 (98.742)\n",
            "Time: Thu Mar 31 00:41:17 2022\n",
            "Step: 7808\t Epoch: [52][60/149]\tTime 0.788 (0.845)\tData 0.296 (0.341)\tLoss 0.2509 (0.0976)\tSoftmaxLoss 0.2005 (0.0462)\tRankLoss 0.0505 (0.0514)\tPrec@1 87.500 (98.297)\n",
            "Time: Thu Mar 31 00:41:34 2022\n",
            "Step: 7828\t Epoch: [52][80/149]\tTime 1.138 (0.843)\tData 0.638 (0.339)\tLoss 0.0630 (0.0964)\tSoftmaxLoss 0.0116 (0.0449)\tRankLoss 0.0514 (0.0514)\tPrec@1 100.000 (98.370)\n",
            "Time: Thu Mar 31 00:41:50 2022\n",
            "Step: 7848\t Epoch: [52][100/149]\tTime 1.298 (0.837)\tData 0.802 (0.333)\tLoss 0.0544 (0.0974)\tSoftmaxLoss 0.0038 (0.0461)\tRankLoss 0.0506 (0.0514)\tPrec@1 100.000 (98.445)\n",
            "Time: Thu Mar 31 00:42:06 2022\n",
            "Step: 7868\t Epoch: [52][120/149]\tTime 0.942 (0.829)\tData 0.451 (0.325)\tLoss 0.0513 (0.0934)\tSoftmaxLoss 0.0012 (0.0421)\tRankLoss 0.0500 (0.0513)\tPrec@1 100.000 (98.605)\n",
            "Time: Thu Mar 31 00:42:21 2022\n",
            "Step: 7888\t Epoch: [52][140/149]\tTime 1.155 (0.821)\tData 0.660 (0.318)\tLoss 0.0523 (0.0894)\tSoftmaxLoss 0.0021 (0.0381)\tRankLoss 0.0502 (0.0514)\tPrec@1 100.000 (98.787)\n",
            "Time: Thu Mar 31 00:42:31 2022\n",
            "Test: [0/11]\tTime 3.123 (3.123)\tSoftmaxLoss 0.1154 (0.1154)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 80.428\n",
            "Time: Thu Mar 31 00:42:47 2022\n",
            "Step: 7897\t Epoch: [53][0/149]\tTime 2.316 (2.316)\tData 1.811 (1.811)\tLoss 0.0531 (0.0531)\tSoftmaxLoss 0.0019 (0.0019)\tRankLoss 0.0512 (0.0512)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:43:06 2022\n",
            "Step: 7917\t Epoch: [53][20/149]\tTime 1.381 (0.991)\tData 0.887 (0.486)\tLoss 0.0527 (0.0694)\tSoftmaxLoss 0.0015 (0.0177)\tRankLoss 0.0511 (0.0517)\tPrec@1 100.000 (99.219)\n",
            "Time: Thu Mar 31 00:43:22 2022\n",
            "Step: 7937\t Epoch: [53][40/149]\tTime 0.975 (0.902)\tData 0.460 (0.398)\tLoss 0.0693 (0.0670)\tSoftmaxLoss 0.0181 (0.0152)\tRankLoss 0.0511 (0.0518)\tPrec@1 100.000 (99.543)\n",
            "Time: Thu Mar 31 00:43:39 2022\n",
            "Step: 7957\t Epoch: [53][60/149]\tTime 1.477 (0.888)\tData 0.971 (0.384)\tLoss 0.0588 (0.0662)\tSoftmaxLoss 0.0037 (0.0143)\tRankLoss 0.0551 (0.0520)\tPrec@1 100.000 (99.577)\n",
            "Time: Thu Mar 31 00:43:55 2022\n",
            "Step: 7977\t Epoch: [53][80/149]\tTime 0.824 (0.862)\tData 0.324 (0.357)\tLoss 0.0807 (0.0667)\tSoftmaxLoss 0.0244 (0.0149)\tRankLoss 0.0563 (0.0519)\tPrec@1 100.000 (99.576)\n",
            "Time: Thu Mar 31 00:44:11 2022\n",
            "Step: 7997\t Epoch: [53][100/149]\tTime 0.498 (0.851)\tData 0.004 (0.347)\tLoss 0.0562 (0.0667)\tSoftmaxLoss 0.0062 (0.0148)\tRankLoss 0.0500 (0.0519)\tPrec@1 100.000 (99.536)\n",
            "Time: Thu Mar 31 00:44:27 2022\n",
            "Step: 8017\t Epoch: [53][120/149]\tTime 0.503 (0.846)\tData 0.002 (0.341)\tLoss 0.0530 (0.0659)\tSoftmaxLoss 0.0001 (0.0140)\tRankLoss 0.0529 (0.0519)\tPrec@1 100.000 (99.587)\n",
            "Time: Thu Mar 31 00:44:43 2022\n",
            "Step: 8037\t Epoch: [53][140/149]\tTime 0.508 (0.838)\tData 0.003 (0.334)\tLoss 0.0512 (0.0648)\tSoftmaxLoss 0.0001 (0.0130)\tRankLoss 0.0511 (0.0518)\tPrec@1 100.000 (99.634)\n",
            "Time: Thu Mar 31 00:44:53 2022\n",
            "Test: [0/11]\tTime 3.179 (3.179)\tSoftmaxLoss 0.3130 (0.3130)\tPrec@1 90.625 (90.625)\n",
            " * Prec@1 78.899\n",
            "Time: Thu Mar 31 00:45:09 2022\n",
            "Step: 8046\t Epoch: [54][0/149]\tTime 1.708 (1.708)\tData 1.201 (1.201)\tLoss 0.0800 (0.0800)\tSoftmaxLoss 0.0294 (0.0294)\tRankLoss 0.0507 (0.0507)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:45:26 2022\n",
            "Step: 8066\t Epoch: [54][20/149]\tTime 0.861 (0.928)\tData 0.357 (0.419)\tLoss 0.0519 (0.0661)\tSoftmaxLoss 0.0017 (0.0144)\tRankLoss 0.0501 (0.0517)\tPrec@1 100.000 (99.219)\n",
            "Time: Thu Mar 31 00:45:44 2022\n",
            "Step: 8086\t Epoch: [54][40/149]\tTime 1.419 (0.905)\tData 0.913 (0.397)\tLoss 0.0515 (0.0614)\tSoftmaxLoss 0.0002 (0.0096)\tRankLoss 0.0514 (0.0517)\tPrec@1 100.000 (99.600)\n",
            "Time: Thu Mar 31 00:46:00 2022\n",
            "Step: 8106\t Epoch: [54][60/149]\tTime 0.632 (0.874)\tData 0.137 (0.368)\tLoss 0.0617 (0.0606)\tSoftmaxLoss 0.0099 (0.0087)\tRankLoss 0.0518 (0.0519)\tPrec@1 100.000 (99.731)\n",
            "Time: Thu Mar 31 00:46:16 2022\n",
            "Step: 8126\t Epoch: [54][80/149]\tTime 0.507 (0.858)\tData 0.002 (0.352)\tLoss 0.0505 (0.0621)\tSoftmaxLoss 0.0002 (0.0103)\tRankLoss 0.0503 (0.0517)\tPrec@1 100.000 (99.537)\n",
            "Time: Thu Mar 31 00:46:32 2022\n",
            "Step: 8146\t Epoch: [54][100/149]\tTime 0.512 (0.846)\tData 0.003 (0.341)\tLoss 0.0506 (0.0614)\tSoftmaxLoss 0.0003 (0.0097)\tRankLoss 0.0503 (0.0517)\tPrec@1 100.000 (99.590)\n",
            "Time: Thu Mar 31 00:46:49 2022\n",
            "Step: 8166\t Epoch: [54][120/149]\tTime 0.506 (0.846)\tData 0.002 (0.342)\tLoss 0.0526 (0.0614)\tSoftmaxLoss 0.0010 (0.0097)\tRankLoss 0.0516 (0.0517)\tPrec@1 100.000 (99.632)\n",
            "Time: Thu Mar 31 00:47:05 2022\n",
            "Step: 8186\t Epoch: [54][140/149]\tTime 0.506 (0.837)\tData 0.002 (0.334)\tLoss 0.3474 (0.0624)\tSoftmaxLoss 0.2965 (0.0108)\tRankLoss 0.0508 (0.0515)\tPrec@1 84.375 (99.573)\n",
            "Time: Thu Mar 31 00:47:14 2022\n",
            "Test: [0/11]\tTime 3.175 (3.175)\tSoftmaxLoss 0.1252 (0.1252)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.428\n",
            "Time: Thu Mar 31 00:47:31 2022\n",
            "Step: 8195\t Epoch: [55][0/149]\tTime 2.117 (2.117)\tData 1.603 (1.603)\tLoss 0.0513 (0.0513)\tSoftmaxLoss 0.0007 (0.0007)\tRankLoss 0.0506 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:47:50 2022\n",
            "Step: 8215\t Epoch: [55][20/149]\tTime 0.997 (1.009)\tData 0.489 (0.497)\tLoss 0.0561 (0.0692)\tSoftmaxLoss 0.0049 (0.0172)\tRankLoss 0.0512 (0.0520)\tPrec@1 100.000 (99.368)\n",
            "Time: Thu Mar 31 00:48:05 2022\n",
            "Step: 8235\t Epoch: [55][40/149]\tTime 0.518 (0.895)\tData 0.002 (0.386)\tLoss 0.0528 (0.0657)\tSoftmaxLoss 0.0004 (0.0136)\tRankLoss 0.0524 (0.0521)\tPrec@1 100.000 (99.447)\n",
            "Time: Thu Mar 31 00:48:22 2022\n",
            "Step: 8255\t Epoch: [55][60/149]\tTime 1.377 (0.875)\tData 0.864 (0.368)\tLoss 0.0551 (0.0646)\tSoftmaxLoss 0.0004 (0.0124)\tRankLoss 0.0547 (0.0522)\tPrec@1 100.000 (99.552)\n",
            "Time: Thu Mar 31 00:48:39 2022\n",
            "Step: 8275\t Epoch: [55][80/149]\tTime 1.168 (0.868)\tData 0.672 (0.362)\tLoss 0.0526 (0.0622)\tSoftmaxLoss 0.0026 (0.0101)\tRankLoss 0.0500 (0.0520)\tPrec@1 100.000 (99.662)\n",
            "Time: Thu Mar 31 00:48:55 2022\n",
            "Step: 8295\t Epoch: [55][100/149]\tTime 1.265 (0.854)\tData 0.759 (0.350)\tLoss 0.0530 (0.0612)\tSoftmaxLoss 0.0001 (0.0092)\tRankLoss 0.0528 (0.0520)\tPrec@1 100.000 (99.722)\n",
            "Time: Thu Mar 31 00:49:11 2022\n",
            "Step: 8315\t Epoch: [55][120/149]\tTime 1.359 (0.844)\tData 0.863 (0.340)\tLoss 0.0502 (0.0603)\tSoftmaxLoss 0.0002 (0.0085)\tRankLoss 0.0500 (0.0518)\tPrec@1 100.000 (99.755)\n",
            "Time: Thu Mar 31 00:49:26 2022\n",
            "Step: 8335\t Epoch: [55][140/149]\tTime 0.867 (0.837)\tData 0.354 (0.333)\tLoss 0.0508 (0.0602)\tSoftmaxLoss 0.0003 (0.0084)\tRankLoss 0.0505 (0.0517)\tPrec@1 100.000 (99.784)\n",
            "Time: Thu Mar 31 00:49:36 2022\n",
            "Test: [0/11]\tTime 3.118 (3.118)\tSoftmaxLoss 0.0866 (0.0866)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 81.040\n",
            "Time: Thu Mar 31 00:49:52 2022\n",
            "Step: 8344\t Epoch: [56][0/149]\tTime 2.077 (2.077)\tData 1.561 (1.561)\tLoss 0.0516 (0.0516)\tSoftmaxLoss 0.0010 (0.0010)\tRankLoss 0.0506 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:50:10 2022\n",
            "Step: 8364\t Epoch: [56][20/149]\tTime 0.624 (0.973)\tData 0.121 (0.464)\tLoss 0.0514 (0.0523)\tSoftmaxLoss 0.0010 (0.0011)\tRankLoss 0.0504 (0.0512)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:50:26 2022\n",
            "Step: 8384\t Epoch: [56][40/149]\tTime 0.502 (0.879)\tData 0.002 (0.372)\tLoss 0.0534 (0.0531)\tSoftmaxLoss 0.0001 (0.0016)\tRankLoss 0.0533 (0.0515)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:50:43 2022\n",
            "Step: 8404\t Epoch: [56][60/149]\tTime 0.497 (0.871)\tData 0.002 (0.364)\tLoss 0.0545 (0.0542)\tSoftmaxLoss 0.0008 (0.0026)\tRankLoss 0.0537 (0.0516)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:50:59 2022\n",
            "Step: 8424\t Epoch: [56][80/149]\tTime 0.832 (0.856)\tData 0.334 (0.349)\tLoss 0.0510 (0.0552)\tSoftmaxLoss 0.0002 (0.0037)\tRankLoss 0.0508 (0.0515)\tPrec@1 100.000 (99.981)\n",
            "Time: Thu Mar 31 00:51:14 2022\n",
            "Step: 8444\t Epoch: [56][100/149]\tTime 0.947 (0.839)\tData 0.437 (0.333)\tLoss 0.0556 (0.0552)\tSoftmaxLoss 0.0016 (0.0037)\tRankLoss 0.0540 (0.0515)\tPrec@1 100.000 (99.985)\n",
            "Time: Thu Mar 31 00:51:30 2022\n",
            "Step: 8464\t Epoch: [56][120/149]\tTime 0.511 (0.831)\tData 0.002 (0.325)\tLoss 0.0548 (0.0549)\tSoftmaxLoss 0.0006 (0.0033)\tRankLoss 0.0542 (0.0516)\tPrec@1 100.000 (99.987)\n",
            "Time: Thu Mar 31 00:51:46 2022\n",
            "Step: 8484\t Epoch: [56][140/149]\tTime 0.504 (0.823)\tData 0.002 (0.318)\tLoss 0.0640 (0.0547)\tSoftmaxLoss 0.0136 (0.0031)\tRankLoss 0.0504 (0.0517)\tPrec@1 100.000 (99.989)\n",
            "Time: Thu Mar 31 00:51:56 2022\n",
            "Test: [0/11]\tTime 3.254 (3.254)\tSoftmaxLoss 0.0981 (0.0981)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 79.205\n",
            "Time: Thu Mar 31 00:52:12 2022\n",
            "Step: 8493\t Epoch: [57][0/149]\tTime 1.781 (1.781)\tData 1.275 (1.275)\tLoss 0.0533 (0.0533)\tSoftmaxLoss 0.0010 (0.0010)\tRankLoss 0.0523 (0.0523)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:52:29 2022\n",
            "Step: 8513\t Epoch: [57][20/149]\tTime 1.141 (0.933)\tData 0.631 (0.425)\tLoss 0.0543 (0.0565)\tSoftmaxLoss 0.0040 (0.0053)\tRankLoss 0.0503 (0.0512)\tPrec@1 100.000 (99.851)\n",
            "Time: Thu Mar 31 00:52:45 2022\n",
            "Step: 8533\t Epoch: [57][40/149]\tTime 1.231 (0.869)\tData 0.739 (0.363)\tLoss 0.0594 (0.0546)\tSoftmaxLoss 0.0084 (0.0033)\tRankLoss 0.0510 (0.0512)\tPrec@1 100.000 (99.924)\n",
            "Time: Thu Mar 31 00:53:02 2022\n",
            "Step: 8553\t Epoch: [57][60/149]\tTime 0.896 (0.854)\tData 0.389 (0.349)\tLoss 0.0524 (0.0542)\tSoftmaxLoss 0.0009 (0.0029)\tRankLoss 0.0515 (0.0512)\tPrec@1 100.000 (99.949)\n",
            "Time: Thu Mar 31 00:53:18 2022\n",
            "Step: 8573\t Epoch: [57][80/149]\tTime 0.513 (0.847)\tData 0.005 (0.342)\tLoss 0.0508 (0.0539)\tSoftmaxLoss 0.0000 (0.0025)\tRankLoss 0.0508 (0.0513)\tPrec@1 100.000 (99.961)\n",
            "Time: Thu Mar 31 00:53:35 2022\n",
            "Step: 8593\t Epoch: [57][100/149]\tTime 0.513 (0.844)\tData 0.002 (0.340)\tLoss 0.0514 (0.0535)\tSoftmaxLoss 0.0004 (0.0022)\tRankLoss 0.0509 (0.0514)\tPrec@1 100.000 (99.969)\n",
            "Time: Thu Mar 31 00:53:52 2022\n",
            "Step: 8613\t Epoch: [57][120/149]\tTime 0.915 (0.841)\tData 0.411 (0.337)\tLoss 0.0521 (0.0534)\tSoftmaxLoss 0.0004 (0.0020)\tRankLoss 0.0517 (0.0514)\tPrec@1 100.000 (99.974)\n",
            "Time: Thu Mar 31 00:54:08 2022\n",
            "Step: 8633\t Epoch: [57][140/149]\tTime 0.500 (0.837)\tData 0.002 (0.333)\tLoss 0.0503 (0.0532)\tSoftmaxLoss 0.0002 (0.0018)\tRankLoss 0.0500 (0.0513)\tPrec@1 100.000 (99.978)\n",
            "Time: Thu Mar 31 00:54:17 2022\n",
            "Test: [0/11]\tTime 3.220 (3.220)\tSoftmaxLoss 0.1731 (0.1731)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.734\n",
            "Time: Thu Mar 31 00:54:33 2022\n",
            "Step: 8642\t Epoch: [58][0/149]\tTime 1.823 (1.823)\tData 1.309 (1.309)\tLoss 0.0529 (0.0529)\tSoftmaxLoss 0.0009 (0.0009)\tRankLoss 0.0520 (0.0520)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:54:51 2022\n",
            "Step: 8662\t Epoch: [58][20/149]\tTime 0.523 (0.949)\tData 0.008 (0.441)\tLoss 0.0507 (0.0523)\tSoftmaxLoss 0.0003 (0.0013)\tRankLoss 0.0505 (0.0511)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:55:08 2022\n",
            "Step: 8682\t Epoch: [58][40/149]\tTime 1.068 (0.898)\tData 0.567 (0.392)\tLoss 0.0559 (0.0526)\tSoftmaxLoss 0.0005 (0.0013)\tRankLoss 0.0554 (0.0513)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:55:25 2022\n",
            "Step: 8702\t Epoch: [58][60/149]\tTime 1.073 (0.883)\tData 0.574 (0.377)\tLoss 0.0547 (0.0524)\tSoftmaxLoss 0.0029 (0.0010)\tRankLoss 0.0518 (0.0514)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:55:40 2022\n",
            "Step: 8722\t Epoch: [58][80/149]\tTime 1.018 (0.854)\tData 0.512 (0.349)\tLoss 0.0514 (0.0524)\tSoftmaxLoss 0.0014 (0.0010)\tRankLoss 0.0500 (0.0514)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:55:56 2022\n",
            "Step: 8742\t Epoch: [58][100/149]\tTime 1.093 (0.842)\tData 0.588 (0.337)\tLoss 0.0530 (0.0536)\tSoftmaxLoss 0.0020 (0.0023)\tRankLoss 0.0511 (0.0513)\tPrec@1 100.000 (99.946)\n",
            "Time: Thu Mar 31 00:56:13 2022\n",
            "Step: 8762\t Epoch: [58][120/149]\tTime 1.250 (0.840)\tData 0.744 (0.336)\tLoss 0.0550 (0.0535)\tSoftmaxLoss 0.0046 (0.0022)\tRankLoss 0.0505 (0.0513)\tPrec@1 100.000 (99.955)\n",
            "Time: Thu Mar 31 00:56:29 2022\n",
            "Step: 8782\t Epoch: [58][140/149]\tTime 1.564 (0.835)\tData 1.043 (0.330)\tLoss 0.0503 (0.0533)\tSoftmaxLoss 0.0003 (0.0019)\tRankLoss 0.0500 (0.0513)\tPrec@1 100.000 (99.961)\n",
            "Time: Thu Mar 31 00:56:39 2022\n",
            "Test: [0/11]\tTime 3.159 (3.159)\tSoftmaxLoss 0.1158 (0.1158)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 81.040\n",
            "Time: Thu Mar 31 00:56:55 2022\n",
            "Step: 8791\t Epoch: [59][0/149]\tTime 1.907 (1.907)\tData 1.399 (1.399)\tLoss 0.0545 (0.0545)\tSoftmaxLoss 0.0024 (0.0024)\tRankLoss 0.0521 (0.0521)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:57:12 2022\n",
            "Step: 8811\t Epoch: [59][20/149]\tTime 1.196 (0.904)\tData 0.685 (0.399)\tLoss 0.0502 (0.0524)\tSoftmaxLoss 0.0000 (0.0014)\tRankLoss 0.0501 (0.0510)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:57:28 2022\n",
            "Step: 8831\t Epoch: [59][40/149]\tTime 1.230 (0.872)\tData 0.723 (0.367)\tLoss 0.0503 (0.0522)\tSoftmaxLoss 0.0002 (0.0012)\tRankLoss 0.0501 (0.0511)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:57:44 2022\n",
            "Step: 8851\t Epoch: [59][60/149]\tTime 0.857 (0.841)\tData 0.345 (0.335)\tLoss 0.0602 (0.0522)\tSoftmaxLoss 0.0100 (0.0013)\tRankLoss 0.0501 (0.0510)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:58:00 2022\n",
            "Step: 8871\t Epoch: [59][80/149]\tTime 0.514 (0.836)\tData 0.002 (0.330)\tLoss 0.0529 (0.0524)\tSoftmaxLoss 0.0012 (0.0014)\tRankLoss 0.0517 (0.0510)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:58:17 2022\n",
            "Step: 8891\t Epoch: [59][100/149]\tTime 0.518 (0.833)\tData 0.003 (0.328)\tLoss 0.0590 (0.0540)\tSoftmaxLoss 0.0025 (0.0029)\tRankLoss 0.0566 (0.0511)\tPrec@1 100.000 (99.907)\n",
            "Time: Thu Mar 31 00:58:33 2022\n",
            "Step: 8911\t Epoch: [59][120/149]\tTime 1.521 (0.832)\tData 1.008 (0.327)\tLoss 0.0540 (0.0542)\tSoftmaxLoss 0.0037 (0.0032)\tRankLoss 0.0503 (0.0510)\tPrec@1 100.000 (99.916)\n",
            "Time: Thu Mar 31 00:58:50 2022\n",
            "Step: 8931\t Epoch: [59][140/149]\tTime 1.096 (0.832)\tData 0.593 (0.327)\tLoss 0.0507 (0.0541)\tSoftmaxLoss 0.0002 (0.0030)\tRankLoss 0.0506 (0.0510)\tPrec@1 100.000 (99.922)\n",
            "Time: Thu Mar 31 00:58:59 2022\n",
            "Test: [0/11]\tTime 3.121 (3.121)\tSoftmaxLoss 0.1155 (0.1155)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 81.346\n",
            "Time: Thu Mar 31 00:59:16 2022\n",
            "Step: 8940\t Epoch: [60][0/149]\tTime 2.133 (2.133)\tData 1.619 (1.619)\tLoss 0.0518 (0.0518)\tSoftmaxLoss 0.0002 (0.0002)\tRankLoss 0.0517 (0.0517)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:59:33 2022\n",
            "Step: 8960\t Epoch: [60][20/149]\tTime 0.846 (0.951)\tData 0.345 (0.444)\tLoss 0.0543 (0.0522)\tSoftmaxLoss 0.0033 (0.0009)\tRankLoss 0.0510 (0.0513)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 00:59:50 2022\n",
            "Step: 8980\t Epoch: [60][40/149]\tTime 0.512 (0.885)\tData 0.006 (0.380)\tLoss 0.0520 (0.0571)\tSoftmaxLoss 0.0005 (0.0059)\tRankLoss 0.0515 (0.0511)\tPrec@1 100.000 (99.657)\n",
            "Time: Thu Mar 31 01:00:07 2022\n",
            "Step: 9000\t Epoch: [60][60/149]\tTime 1.007 (0.870)\tData 0.499 (0.365)\tLoss 0.0534 (0.0558)\tSoftmaxLoss 0.0001 (0.0048)\tRankLoss 0.0533 (0.0511)\tPrec@1 100.000 (99.769)\n",
            "Time: Thu Mar 31 01:00:22 2022\n",
            "Step: 9020\t Epoch: [60][80/149]\tTime 0.504 (0.844)\tData 0.002 (0.340)\tLoss 0.0526 (0.0552)\tSoftmaxLoss 0.0007 (0.0041)\tRankLoss 0.0520 (0.0511)\tPrec@1 100.000 (99.826)\n",
            "Time: Thu Mar 31 01:00:38 2022\n",
            "Step: 9040\t Epoch: [60][100/149]\tTime 1.184 (0.840)\tData 0.675 (0.337)\tLoss 0.0852 (0.0548)\tSoftmaxLoss 0.0349 (0.0038)\tRankLoss 0.0504 (0.0510)\tPrec@1 100.000 (99.861)\n",
            "Time: Thu Mar 31 01:00:55 2022\n",
            "Step: 9060\t Epoch: [60][120/149]\tTime 0.792 (0.840)\tData 0.300 (0.337)\tLoss 0.0516 (0.0554)\tSoftmaxLoss 0.0011 (0.0044)\tRankLoss 0.0505 (0.0510)\tPrec@1 100.000 (99.851)\n",
            "Time: Thu Mar 31 01:01:12 2022\n",
            "Step: 9080\t Epoch: [60][140/149]\tTime 1.244 (0.838)\tData 0.740 (0.335)\tLoss 0.0527 (0.0549)\tSoftmaxLoss 0.0001 (0.0039)\tRankLoss 0.0526 (0.0511)\tPrec@1 100.000 (99.873)\n",
            "Time: Thu Mar 31 01:01:21 2022\n",
            "Test: [0/11]\tTime 3.216 (3.216)\tSoftmaxLoss 0.1531 (0.1531)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.428\n",
            "Time: Thu Mar 31 01:01:37 2022\n",
            "Step: 9089\t Epoch: [61][0/149]\tTime 1.968 (1.968)\tData 1.441 (1.441)\tLoss 0.0504 (0.0504)\tSoftmaxLoss 0.0002 (0.0002)\tRankLoss 0.0502 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:01:54 2022\n",
            "Step: 9109\t Epoch: [61][20/149]\tTime 1.020 (0.902)\tData 0.520 (0.392)\tLoss 0.0532 (0.0538)\tSoftmaxLoss 0.0026 (0.0029)\tRankLoss 0.0505 (0.0509)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:02:11 2022\n",
            "Step: 9129\t Epoch: [61][40/149]\tTime 0.664 (0.853)\tData 0.165 (0.346)\tLoss 0.0503 (0.0528)\tSoftmaxLoss 0.0001 (0.0020)\tRankLoss 0.0502 (0.0507)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:02:27 2022\n",
            "Step: 9149\t Epoch: [61][60/149]\tTime 0.521 (0.845)\tData 0.006 (0.339)\tLoss 0.0536 (0.0577)\tSoftmaxLoss 0.0014 (0.0069)\tRankLoss 0.0522 (0.0508)\tPrec@1 100.000 (99.744)\n",
            "Time: Thu Mar 31 01:02:43 2022\n",
            "Step: 9169\t Epoch: [61][80/149]\tTime 1.251 (0.835)\tData 0.745 (0.329)\tLoss 0.0501 (0.0563)\tSoftmaxLoss 0.0001 (0.0056)\tRankLoss 0.0500 (0.0507)\tPrec@1 100.000 (99.807)\n",
            "Time: Thu Mar 31 01:03:00 2022\n",
            "Step: 9189\t Epoch: [61][100/149]\tTime 1.040 (0.836)\tData 0.528 (0.330)\tLoss 0.0524 (0.0592)\tSoftmaxLoss 0.0013 (0.0085)\tRankLoss 0.0511 (0.0508)\tPrec@1 100.000 (99.691)\n",
            "Time: Thu Mar 31 01:03:18 2022\n",
            "Step: 9209\t Epoch: [61][120/149]\tTime 1.258 (0.844)\tData 0.750 (0.339)\tLoss 0.0511 (0.0583)\tSoftmaxLoss 0.0011 (0.0075)\tRankLoss 0.0500 (0.0507)\tPrec@1 100.000 (99.742)\n",
            "Time: Thu Mar 31 01:03:33 2022\n",
            "Step: 9229\t Epoch: [61][140/149]\tTime 1.246 (0.832)\tData 0.743 (0.327)\tLoss 0.0516 (0.0582)\tSoftmaxLoss 0.0009 (0.0074)\tRankLoss 0.0507 (0.0508)\tPrec@1 100.000 (99.773)\n",
            "Time: Thu Mar 31 01:03:42 2022\n",
            "Test: [0/11]\tTime 3.100 (3.100)\tSoftmaxLoss 0.2677 (0.2677)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 81.040\n",
            "Time: Thu Mar 31 01:03:59 2022\n",
            "Step: 9238\t Epoch: [62][0/149]\tTime 2.216 (2.216)\tData 1.711 (1.711)\tLoss 0.0531 (0.0531)\tSoftmaxLoss 0.0031 (0.0031)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:04:17 2022\n",
            "Step: 9258\t Epoch: [62][20/149]\tTime 0.500 (0.984)\tData 0.007 (0.475)\tLoss 0.0521 (0.0532)\tSoftmaxLoss 0.0009 (0.0024)\tRankLoss 0.0513 (0.0508)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:04:34 2022\n",
            "Step: 9278\t Epoch: [62][40/149]\tTime 0.497 (0.906)\tData 0.003 (0.400)\tLoss 0.0563 (0.0528)\tSoftmaxLoss 0.0048 (0.0020)\tRankLoss 0.0516 (0.0508)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:04:50 2022\n",
            "Step: 9298\t Epoch: [62][60/149]\tTime 0.507 (0.878)\tData 0.002 (0.373)\tLoss 0.0515 (0.0523)\tSoftmaxLoss 0.0000 (0.0017)\tRankLoss 0.0514 (0.0507)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:05:06 2022\n",
            "Step: 9318\t Epoch: [62][80/149]\tTime 0.514 (0.856)\tData 0.008 (0.351)\tLoss 0.0505 (0.0525)\tSoftmaxLoss 0.0005 (0.0018)\tRankLoss 0.0500 (0.0507)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:05:23 2022\n",
            "Step: 9338\t Epoch: [62][100/149]\tTime 0.508 (0.858)\tData 0.003 (0.353)\tLoss 0.0516 (0.0523)\tSoftmaxLoss 0.0003 (0.0016)\tRankLoss 0.0512 (0.0507)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:05:39 2022\n",
            "Step: 9358\t Epoch: [62][120/149]\tTime 0.517 (0.850)\tData 0.002 (0.346)\tLoss 0.0506 (0.0521)\tSoftmaxLoss 0.0001 (0.0015)\tRankLoss 0.0505 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:05:56 2022\n",
            "Step: 9378\t Epoch: [62][140/149]\tTime 0.817 (0.848)\tData 0.325 (0.344)\tLoss 0.0502 (0.0522)\tSoftmaxLoss 0.0002 (0.0015)\tRankLoss 0.0500 (0.0507)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:06:05 2022\n",
            "Test: [0/11]\tTime 3.142 (3.142)\tSoftmaxLoss 0.2166 (0.2166)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 81.040\n",
            "Time: Thu Mar 31 01:06:21 2022\n",
            "Step: 9387\t Epoch: [63][0/149]\tTime 1.854 (1.854)\tData 1.328 (1.328)\tLoss 0.0505 (0.0505)\tSoftmaxLoss 0.0004 (0.0004)\tRankLoss 0.0501 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:06:38 2022\n",
            "Step: 9407\t Epoch: [63][20/149]\tTime 0.507 (0.898)\tData 0.002 (0.389)\tLoss 0.0557 (0.0515)\tSoftmaxLoss 0.0038 (0.0009)\tRankLoss 0.0518 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:06:55 2022\n",
            "Step: 9427\t Epoch: [63][40/149]\tTime 0.938 (0.863)\tData 0.430 (0.356)\tLoss 0.0505 (0.0522)\tSoftmaxLoss 0.0002 (0.0015)\tRankLoss 0.0503 (0.0507)\tPrec@1 100.000 (99.981)\n",
            "Time: Thu Mar 31 01:07:11 2022\n",
            "Step: 9447\t Epoch: [63][60/149]\tTime 1.276 (0.847)\tData 0.765 (0.341)\tLoss 0.0506 (0.0520)\tSoftmaxLoss 0.0002 (0.0013)\tRankLoss 0.0504 (0.0507)\tPrec@1 100.000 (99.987)\n",
            "Time: Thu Mar 31 01:07:27 2022\n",
            "Step: 9467\t Epoch: [63][80/149]\tTime 0.855 (0.833)\tData 0.352 (0.327)\tLoss 0.0520 (0.0521)\tSoftmaxLoss 0.0013 (0.0013)\tRankLoss 0.0507 (0.0508)\tPrec@1 100.000 (99.990)\n",
            "Time: Thu Mar 31 01:07:44 2022\n",
            "Step: 9487\t Epoch: [63][100/149]\tTime 1.136 (0.834)\tData 0.632 (0.329)\tLoss 0.0647 (0.0529)\tSoftmaxLoss 0.0118 (0.0021)\tRankLoss 0.0530 (0.0508)\tPrec@1 100.000 (99.938)\n",
            "Time: Thu Mar 31 01:08:00 2022\n",
            "Step: 9507\t Epoch: [63][120/149]\tTime 0.506 (0.832)\tData 0.002 (0.327)\tLoss 0.0502 (0.0526)\tSoftmaxLoss 0.0001 (0.0018)\tRankLoss 0.0501 (0.0508)\tPrec@1 100.000 (99.948)\n",
            "Time: Thu Mar 31 01:08:17 2022\n",
            "Step: 9527\t Epoch: [63][140/149]\tTime 0.498 (0.835)\tData 0.003 (0.330)\tLoss 0.0572 (0.0525)\tSoftmaxLoss 0.0072 (0.0018)\tRankLoss 0.0500 (0.0507)\tPrec@1 100.000 (99.956)\n",
            "Time: Thu Mar 31 01:08:27 2022\n",
            "Test: [0/11]\tTime 3.163 (3.163)\tSoftmaxLoss 0.1973 (0.1973)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 79.511\n",
            "Time: Thu Mar 31 01:08:43 2022\n",
            "Step: 9536\t Epoch: [64][0/149]\tTime 1.942 (1.942)\tData 1.430 (1.430)\tLoss 0.0501 (0.0501)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:09:02 2022\n",
            "Step: 9556\t Epoch: [64][20/149]\tTime 1.567 (1.005)\tData 1.074 (0.498)\tLoss 0.0504 (0.0508)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0501 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:09:18 2022\n",
            "Step: 9576\t Epoch: [64][40/149]\tTime 1.066 (0.908)\tData 0.564 (0.401)\tLoss 0.0514 (0.0537)\tSoftmaxLoss 0.0001 (0.0031)\tRankLoss 0.0513 (0.0507)\tPrec@1 100.000 (99.924)\n",
            "Time: Thu Mar 31 01:09:34 2022\n",
            "Step: 9596\t Epoch: [64][60/149]\tTime 1.111 (0.872)\tData 0.613 (0.366)\tLoss 0.0508 (0.0534)\tSoftmaxLoss 0.0001 (0.0026)\tRankLoss 0.0508 (0.0507)\tPrec@1 100.000 (99.949)\n",
            "Time: Thu Mar 31 01:09:50 2022\n",
            "Step: 9616\t Epoch: [64][80/149]\tTime 0.745 (0.852)\tData 0.252 (0.346)\tLoss 0.0501 (0.0529)\tSoftmaxLoss 0.0001 (0.0022)\tRankLoss 0.0500 (0.0507)\tPrec@1 100.000 (99.961)\n",
            "Time: Thu Mar 31 01:10:07 2022\n",
            "Step: 9636\t Epoch: [64][100/149]\tTime 0.917 (0.848)\tData 0.410 (0.343)\tLoss 0.0505 (0.0527)\tSoftmaxLoss 0.0003 (0.0019)\tRankLoss 0.0502 (0.0507)\tPrec@1 100.000 (99.969)\n",
            "Time: Thu Mar 31 01:10:23 2022\n",
            "Step: 9656\t Epoch: [64][120/149]\tTime 0.914 (0.840)\tData 0.416 (0.335)\tLoss 0.0528 (0.0525)\tSoftmaxLoss 0.0014 (0.0018)\tRankLoss 0.0513 (0.0507)\tPrec@1 100.000 (99.974)\n",
            "Time: Thu Mar 31 01:10:39 2022\n",
            "Step: 9676\t Epoch: [64][140/149]\tTime 0.523 (0.832)\tData 0.005 (0.327)\tLoss 0.0503 (0.0525)\tSoftmaxLoss 0.0003 (0.0018)\tRankLoss 0.0500 (0.0507)\tPrec@1 100.000 (99.978)\n",
            "Time: Thu Mar 31 01:10:49 2022\n",
            "Test: [0/11]\tTime 3.306 (3.306)\tSoftmaxLoss 0.1731 (0.1731)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 81.651\n",
            "Time: Thu Mar 31 01:11:05 2022\n",
            "Step: 9685\t Epoch: [65][0/149]\tTime 2.119 (2.119)\tData 1.608 (1.608)\tLoss 0.0506 (0.0506)\tSoftmaxLoss 0.0004 (0.0004)\tRankLoss 0.0503 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:11:23 2022\n",
            "Step: 9705\t Epoch: [65][20/149]\tTime 1.525 (0.966)\tData 1.017 (0.461)\tLoss 0.0519 (0.0515)\tSoftmaxLoss 0.0019 (0.0012)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:11:39 2022\n",
            "Step: 9725\t Epoch: [65][40/149]\tTime 1.074 (0.889)\tData 0.570 (0.384)\tLoss 0.0502 (0.0516)\tSoftmaxLoss 0.0002 (0.0010)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:11:56 2022\n",
            "Step: 9745\t Epoch: [65][60/149]\tTime 1.325 (0.879)\tData 0.813 (0.374)\tLoss 0.0503 (0.0515)\tSoftmaxLoss 0.0000 (0.0008)\tRankLoss 0.0503 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:12:12 2022\n",
            "Step: 9765\t Epoch: [65][80/149]\tTime 1.250 (0.853)\tData 0.743 (0.348)\tLoss 0.0502 (0.0520)\tSoftmaxLoss 0.0002 (0.0014)\tRankLoss 0.0500 (0.0506)\tPrec@1 100.000 (99.990)\n",
            "Time: Thu Mar 31 01:12:28 2022\n",
            "Step: 9785\t Epoch: [65][100/149]\tTime 0.883 (0.848)\tData 0.360 (0.344)\tLoss 0.0503 (0.0521)\tSoftmaxLoss 0.0000 (0.0015)\tRankLoss 0.0503 (0.0506)\tPrec@1 100.000 (99.992)\n",
            "Time: Thu Mar 31 01:12:44 2022\n",
            "Step: 9805\t Epoch: [65][120/149]\tTime 0.978 (0.839)\tData 0.466 (0.336)\tLoss 0.0516 (0.0520)\tSoftmaxLoss 0.0000 (0.0014)\tRankLoss 0.0516 (0.0506)\tPrec@1 100.000 (99.994)\n",
            "Time: Thu Mar 31 01:13:00 2022\n",
            "Step: 9825\t Epoch: [65][140/149]\tTime 0.883 (0.834)\tData 0.371 (0.331)\tLoss 0.0510 (0.0519)\tSoftmaxLoss 0.0006 (0.0013)\tRankLoss 0.0504 (0.0506)\tPrec@1 100.000 (99.994)\n",
            "Time: Thu Mar 31 01:13:09 2022\n",
            "Test: [0/11]\tTime 3.112 (3.112)\tSoftmaxLoss 0.1395 (0.1395)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.734\n",
            "Time: Thu Mar 31 01:13:26 2022\n",
            "Step: 9834\t Epoch: [66][0/149]\tTime 2.549 (2.549)\tData 2.053 (2.053)\tLoss 0.0549 (0.0549)\tSoftmaxLoss 0.0049 (0.0049)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:13:43 2022\n",
            "Step: 9854\t Epoch: [66][20/149]\tTime 0.761 (0.948)\tData 0.247 (0.438)\tLoss 0.0529 (0.0513)\tSoftmaxLoss 0.0004 (0.0007)\tRankLoss 0.0525 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:14:00 2022\n",
            "Step: 9874\t Epoch: [66][40/149]\tTime 0.503 (0.897)\tData 0.002 (0.390)\tLoss 0.0517 (0.0512)\tSoftmaxLoss 0.0002 (0.0007)\tRankLoss 0.0515 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:14:16 2022\n",
            "Step: 9894\t Epoch: [66][60/149]\tTime 0.495 (0.857)\tData 0.003 (0.351)\tLoss 0.0504 (0.0512)\tSoftmaxLoss 0.0002 (0.0007)\tRankLoss 0.0502 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:14:32 2022\n",
            "Step: 9914\t Epoch: [66][80/149]\tTime 0.529 (0.848)\tData 0.014 (0.342)\tLoss 0.0532 (0.0512)\tSoftmaxLoss 0.0029 (0.0007)\tRankLoss 0.0503 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:14:48 2022\n",
            "Step: 9934\t Epoch: [66][100/149]\tTime 0.498 (0.839)\tData 0.002 (0.334)\tLoss 0.0500 (0.0512)\tSoftmaxLoss 0.0000 (0.0007)\tRankLoss 0.0500 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:15:05 2022\n",
            "Step: 9954\t Epoch: [66][120/149]\tTime 0.609 (0.838)\tData 0.110 (0.333)\tLoss 0.0511 (0.0512)\tSoftmaxLoss 0.0001 (0.0006)\tRankLoss 0.0510 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:15:21 2022\n",
            "Step: 9974\t Epoch: [66][140/149]\tTime 0.837 (0.835)\tData 0.335 (0.330)\tLoss 0.0525 (0.0516)\tSoftmaxLoss 0.0022 (0.0010)\tRankLoss 0.0503 (0.0506)\tPrec@1 100.000 (99.989)\n",
            "Time: Thu Mar 31 01:15:31 2022\n",
            "Test: [0/11]\tTime 3.194 (3.194)\tSoftmaxLoss 0.2158 (0.2158)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 81.957\n",
            "Time: Thu Mar 31 01:15:47 2022\n",
            "Step: 9983\t Epoch: [67][0/149]\tTime 1.996 (1.996)\tData 1.494 (1.494)\tLoss 0.0519 (0.0519)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0518 (0.0518)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:16:05 2022\n",
            "Step: 10003\t Epoch: [67][20/149]\tTime 1.430 (0.967)\tData 0.939 (0.459)\tLoss 0.0504 (0.0512)\tSoftmaxLoss 0.0004 (0.0006)\tRankLoss 0.0500 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:16:21 2022\n",
            "Step: 10023\t Epoch: [67][40/149]\tTime 0.561 (0.887)\tData 0.058 (0.381)\tLoss 0.0511 (0.0511)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0511 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:16:37 2022\n",
            "Step: 10043\t Epoch: [67][60/149]\tTime 0.817 (0.863)\tData 0.316 (0.358)\tLoss 0.0503 (0.0510)\tSoftmaxLoss 0.0003 (0.0005)\tRankLoss 0.0500 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:16:54 2022\n",
            "Step: 10063\t Epoch: [67][80/149]\tTime 0.792 (0.860)\tData 0.296 (0.355)\tLoss 0.0505 (0.0510)\tSoftmaxLoss 0.0002 (0.0004)\tRankLoss 0.0503 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:17:10 2022\n",
            "Step: 10083\t Epoch: [67][100/149]\tTime 1.003 (0.843)\tData 0.488 (0.338)\tLoss 0.0526 (0.0510)\tSoftmaxLoss 0.0024 (0.0004)\tRankLoss 0.0502 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:17:26 2022\n",
            "Step: 10103\t Epoch: [67][120/149]\tTime 0.817 (0.841)\tData 0.311 (0.336)\tLoss 0.0568 (0.0511)\tSoftmaxLoss 0.0043 (0.0005)\tRankLoss 0.0526 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:17:42 2022\n",
            "Step: 10123\t Epoch: [67][140/149]\tTime 0.755 (0.833)\tData 0.259 (0.328)\tLoss 0.0509 (0.0511)\tSoftmaxLoss 0.0005 (0.0005)\tRankLoss 0.0504 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:17:51 2022\n",
            "Test: [0/11]\tTime 3.155 (3.155)\tSoftmaxLoss 0.1729 (0.1729)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 81.040\n",
            "Time: Thu Mar 31 01:18:09 2022\n",
            "Step: 10132\t Epoch: [68][0/149]\tTime 2.304 (2.304)\tData 1.810 (1.810)\tLoss 0.0529 (0.0529)\tSoftmaxLoss 0.0002 (0.0002)\tRankLoss 0.0526 (0.0526)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:18:27 2022\n",
            "Step: 10152\t Epoch: [68][20/149]\tTime 1.144 (0.979)\tData 0.636 (0.473)\tLoss 0.0506 (0.0514)\tSoftmaxLoss 0.0004 (0.0007)\tRankLoss 0.0502 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:18:44 2022\n",
            "Step: 10172\t Epoch: [68][40/149]\tTime 0.931 (0.920)\tData 0.423 (0.414)\tLoss 0.0500 (0.0519)\tSoftmaxLoss 0.0000 (0.0012)\tRankLoss 0.0500 (0.0507)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:19:00 2022\n",
            "Step: 10192\t Epoch: [68][60/149]\tTime 1.018 (0.878)\tData 0.501 (0.372)\tLoss 0.0511 (0.0522)\tSoftmaxLoss 0.0002 (0.0016)\tRankLoss 0.0509 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:19:15 2022\n",
            "Step: 10212\t Epoch: [68][80/149]\tTime 0.559 (0.843)\tData 0.047 (0.337)\tLoss 0.0525 (0.0518)\tSoftmaxLoss 0.0002 (0.0013)\tRankLoss 0.0523 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:19:31 2022\n",
            "Step: 10232\t Epoch: [68][100/149]\tTime 0.517 (0.841)\tData 0.003 (0.336)\tLoss 0.0506 (0.0519)\tSoftmaxLoss 0.0004 (0.0013)\tRankLoss 0.0503 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:19:47 2022\n",
            "Step: 10252\t Epoch: [68][120/149]\tTime 0.498 (0.833)\tData 0.004 (0.328)\tLoss 0.0525 (0.0518)\tSoftmaxLoss 0.0016 (0.0012)\tRankLoss 0.0509 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:20:03 2022\n",
            "Step: 10272\t Epoch: [68][140/149]\tTime 0.501 (0.826)\tData 0.002 (0.321)\tLoss 0.0505 (0.0517)\tSoftmaxLoss 0.0002 (0.0011)\tRankLoss 0.0502 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:20:13 2022\n",
            "Test: [0/11]\tTime 3.174 (3.174)\tSoftmaxLoss 0.1557 (0.1557)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 81.346\n",
            "Time: Thu Mar 31 01:20:29 2022\n",
            "Step: 10281\t Epoch: [69][0/149]\tTime 1.989 (1.989)\tData 1.486 (1.486)\tLoss 0.0500 (0.0500)\tSoftmaxLoss 0.0000 (0.0000)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:20:47 2022\n",
            "Step: 10301\t Epoch: [69][20/149]\tTime 0.517 (0.951)\tData 0.006 (0.442)\tLoss 0.0500 (0.0538)\tSoftmaxLoss 0.0000 (0.0033)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (99.926)\n",
            "Time: Thu Mar 31 01:21:04 2022\n",
            "Step: 10321\t Epoch: [69][40/149]\tTime 0.511 (0.895)\tData 0.003 (0.388)\tLoss 0.0500 (0.0524)\tSoftmaxLoss 0.0000 (0.0019)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (99.962)\n",
            "Time: Thu Mar 31 01:21:20 2022\n",
            "Step: 10341\t Epoch: [69][60/149]\tTime 1.072 (0.862)\tData 0.572 (0.357)\tLoss 0.0509 (0.0519)\tSoftmaxLoss 0.0001 (0.0014)\tRankLoss 0.0508 (0.0505)\tPrec@1 100.000 (99.974)\n",
            "Time: Thu Mar 31 01:21:36 2022\n",
            "Step: 10361\t Epoch: [69][80/149]\tTime 0.664 (0.847)\tData 0.153 (0.342)\tLoss 0.0502 (0.0517)\tSoftmaxLoss 0.0002 (0.0012)\tRankLoss 0.0501 (0.0505)\tPrec@1 100.000 (99.981)\n",
            "Time: Thu Mar 31 01:21:53 2022\n",
            "Step: 10381\t Epoch: [69][100/149]\tTime 0.523 (0.851)\tData 0.010 (0.347)\tLoss 0.0501 (0.0515)\tSoftmaxLoss 0.0001 (0.0010)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (99.985)\n",
            "Time: Thu Mar 31 01:22:09 2022\n",
            "Step: 10401\t Epoch: [69][120/149]\tTime 1.656 (0.845)\tData 1.159 (0.341)\tLoss 0.0516 (0.0515)\tSoftmaxLoss 0.0012 (0.0009)\tRankLoss 0.0505 (0.0506)\tPrec@1 100.000 (99.987)\n",
            "Time: Thu Mar 31 01:22:25 2022\n",
            "Step: 10421\t Epoch: [69][140/149]\tTime 0.522 (0.835)\tData 0.005 (0.331)\tLoss 0.0600 (0.0515)\tSoftmaxLoss 0.0094 (0.0009)\tRankLoss 0.0507 (0.0506)\tPrec@1 100.000 (99.989)\n",
            "Time: Thu Mar 31 01:22:35 2022\n",
            "Test: [0/11]\tTime 3.136 (3.136)\tSoftmaxLoss 0.1364 (0.1364)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.734\n",
            "Time: Thu Mar 31 01:22:51 2022\n",
            "Step: 10430\t Epoch: [70][0/149]\tTime 2.173 (2.173)\tData 1.663 (1.663)\tLoss 0.0521 (0.0521)\tSoftmaxLoss 0.0002 (0.0002)\tRankLoss 0.0519 (0.0519)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:23:09 2022\n",
            "Step: 10450\t Epoch: [70][20/149]\tTime 0.974 (0.954)\tData 0.463 (0.446)\tLoss 0.0516 (0.0511)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0513 (0.0508)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:23:26 2022\n",
            "Step: 10470\t Epoch: [70][40/149]\tTime 0.502 (0.890)\tData 0.002 (0.383)\tLoss 0.0528 (0.0513)\tSoftmaxLoss 0.0021 (0.0006)\tRankLoss 0.0507 (0.0507)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:23:41 2022\n",
            "Step: 10490\t Epoch: [70][60/149]\tTime 0.516 (0.856)\tData 0.003 (0.351)\tLoss 0.0522 (0.0512)\tSoftmaxLoss 0.0007 (0.0005)\tRankLoss 0.0516 (0.0507)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:23:58 2022\n",
            "Step: 10510\t Epoch: [70][80/149]\tTime 1.111 (0.847)\tData 0.611 (0.342)\tLoss 0.0502 (0.0512)\tSoftmaxLoss 0.0001 (0.0005)\tRankLoss 0.0501 (0.0507)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:24:14 2022\n",
            "Step: 10530\t Epoch: [70][100/149]\tTime 1.056 (0.836)\tData 0.556 (0.331)\tLoss 0.0502 (0.0513)\tSoftmaxLoss 0.0002 (0.0007)\tRankLoss 0.0500 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:24:31 2022\n",
            "Step: 10550\t Epoch: [70][120/149]\tTime 1.378 (0.845)\tData 0.867 (0.341)\tLoss 0.0503 (0.0512)\tSoftmaxLoss 0.0001 (0.0006)\tRankLoss 0.0502 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:24:47 2022\n",
            "Step: 10570\t Epoch: [70][140/149]\tTime 0.610 (0.837)\tData 0.106 (0.333)\tLoss 0.0523 (0.0512)\tSoftmaxLoss 0.0007 (0.0006)\tRankLoss 0.0517 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:24:57 2022\n",
            "Test: [0/11]\tTime 3.134 (3.134)\tSoftmaxLoss 0.0985 (0.0985)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.428\n",
            "Time: Thu Mar 31 01:25:12 2022\n",
            "Step: 10579\t Epoch: [71][0/149]\tTime 1.821 (1.821)\tData 1.303 (1.303)\tLoss 0.0505 (0.0505)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0504 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:25:29 2022\n",
            "Step: 10599\t Epoch: [71][20/149]\tTime 0.497 (0.894)\tData 0.002 (0.389)\tLoss 0.0501 (0.0511)\tSoftmaxLoss 0.0001 (0.0007)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:25:46 2022\n",
            "Step: 10619\t Epoch: [71][40/149]\tTime 0.584 (0.858)\tData 0.078 (0.352)\tLoss 0.0507 (0.0512)\tSoftmaxLoss 0.0004 (0.0007)\tRankLoss 0.0503 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:26:03 2022\n",
            "Step: 10639\t Epoch: [71][60/149]\tTime 0.513 (0.857)\tData 0.002 (0.352)\tLoss 0.0501 (0.0512)\tSoftmaxLoss 0.0001 (0.0007)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:26:18 2022\n",
            "Step: 10659\t Epoch: [71][80/149]\tTime 0.707 (0.832)\tData 0.205 (0.327)\tLoss 0.0509 (0.0511)\tSoftmaxLoss 0.0001 (0.0006)\tRankLoss 0.0508 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:26:35 2022\n",
            "Step: 10679\t Epoch: [71][100/149]\tTime 0.505 (0.836)\tData 0.002 (0.332)\tLoss 0.0506 (0.0510)\tSoftmaxLoss 0.0004 (0.0006)\tRankLoss 0.0502 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:26:51 2022\n",
            "Step: 10699\t Epoch: [71][120/149]\tTime 0.833 (0.834)\tData 0.327 (0.330)\tLoss 0.0510 (0.0510)\tSoftmaxLoss 0.0010 (0.0005)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:27:08 2022\n",
            "Step: 10719\t Epoch: [71][140/149]\tTime 1.266 (0.836)\tData 0.770 (0.332)\tLoss 0.0500 (0.0510)\tSoftmaxLoss 0.0000 (0.0005)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:27:18 2022\n",
            "Test: [0/11]\tTime 3.199 (3.199)\tSoftmaxLoss 0.1257 (0.1257)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 81.346\n",
            "Time: Thu Mar 31 01:27:34 2022\n",
            "Step: 10728\t Epoch: [72][0/149]\tTime 2.057 (2.057)\tData 1.548 (1.548)\tLoss 0.0516 (0.0516)\tSoftmaxLoss 0.0000 (0.0000)\tRankLoss 0.0516 (0.0516)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:27:51 2022\n",
            "Step: 10748\t Epoch: [72][20/149]\tTime 0.631 (0.912)\tData 0.137 (0.401)\tLoss 0.0501 (0.0506)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:28:07 2022\n",
            "Step: 10768\t Epoch: [72][40/149]\tTime 0.969 (0.863)\tData 0.461 (0.357)\tLoss 0.0500 (0.0510)\tSoftmaxLoss 0.0000 (0.0007)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:28:24 2022\n",
            "Step: 10788\t Epoch: [72][60/149]\tTime 0.497 (0.852)\tData 0.003 (0.346)\tLoss 0.0512 (0.0510)\tSoftmaxLoss 0.0001 (0.0005)\tRankLoss 0.0512 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:28:40 2022\n",
            "Step: 10808\t Epoch: [72][80/149]\tTime 0.501 (0.834)\tData 0.002 (0.329)\tLoss 0.0513 (0.0511)\tSoftmaxLoss 0.0001 (0.0005)\tRankLoss 0.0512 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:28:56 2022\n",
            "Step: 10828\t Epoch: [72][100/149]\tTime 1.283 (0.835)\tData 0.773 (0.330)\tLoss 0.0502 (0.0510)\tSoftmaxLoss 0.0002 (0.0005)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:29:12 2022\n",
            "Step: 10848\t Epoch: [72][120/149]\tTime 1.384 (0.830)\tData 0.866 (0.325)\tLoss 0.0514 (0.0515)\tSoftmaxLoss 0.0009 (0.0009)\tRankLoss 0.0505 (0.0505)\tPrec@1 100.000 (99.987)\n",
            "Time: Thu Mar 31 01:29:29 2022\n",
            "Step: 10868\t Epoch: [72][140/149]\tTime 1.446 (0.830)\tData 0.954 (0.325)\tLoss 0.0508 (0.0514)\tSoftmaxLoss 0.0008 (0.0009)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (99.989)\n",
            "Time: Thu Mar 31 01:29:38 2022\n",
            "Test: [0/11]\tTime 3.152 (3.152)\tSoftmaxLoss 0.1581 (0.1581)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 81.040\n",
            "Time: Thu Mar 31 01:29:54 2022\n",
            "Step: 10877\t Epoch: [73][0/149]\tTime 2.067 (2.067)\tData 1.545 (1.545)\tLoss 0.0501 (0.0501)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:30:12 2022\n",
            "Step: 10897\t Epoch: [73][20/149]\tTime 0.504 (0.923)\tData 0.002 (0.410)\tLoss 0.0509 (0.0509)\tSoftmaxLoss 0.0001 (0.0005)\tRankLoss 0.0508 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:30:27 2022\n",
            "Step: 10917\t Epoch: [73][40/149]\tTime 0.498 (0.846)\tData 0.002 (0.338)\tLoss 0.0506 (0.0515)\tSoftmaxLoss 0.0005 (0.0010)\tRankLoss 0.0501 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:30:44 2022\n",
            "Step: 10937\t Epoch: [73][60/149]\tTime 0.503 (0.844)\tData 0.003 (0.337)\tLoss 0.0503 (0.0513)\tSoftmaxLoss 0.0003 (0.0009)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:31:01 2022\n",
            "Step: 10957\t Epoch: [73][80/149]\tTime 1.397 (0.845)\tData 0.898 (0.338)\tLoss 0.0500 (0.0511)\tSoftmaxLoss 0.0000 (0.0007)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:31:17 2022\n",
            "Step: 10977\t Epoch: [73][100/149]\tTime 0.791 (0.845)\tData 0.299 (0.340)\tLoss 0.0508 (0.0510)\tSoftmaxLoss 0.0001 (0.0006)\tRankLoss 0.0507 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:31:33 2022\n",
            "Step: 10997\t Epoch: [73][120/149]\tTime 0.864 (0.832)\tData 0.357 (0.328)\tLoss 0.0506 (0.0510)\tSoftmaxLoss 0.0005 (0.0006)\tRankLoss 0.0501 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:31:49 2022\n",
            "Step: 11017\t Epoch: [73][140/149]\tTime 1.155 (0.831)\tData 0.652 (0.327)\tLoss 0.0518 (0.0510)\tSoftmaxLoss 0.0001 (0.0006)\tRankLoss 0.0518 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:31:59 2022\n",
            "Test: [0/11]\tTime 3.266 (3.266)\tSoftmaxLoss 0.1570 (0.1570)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.428\n",
            "Time: Thu Mar 31 01:32:15 2022\n",
            "Step: 11026\t Epoch: [74][0/149]\tTime 2.356 (2.356)\tData 1.849 (1.849)\tLoss 0.0505 (0.0505)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0502 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:32:33 2022\n",
            "Step: 11046\t Epoch: [74][20/149]\tTime 1.187 (0.975)\tData 0.685 (0.463)\tLoss 0.0506 (0.0512)\tSoftmaxLoss 0.0001 (0.0007)\tRankLoss 0.0504 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:32:49 2022\n",
            "Step: 11066\t Epoch: [74][40/149]\tTime 1.120 (0.894)\tData 0.625 (0.387)\tLoss 0.0514 (0.0510)\tSoftmaxLoss 0.0013 (0.0005)\tRankLoss 0.0501 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:33:06 2022\n",
            "Step: 11086\t Epoch: [74][60/149]\tTime 1.383 (0.873)\tData 0.877 (0.367)\tLoss 0.0501 (0.0511)\tSoftmaxLoss 0.0001 (0.0006)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:33:21 2022\n",
            "Step: 11106\t Epoch: [74][80/149]\tTime 1.072 (0.848)\tData 0.568 (0.343)\tLoss 0.0502 (0.0511)\tSoftmaxLoss 0.0001 (0.0006)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:33:38 2022\n",
            "Step: 11126\t Epoch: [74][100/149]\tTime 0.970 (0.839)\tData 0.477 (0.335)\tLoss 0.0511 (0.0511)\tSoftmaxLoss 0.0011 (0.0006)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:33:54 2022\n",
            "Step: 11146\t Epoch: [74][120/149]\tTime 0.510 (0.837)\tData 0.003 (0.333)\tLoss 0.0514 (0.0510)\tSoftmaxLoss 0.0008 (0.0005)\tRankLoss 0.0505 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:34:11 2022\n",
            "Step: 11166\t Epoch: [74][140/149]\tTime 0.538 (0.840)\tData 0.014 (0.336)\tLoss 0.0500 (0.0511)\tSoftmaxLoss 0.0000 (0.0006)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:34:20 2022\n",
            "Test: [0/11]\tTime 3.169 (3.169)\tSoftmaxLoss 0.1711 (0.1711)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 81.346\n",
            "Time: Thu Mar 31 01:34:37 2022\n",
            "Step: 11175\t Epoch: [75][0/149]\tTime 1.971 (1.971)\tData 1.471 (1.471)\tLoss 0.0501 (0.0501)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:34:55 2022\n",
            "Step: 11195\t Epoch: [75][20/149]\tTime 1.369 (0.958)\tData 0.876 (0.448)\tLoss 0.0510 (0.0509)\tSoftmaxLoss 0.0003 (0.0004)\tRankLoss 0.0507 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:35:11 2022\n",
            "Step: 11215\t Epoch: [75][40/149]\tTime 0.687 (0.889)\tData 0.191 (0.382)\tLoss 0.0513 (0.0511)\tSoftmaxLoss 0.0002 (0.0004)\tRankLoss 0.0512 (0.0507)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:35:27 2022\n",
            "Step: 11235\t Epoch: [75][60/149]\tTime 1.280 (0.868)\tData 0.772 (0.362)\tLoss 0.0501 (0.0509)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0500 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:35:44 2022\n",
            "Step: 11255\t Epoch: [75][80/149]\tTime 1.005 (0.859)\tData 0.493 (0.353)\tLoss 0.0502 (0.0510)\tSoftmaxLoss 0.0000 (0.0005)\tRankLoss 0.0501 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:36:01 2022\n",
            "Step: 11275\t Epoch: [75][100/149]\tTime 1.063 (0.855)\tData 0.569 (0.350)\tLoss 0.0507 (0.0508)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0505 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:36:17 2022\n",
            "Step: 11295\t Epoch: [75][120/149]\tTime 0.510 (0.845)\tData 0.002 (0.340)\tLoss 0.0553 (0.0509)\tSoftmaxLoss 0.0041 (0.0004)\tRankLoss 0.0513 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:36:33 2022\n",
            "Step: 11315\t Epoch: [75][140/149]\tTime 1.024 (0.839)\tData 0.511 (0.334)\tLoss 0.0541 (0.0509)\tSoftmaxLoss 0.0022 (0.0004)\tRankLoss 0.0518 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:36:42 2022\n",
            "Test: [0/11]\tTime 3.234 (3.234)\tSoftmaxLoss 0.1475 (0.1475)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 81.346\n",
            "Time: Thu Mar 31 01:36:59 2022\n",
            "Step: 11324\t Epoch: [76][0/149]\tTime 2.233 (2.233)\tData 1.724 (1.724)\tLoss 0.0504 (0.0504)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0504 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:37:16 2022\n",
            "Step: 11344\t Epoch: [76][20/149]\tTime 1.671 (0.937)\tData 1.140 (0.427)\tLoss 0.0526 (0.0506)\tSoftmaxLoss 0.0018 (0.0003)\tRankLoss 0.0508 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:37:32 2022\n",
            "Step: 11364\t Epoch: [76][40/149]\tTime 0.632 (0.869)\tData 0.127 (0.362)\tLoss 0.0528 (0.0510)\tSoftmaxLoss 0.0001 (0.0006)\tRankLoss 0.0527 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:37:49 2022\n",
            "Step: 11384\t Epoch: [76][60/149]\tTime 1.103 (0.863)\tData 0.595 (0.357)\tLoss 0.0503 (0.0514)\tSoftmaxLoss 0.0003 (0.0009)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:38:05 2022\n",
            "Step: 11404\t Epoch: [76][80/149]\tTime 0.662 (0.849)\tData 0.167 (0.343)\tLoss 0.0506 (0.0513)\tSoftmaxLoss 0.0000 (0.0008)\tRankLoss 0.0506 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:38:22 2022\n",
            "Step: 11424\t Epoch: [76][100/149]\tTime 1.463 (0.849)\tData 0.951 (0.343)\tLoss 0.0519 (0.0512)\tSoftmaxLoss 0.0002 (0.0007)\tRankLoss 0.0517 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:38:37 2022\n",
            "Step: 11444\t Epoch: [76][120/149]\tTime 0.505 (0.832)\tData 0.002 (0.327)\tLoss 0.0503 (0.0511)\tSoftmaxLoss 0.0000 (0.0007)\tRankLoss 0.0503 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:38:53 2022\n",
            "Step: 11464\t Epoch: [76][140/149]\tTime 0.513 (0.829)\tData 0.003 (0.324)\tLoss 0.0501 (0.0511)\tSoftmaxLoss 0.0000 (0.0007)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:39:02 2022\n",
            "Test: [0/11]\tTime 3.184 (3.184)\tSoftmaxLoss 0.1214 (0.1214)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.734\n",
            "Time: Thu Mar 31 01:39:18 2022\n",
            "Step: 11473\t Epoch: [77][0/149]\tTime 2.627 (2.627)\tData 2.118 (2.118)\tLoss 0.0504 (0.0504)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:39:37 2022\n",
            "Step: 11493\t Epoch: [77][20/149]\tTime 1.628 (1.002)\tData 1.119 (0.496)\tLoss 0.0508 (0.0517)\tSoftmaxLoss 0.0002 (0.0012)\tRankLoss 0.0505 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:39:53 2022\n",
            "Step: 11513\t Epoch: [77][40/149]\tTime 1.300 (0.918)\tData 0.795 (0.414)\tLoss 0.0503 (0.0512)\tSoftmaxLoss 0.0001 (0.0008)\tRankLoss 0.0501 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:40:09 2022\n",
            "Step: 11533\t Epoch: [77][60/149]\tTime 0.497 (0.879)\tData 0.002 (0.376)\tLoss 0.0517 (0.0510)\tSoftmaxLoss 0.0009 (0.0006)\tRankLoss 0.0508 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:40:25 2022\n",
            "Step: 11553\t Epoch: [77][80/149]\tTime 0.821 (0.853)\tData 0.312 (0.349)\tLoss 0.0510 (0.0510)\tSoftmaxLoss 0.0010 (0.0006)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:40:40 2022\n",
            "Step: 11573\t Epoch: [77][100/149]\tTime 0.514 (0.835)\tData 0.007 (0.332)\tLoss 0.0500 (0.0510)\tSoftmaxLoss 0.0000 (0.0006)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:40:58 2022\n",
            "Step: 11593\t Epoch: [77][120/149]\tTime 1.225 (0.841)\tData 0.730 (0.337)\tLoss 0.0511 (0.0509)\tSoftmaxLoss 0.0001 (0.0005)\tRankLoss 0.0510 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:41:13 2022\n",
            "Step: 11613\t Epoch: [77][140/149]\tTime 0.513 (0.830)\tData 0.002 (0.326)\tLoss 0.0511 (0.0510)\tSoftmaxLoss 0.0000 (0.0005)\tRankLoss 0.0510 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:41:23 2022\n",
            "Test: [0/11]\tTime 3.250 (3.250)\tSoftmaxLoss 0.1389 (0.1389)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.734\n",
            "Time: Thu Mar 31 01:41:39 2022\n",
            "Step: 11622\t Epoch: [78][0/149]\tTime 1.558 (1.558)\tData 1.042 (1.042)\tLoss 0.0512 (0.0512)\tSoftmaxLoss 0.0006 (0.0006)\tRankLoss 0.0506 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:41:57 2022\n",
            "Step: 11642\t Epoch: [78][20/149]\tTime 0.506 (0.939)\tData 0.002 (0.431)\tLoss 0.0500 (0.0508)\tSoftmaxLoss 0.0000 (0.0005)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:42:14 2022\n",
            "Step: 11662\t Epoch: [78][40/149]\tTime 0.515 (0.889)\tData 0.008 (0.383)\tLoss 0.0502 (0.0509)\tSoftmaxLoss 0.0000 (0.0005)\tRankLoss 0.0502 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:42:30 2022\n",
            "Step: 11682\t Epoch: [78][60/149]\tTime 0.509 (0.864)\tData 0.001 (0.360)\tLoss 0.0501 (0.0509)\tSoftmaxLoss 0.0001 (0.0005)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:42:47 2022\n",
            "Step: 11702\t Epoch: [78][80/149]\tTime 0.511 (0.857)\tData 0.002 (0.353)\tLoss 0.0508 (0.0509)\tSoftmaxLoss 0.0005 (0.0005)\tRankLoss 0.0503 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:43:04 2022\n",
            "Step: 11722\t Epoch: [78][100/149]\tTime 0.513 (0.860)\tData 0.006 (0.356)\tLoss 0.0502 (0.0508)\tSoftmaxLoss 0.0002 (0.0005)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:43:20 2022\n",
            "Step: 11742\t Epoch: [78][120/149]\tTime 0.560 (0.846)\tData 0.060 (0.343)\tLoss 0.0500 (0.0508)\tSoftmaxLoss 0.0000 (0.0005)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:43:37 2022\n",
            "Step: 11762\t Epoch: [78][140/149]\tTime 0.522 (0.847)\tData 0.002 (0.343)\tLoss 0.0515 (0.0510)\tSoftmaxLoss 0.0013 (0.0007)\tRankLoss 0.0502 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:43:46 2022\n",
            "Test: [0/11]\tTime 3.126 (3.126)\tSoftmaxLoss 0.1028 (0.1028)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.122\n",
            "Time: Thu Mar 31 01:44:02 2022\n",
            "Step: 11771\t Epoch: [79][0/149]\tTime 2.088 (2.088)\tData 1.569 (1.569)\tLoss 0.0509 (0.0509)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0506 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:44:20 2022\n",
            "Step: 11791\t Epoch: [79][20/149]\tTime 1.343 (0.996)\tData 0.833 (0.492)\tLoss 0.0532 (0.0511)\tSoftmaxLoss 0.0012 (0.0005)\tRankLoss 0.0521 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:44:35 2022\n",
            "Step: 11811\t Epoch: [79][40/149]\tTime 0.500 (0.868)\tData 0.009 (0.363)\tLoss 0.0501 (0.0509)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:44:52 2022\n",
            "Step: 11831\t Epoch: [79][60/149]\tTime 0.495 (0.860)\tData 0.002 (0.356)\tLoss 0.0510 (0.0509)\tSoftmaxLoss 0.0002 (0.0004)\tRankLoss 0.0507 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:45:09 2022\n",
            "Step: 11851\t Epoch: [79][80/149]\tTime 0.691 (0.855)\tData 0.193 (0.351)\tLoss 0.0505 (0.0509)\tSoftmaxLoss 0.0005 (0.0005)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:45:24 2022\n",
            "Step: 11871\t Epoch: [79][100/149]\tTime 0.495 (0.836)\tData 0.002 (0.332)\tLoss 0.0515 (0.0509)\tSoftmaxLoss 0.0001 (0.0005)\tRankLoss 0.0514 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:45:40 2022\n",
            "Step: 11891\t Epoch: [79][120/149]\tTime 0.499 (0.833)\tData 0.002 (0.330)\tLoss 0.0509 (0.0509)\tSoftmaxLoss 0.0009 (0.0005)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:45:56 2022\n",
            "Step: 11911\t Epoch: [79][140/149]\tTime 0.719 (0.829)\tData 0.221 (0.326)\tLoss 0.0502 (0.0509)\tSoftmaxLoss 0.0002 (0.0004)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:46:06 2022\n",
            "Test: [0/11]\tTime 3.108 (3.108)\tSoftmaxLoss 0.1465 (0.1465)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.122\n",
            "Time: Thu Mar 31 01:46:21 2022\n",
            "Step: 11920\t Epoch: [80][0/149]\tTime 1.799 (1.799)\tData 1.300 (1.300)\tLoss 0.0512 (0.0512)\tSoftmaxLoss 0.0011 (0.0011)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:46:39 2022\n",
            "Step: 11940\t Epoch: [80][20/149]\tTime 0.572 (0.937)\tData 0.059 (0.429)\tLoss 0.0509 (0.0514)\tSoftmaxLoss 0.0004 (0.0008)\tRankLoss 0.0505 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:46:56 2022\n",
            "Step: 11960\t Epoch: [80][40/149]\tTime 0.500 (0.896)\tData 0.002 (0.390)\tLoss 0.0510 (0.0511)\tSoftmaxLoss 0.0010 (0.0007)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:47:13 2022\n",
            "Step: 11980\t Epoch: [80][60/149]\tTime 0.966 (0.874)\tData 0.456 (0.370)\tLoss 0.0510 (0.0510)\tSoftmaxLoss 0.0001 (0.0005)\tRankLoss 0.0509 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:47:28 2022\n",
            "Step: 12000\t Epoch: [80][80/149]\tTime 0.848 (0.848)\tData 0.344 (0.345)\tLoss 0.0508 (0.0510)\tSoftmaxLoss 0.0004 (0.0005)\tRankLoss 0.0504 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:47:45 2022\n",
            "Step: 12020\t Epoch: [80][100/149]\tTime 1.447 (0.846)\tData 0.948 (0.343)\tLoss 0.0504 (0.0509)\tSoftmaxLoss 0.0000 (0.0005)\tRankLoss 0.0504 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:48:01 2022\n",
            "Step: 12040\t Epoch: [80][120/149]\tTime 1.487 (0.836)\tData 0.978 (0.332)\tLoss 0.0501 (0.0508)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0501 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:48:17 2022\n",
            "Step: 12060\t Epoch: [80][140/149]\tTime 1.345 (0.833)\tData 0.841 (0.330)\tLoss 0.0501 (0.0508)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:48:26 2022\n",
            "Test: [0/11]\tTime 3.188 (3.188)\tSoftmaxLoss 0.1175 (0.1175)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.428\n",
            "Time: Thu Mar 31 01:48:42 2022\n",
            "Step: 12069\t Epoch: [81][0/149]\tTime 1.680 (1.680)\tData 1.173 (1.173)\tLoss 0.0583 (0.0583)\tSoftmaxLoss 0.0083 (0.0083)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:48:59 2022\n",
            "Step: 12089\t Epoch: [81][20/149]\tTime 1.000 (0.928)\tData 0.483 (0.414)\tLoss 0.0503 (0.0510)\tSoftmaxLoss 0.0002 (0.0006)\tRankLoss 0.0501 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:49:17 2022\n",
            "Step: 12109\t Epoch: [81][40/149]\tTime 0.512 (0.897)\tData 0.002 (0.389)\tLoss 0.0502 (0.0510)\tSoftmaxLoss 0.0002 (0.0005)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:49:32 2022\n",
            "Step: 12129\t Epoch: [81][60/149]\tTime 0.855 (0.854)\tData 0.347 (0.346)\tLoss 0.0513 (0.0509)\tSoftmaxLoss 0.0013 (0.0005)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:49:48 2022\n",
            "Step: 12149\t Epoch: [81][80/149]\tTime 1.108 (0.840)\tData 0.596 (0.334)\tLoss 0.0508 (0.0509)\tSoftmaxLoss 0.0008 (0.0005)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:50:04 2022\n",
            "Step: 12169\t Epoch: [81][100/149]\tTime 0.500 (0.834)\tData 0.002 (0.329)\tLoss 0.0501 (0.0508)\tSoftmaxLoss 0.0001 (0.0005)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:50:21 2022\n",
            "Step: 12189\t Epoch: [81][120/149]\tTime 0.522 (0.835)\tData 0.005 (0.329)\tLoss 0.0505 (0.0508)\tSoftmaxLoss 0.0005 (0.0004)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:50:37 2022\n",
            "Step: 12209\t Epoch: [81][140/149]\tTime 0.499 (0.829)\tData 0.002 (0.324)\tLoss 0.0508 (0.0508)\tSoftmaxLoss 0.0004 (0.0004)\tRankLoss 0.0505 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:50:46 2022\n",
            "Test: [0/11]\tTime 3.168 (3.168)\tSoftmaxLoss 0.1170 (0.1170)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.428\n",
            "Time: Thu Mar 31 01:51:02 2022\n",
            "Step: 12218\t Epoch: [82][0/149]\tTime 1.729 (1.729)\tData 1.234 (1.234)\tLoss 0.0502 (0.0502)\tSoftmaxLoss 0.0002 (0.0002)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:51:20 2022\n",
            "Step: 12238\t Epoch: [82][20/149]\tTime 0.534 (0.932)\tData 0.008 (0.424)\tLoss 0.0508 (0.0508)\tSoftmaxLoss 0.0007 (0.0005)\tRankLoss 0.0502 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:51:37 2022\n",
            "Step: 12258\t Epoch: [82][40/149]\tTime 1.006 (0.897)\tData 0.499 (0.390)\tLoss 0.0511 (0.0507)\tSoftmaxLoss 0.0007 (0.0004)\tRankLoss 0.0504 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:51:53 2022\n",
            "Step: 12278\t Epoch: [82][60/149]\tTime 0.496 (0.868)\tData 0.004 (0.362)\tLoss 0.0507 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0506 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:52:09 2022\n",
            "Step: 12298\t Epoch: [82][80/149]\tTime 1.068 (0.851)\tData 0.554 (0.345)\tLoss 0.0500 (0.0507)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:52:24 2022\n",
            "Step: 12318\t Epoch: [82][100/149]\tTime 0.501 (0.830)\tData 0.002 (0.326)\tLoss 0.0501 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:52:41 2022\n",
            "Step: 12338\t Epoch: [82][120/149]\tTime 0.495 (0.834)\tData 0.003 (0.330)\tLoss 0.0501 (0.0507)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:52:57 2022\n",
            "Step: 12358\t Epoch: [82][140/149]\tTime 0.981 (0.827)\tData 0.474 (0.323)\tLoss 0.0509 (0.0507)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0509 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:53:06 2022\n",
            "Test: [0/11]\tTime 3.143 (3.143)\tSoftmaxLoss 0.1323 (0.1323)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.734\n",
            "Time: Thu Mar 31 01:53:22 2022\n",
            "Step: 12367\t Epoch: [83][0/149]\tTime 1.750 (1.750)\tData 1.211 (1.211)\tLoss 0.0512 (0.0512)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0511 (0.0511)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:53:41 2022\n",
            "Step: 12387\t Epoch: [83][20/149]\tTime 1.585 (1.002)\tData 1.079 (0.493)\tLoss 0.0512 (0.0526)\tSoftmaxLoss 0.0002 (0.0022)\tRankLoss 0.0510 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:53:57 2022\n",
            "Step: 12407\t Epoch: [83][40/149]\tTime 1.006 (0.903)\tData 0.500 (0.396)\tLoss 0.0515 (0.0516)\tSoftmaxLoss 0.0005 (0.0012)\tRankLoss 0.0510 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:54:13 2022\n",
            "Step: 12427\t Epoch: [83][60/149]\tTime 0.500 (0.866)\tData 0.002 (0.361)\tLoss 0.0505 (0.0513)\tSoftmaxLoss 0.0004 (0.0009)\tRankLoss 0.0502 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:54:29 2022\n",
            "Step: 12447\t Epoch: [83][80/149]\tTime 0.515 (0.852)\tData 0.007 (0.347)\tLoss 0.0504 (0.0511)\tSoftmaxLoss 0.0001 (0.0008)\tRankLoss 0.0503 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:54:46 2022\n",
            "Step: 12467\t Epoch: [83][100/149]\tTime 0.522 (0.850)\tData 0.007 (0.347)\tLoss 0.0507 (0.0511)\tSoftmaxLoss 0.0004 (0.0007)\tRankLoss 0.0504 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:55:02 2022\n",
            "Step: 12487\t Epoch: [83][120/149]\tTime 0.890 (0.841)\tData 0.380 (0.337)\tLoss 0.0501 (0.0510)\tSoftmaxLoss 0.0001 (0.0006)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:55:19 2022\n",
            "Step: 12507\t Epoch: [83][140/149]\tTime 1.052 (0.843)\tData 0.540 (0.339)\tLoss 0.0502 (0.0510)\tSoftmaxLoss 0.0002 (0.0006)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:55:28 2022\n",
            "Test: [0/11]\tTime 3.178 (3.178)\tSoftmaxLoss 0.1149 (0.1149)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.428\n",
            "Time: Thu Mar 31 01:55:44 2022\n",
            "Step: 12516\t Epoch: [84][0/149]\tTime 2.060 (2.060)\tData 1.558 (1.558)\tLoss 0.0518 (0.0518)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0517 (0.0517)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:56:01 2022\n",
            "Step: 12536\t Epoch: [84][20/149]\tTime 0.502 (0.926)\tData 0.002 (0.418)\tLoss 0.0504 (0.0509)\tSoftmaxLoss 0.0004 (0.0005)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:56:18 2022\n",
            "Step: 12556\t Epoch: [84][40/149]\tTime 0.512 (0.880)\tData 0.003 (0.373)\tLoss 0.0509 (0.0507)\tSoftmaxLoss 0.0009 (0.0003)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:56:35 2022\n",
            "Step: 12576\t Epoch: [84][60/149]\tTime 0.520 (0.873)\tData 0.002 (0.367)\tLoss 0.0508 (0.0508)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0508 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:56:51 2022\n",
            "Step: 12596\t Epoch: [84][80/149]\tTime 0.495 (0.860)\tData 0.003 (0.355)\tLoss 0.0928 (0.0519)\tSoftmaxLoss 0.0404 (0.0014)\tRankLoss 0.0524 (0.0504)\tPrec@1 99.219 (99.990)\n",
            "Time: Thu Mar 31 01:57:08 2022\n",
            "Step: 12616\t Epoch: [84][100/149]\tTime 0.505 (0.856)\tData 0.003 (0.352)\tLoss 0.0514 (0.0516)\tSoftmaxLoss 0.0004 (0.0012)\tRankLoss 0.0510 (0.0504)\tPrec@1 100.000 (99.992)\n",
            "Time: Thu Mar 31 01:57:23 2022\n",
            "Step: 12636\t Epoch: [84][120/149]\tTime 0.511 (0.841)\tData 0.002 (0.337)\tLoss 0.0501 (0.0515)\tSoftmaxLoss 0.0001 (0.0010)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (99.994)\n",
            "Time: Thu Mar 31 01:57:39 2022\n",
            "Step: 12656\t Epoch: [84][140/149]\tTime 0.511 (0.831)\tData 0.007 (0.327)\tLoss 0.0508 (0.0514)\tSoftmaxLoss 0.0005 (0.0009)\tRankLoss 0.0503 (0.0504)\tPrec@1 100.000 (99.994)\n",
            "Time: Thu Mar 31 01:57:49 2022\n",
            "Test: [0/11]\tTime 3.143 (3.143)\tSoftmaxLoss 0.1152 (0.1152)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.122\n",
            "Time: Thu Mar 31 01:58:05 2022\n",
            "Step: 12665\t Epoch: [85][0/149]\tTime 2.066 (2.066)\tData 1.556 (1.556)\tLoss 0.0538 (0.0538)\tSoftmaxLoss 0.0015 (0.0015)\tRankLoss 0.0523 (0.0523)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:58:22 2022\n",
            "Step: 12685\t Epoch: [85][20/149]\tTime 1.032 (0.898)\tData 0.533 (0.388)\tLoss 0.0507 (0.0506)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0507 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:58:38 2022\n",
            "Step: 12705\t Epoch: [85][40/149]\tTime 0.515 (0.860)\tData 0.002 (0.354)\tLoss 0.0518 (0.0508)\tSoftmaxLoss 0.0006 (0.0005)\tRankLoss 0.0512 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:58:53 2022\n",
            "Step: 12725\t Epoch: [85][60/149]\tTime 0.977 (0.822)\tData 0.464 (0.317)\tLoss 0.0523 (0.0507)\tSoftmaxLoss 0.0005 (0.0004)\tRankLoss 0.0518 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:59:12 2022\n",
            "Step: 12745\t Epoch: [85][80/149]\tTime 2.213 (0.844)\tData 1.705 (0.340)\tLoss 0.0510 (0.0507)\tSoftmaxLoss 0.0009 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:59:26 2022\n",
            "Step: 12765\t Epoch: [85][100/149]\tTime 0.514 (0.823)\tData 0.002 (0.319)\tLoss 0.0501 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:59:42 2022\n",
            "Step: 12785\t Epoch: [85][120/149]\tTime 0.851 (0.819)\tData 0.358 (0.315)\tLoss 0.0507 (0.0508)\tSoftmaxLoss 0.0004 (0.0005)\tRankLoss 0.0503 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 01:59:59 2022\n",
            "Step: 12805\t Epoch: [85][140/149]\tTime 1.334 (0.822)\tData 0.833 (0.317)\tLoss 0.0516 (0.0508)\tSoftmaxLoss 0.0015 (0.0005)\tRankLoss 0.0502 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:00:07 2022\n",
            "Test: [0/11]\tTime 3.089 (3.089)\tSoftmaxLoss 0.1320 (0.1320)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.734\n",
            "Time: Thu Mar 31 02:00:23 2022\n",
            "Step: 12814\t Epoch: [86][0/149]\tTime 2.290 (2.290)\tData 1.774 (1.774)\tLoss 0.0515 (0.0515)\tSoftmaxLoss 0.0004 (0.0004)\tRankLoss 0.0511 (0.0511)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:00:42 2022\n",
            "Step: 12834\t Epoch: [86][20/149]\tTime 1.102 (0.976)\tData 0.588 (0.468)\tLoss 0.0500 (0.0507)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:00:57 2022\n",
            "Step: 12854\t Epoch: [86][40/149]\tTime 0.834 (0.887)\tData 0.327 (0.382)\tLoss 0.0510 (0.0507)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0507 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:01:15 2022\n",
            "Step: 12874\t Epoch: [86][60/149]\tTime 0.542 (0.886)\tData 0.008 (0.380)\tLoss 0.0502 (0.0508)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0502 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:01:31 2022\n",
            "Step: 12894\t Epoch: [86][80/149]\tTime 0.498 (0.868)\tData 0.002 (0.363)\tLoss 0.0501 (0.0507)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:01:48 2022\n",
            "Step: 12914\t Epoch: [86][100/149]\tTime 1.184 (0.863)\tData 0.679 (0.358)\tLoss 0.0501 (0.0507)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:02:04 2022\n",
            "Step: 12934\t Epoch: [86][120/149]\tTime 0.896 (0.850)\tData 0.389 (0.346)\tLoss 0.0502 (0.0507)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0501 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:02:20 2022\n",
            "Step: 12954\t Epoch: [86][140/149]\tTime 0.810 (0.846)\tData 0.315 (0.341)\tLoss 0.0507 (0.0507)\tSoftmaxLoss 0.0007 (0.0003)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:02:29 2022\n",
            "Test: [0/11]\tTime 3.164 (3.164)\tSoftmaxLoss 0.1164 (0.1164)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 81.346\n",
            "Time: Thu Mar 31 02:02:45 2022\n",
            "Step: 12963\t Epoch: [87][0/149]\tTime 1.916 (1.916)\tData 1.422 (1.422)\tLoss 0.0504 (0.0504)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0501 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:03:02 2022\n",
            "Step: 12983\t Epoch: [87][20/149]\tTime 1.253 (0.916)\tData 0.749 (0.412)\tLoss 0.0502 (0.0514)\tSoftmaxLoss 0.0002 (0.0009)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:03:20 2022\n",
            "Step: 13003\t Epoch: [87][40/149]\tTime 1.173 (0.901)\tData 0.673 (0.396)\tLoss 0.0509 (0.0510)\tSoftmaxLoss 0.0003 (0.0006)\tRankLoss 0.0507 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:03:36 2022\n",
            "Step: 13023\t Epoch: [87][60/149]\tTime 0.697 (0.874)\tData 0.204 (0.371)\tLoss 0.0500 (0.0510)\tSoftmaxLoss 0.0000 (0.0006)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:03:52 2022\n",
            "Step: 13043\t Epoch: [87][80/149]\tTime 1.514 (0.857)\tData 1.006 (0.354)\tLoss 0.0540 (0.0510)\tSoftmaxLoss 0.0024 (0.0006)\tRankLoss 0.0516 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:04:07 2022\n",
            "Step: 13063\t Epoch: [87][100/149]\tTime 0.733 (0.837)\tData 0.233 (0.334)\tLoss 0.0509 (0.0510)\tSoftmaxLoss 0.0005 (0.0005)\tRankLoss 0.0503 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:04:24 2022\n",
            "Step: 13083\t Epoch: [87][120/149]\tTime 0.508 (0.834)\tData 0.004 (0.331)\tLoss 0.0500 (0.0509)\tSoftmaxLoss 0.0000 (0.0005)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:04:39 2022\n",
            "Step: 13103\t Epoch: [87][140/149]\tTime 0.949 (0.826)\tData 0.435 (0.322)\tLoss 0.0506 (0.0509)\tSoftmaxLoss 0.0000 (0.0005)\tRankLoss 0.0506 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:04:49 2022\n",
            "Test: [0/11]\tTime 3.184 (3.184)\tSoftmaxLoss 0.1138 (0.1138)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 79.817\n",
            "Time: Thu Mar 31 02:05:05 2022\n",
            "Step: 13112\t Epoch: [88][0/149]\tTime 2.200 (2.200)\tData 1.683 (1.683)\tLoss 0.0506 (0.0506)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0505 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:05:24 2022\n",
            "Step: 13132\t Epoch: [88][20/149]\tTime 1.178 (0.993)\tData 0.662 (0.483)\tLoss 0.0515 (0.0508)\tSoftmaxLoss 0.0002 (0.0005)\tRankLoss 0.0513 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:05:39 2022\n",
            "Step: 13152\t Epoch: [88][40/149]\tTime 1.073 (0.886)\tData 0.581 (0.380)\tLoss 0.0500 (0.0507)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:05:55 2022\n",
            "Step: 13172\t Epoch: [88][60/149]\tTime 1.237 (0.849)\tData 0.743 (0.344)\tLoss 0.0507 (0.0507)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0507 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:06:11 2022\n",
            "Step: 13192\t Epoch: [88][80/149]\tTime 0.636 (0.842)\tData 0.133 (0.337)\tLoss 0.0513 (0.0518)\tSoftmaxLoss 0.0011 (0.0014)\tRankLoss 0.0502 (0.0503)\tPrec@1 100.000 (99.961)\n",
            "Time: Thu Mar 31 02:06:28 2022\n",
            "Step: 13212\t Epoch: [88][100/149]\tTime 1.217 (0.848)\tData 0.706 (0.343)\tLoss 0.0532 (0.0516)\tSoftmaxLoss 0.0032 (0.0013)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (99.969)\n",
            "Time: Thu Mar 31 02:06:45 2022\n",
            "Step: 13232\t Epoch: [88][120/149]\tTime 1.502 (0.846)\tData 0.989 (0.341)\tLoss 0.0500 (0.0514)\tSoftmaxLoss 0.0000 (0.0011)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (99.974)\n",
            "Time: Thu Mar 31 02:07:01 2022\n",
            "Step: 13252\t Epoch: [88][140/149]\tTime 0.900 (0.840)\tData 0.407 (0.336)\tLoss 0.0503 (0.0513)\tSoftmaxLoss 0.0001 (0.0010)\tRankLoss 0.0502 (0.0503)\tPrec@1 100.000 (99.978)\n",
            "Time: Thu Mar 31 02:07:10 2022\n",
            "Test: [0/11]\tTime 3.148 (3.148)\tSoftmaxLoss 0.1253 (0.1253)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 79.817\n",
            "Time: Thu Mar 31 02:07:27 2022\n",
            "Step: 13261\t Epoch: [89][0/149]\tTime 2.419 (2.419)\tData 1.919 (1.919)\tLoss 0.0503 (0.0503)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0501 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:07:45 2022\n",
            "Step: 13281\t Epoch: [89][20/149]\tTime 0.985 (0.962)\tData 0.486 (0.455)\tLoss 0.0503 (0.0509)\tSoftmaxLoss 0.0003 (0.0006)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:08:02 2022\n",
            "Step: 13301\t Epoch: [89][40/149]\tTime 1.505 (0.909)\tData 1.004 (0.404)\tLoss 0.0502 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0501 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:08:17 2022\n",
            "Step: 13321\t Epoch: [89][60/149]\tTime 0.902 (0.864)\tData 0.409 (0.359)\tLoss 0.0505 (0.0506)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0502 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:08:34 2022\n",
            "Step: 13341\t Epoch: [89][80/149]\tTime 1.227 (0.857)\tData 0.715 (0.354)\tLoss 0.0502 (0.0506)\tSoftmaxLoss 0.0002 (0.0003)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:08:50 2022\n",
            "Step: 13361\t Epoch: [89][100/149]\tTime 0.947 (0.845)\tData 0.454 (0.341)\tLoss 0.0504 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0503 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:09:06 2022\n",
            "Step: 13381\t Epoch: [89][120/149]\tTime 1.247 (0.840)\tData 0.738 (0.336)\tLoss 0.0511 (0.0508)\tSoftmaxLoss 0.0002 (0.0004)\tRankLoss 0.0509 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:09:22 2022\n",
            "Step: 13401\t Epoch: [89][140/149]\tTime 1.287 (0.833)\tData 0.785 (0.330)\tLoss 0.0506 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0505 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:09:30 2022\n",
            "Test: [0/11]\tTime 3.050 (3.050)\tSoftmaxLoss 0.1019 (0.1019)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.122\n",
            "Time: Thu Mar 31 02:09:46 2022\n",
            "Step: 13410\t Epoch: [90][0/149]\tTime 2.143 (2.143)\tData 1.640 (1.640)\tLoss 0.0507 (0.0507)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0506 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:10:04 2022\n",
            "Step: 13430\t Epoch: [90][20/149]\tTime 0.509 (0.954)\tData 0.003 (0.448)\tLoss 0.0503 (0.0508)\tSoftmaxLoss 0.0001 (0.0002)\tRankLoss 0.0501 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:10:21 2022\n",
            "Step: 13450\t Epoch: [90][40/149]\tTime 1.539 (0.916)\tData 1.046 (0.411)\tLoss 0.0502 (0.0511)\tSoftmaxLoss 0.0002 (0.0007)\tRankLoss 0.0501 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:10:36 2022\n",
            "Step: 13470\t Epoch: [90][60/149]\tTime 0.941 (0.854)\tData 0.432 (0.351)\tLoss 0.0501 (0.0514)\tSoftmaxLoss 0.0001 (0.0011)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:10:52 2022\n",
            "Step: 13490\t Epoch: [90][80/149]\tTime 0.898 (0.836)\tData 0.400 (0.333)\tLoss 0.0511 (0.0513)\tSoftmaxLoss 0.0011 (0.0009)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:11:09 2022\n",
            "Step: 13510\t Epoch: [90][100/149]\tTime 0.799 (0.838)\tData 0.292 (0.334)\tLoss 0.0502 (0.0511)\tSoftmaxLoss 0.0002 (0.0008)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:11:25 2022\n",
            "Step: 13530\t Epoch: [90][120/149]\tTime 1.226 (0.833)\tData 0.712 (0.330)\tLoss 0.0507 (0.0511)\tSoftmaxLoss 0.0001 (0.0007)\tRankLoss 0.0506 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:11:40 2022\n",
            "Step: 13550\t Epoch: [90][140/149]\tTime 0.521 (0.827)\tData 0.002 (0.324)\tLoss 0.0514 (0.0510)\tSoftmaxLoss 0.0003 (0.0006)\tRankLoss 0.0511 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:11:50 2022\n",
            "Test: [0/11]\tTime 3.095 (3.095)\tSoftmaxLoss 0.1252 (0.1252)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.428\n",
            "Time: Thu Mar 31 02:12:06 2022\n",
            "Step: 13559\t Epoch: [91][0/149]\tTime 1.932 (1.932)\tData 1.417 (1.417)\tLoss 0.0549 (0.0549)\tSoftmaxLoss 0.0049 (0.0049)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:12:23 2022\n",
            "Step: 13579\t Epoch: [91][20/149]\tTime 0.936 (0.918)\tData 0.437 (0.410)\tLoss 0.0559 (0.0510)\tSoftmaxLoss 0.0054 (0.0007)\tRankLoss 0.0505 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:12:40 2022\n",
            "Step: 13599\t Epoch: [91][40/149]\tTime 0.495 (0.883)\tData 0.003 (0.377)\tLoss 0.0532 (0.0509)\tSoftmaxLoss 0.0001 (0.0005)\tRankLoss 0.0531 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:12:57 2022\n",
            "Step: 13619\t Epoch: [91][60/149]\tTime 0.500 (0.872)\tData 0.002 (0.366)\tLoss 0.0500 (0.0512)\tSoftmaxLoss 0.0000 (0.0008)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:13:13 2022\n",
            "Step: 13639\t Epoch: [91][80/149]\tTime 1.286 (0.850)\tData 0.780 (0.344)\tLoss 0.0505 (0.0510)\tSoftmaxLoss 0.0003 (0.0006)\tRankLoss 0.0502 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:13:30 2022\n",
            "Step: 13659\t Epoch: [91][100/149]\tTime 1.114 (0.854)\tData 0.617 (0.349)\tLoss 0.0501 (0.0519)\tSoftmaxLoss 0.0000 (0.0015)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (99.954)\n",
            "Time: Thu Mar 31 02:13:47 2022\n",
            "Step: 13679\t Epoch: [91][120/149]\tTime 1.077 (0.848)\tData 0.585 (0.344)\tLoss 0.0519 (0.0517)\tSoftmaxLoss 0.0013 (0.0013)\tRankLoss 0.0505 (0.0504)\tPrec@1 100.000 (99.961)\n",
            "Time: Thu Mar 31 02:14:02 2022\n",
            "Step: 13699\t Epoch: [91][140/149]\tTime 0.788 (0.836)\tData 0.287 (0.332)\tLoss 0.0527 (0.0516)\tSoftmaxLoss 0.0020 (0.0012)\tRankLoss 0.0506 (0.0504)\tPrec@1 100.000 (99.967)\n",
            "Time: Thu Mar 31 02:14:11 2022\n",
            "Test: [0/11]\tTime 3.117 (3.117)\tSoftmaxLoss 0.1230 (0.1230)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.734\n",
            "Time: Thu Mar 31 02:14:27 2022\n",
            "Step: 13708\t Epoch: [92][0/149]\tTime 2.160 (2.160)\tData 1.665 (1.665)\tLoss 0.0500 (0.0500)\tSoftmaxLoss 0.0000 (0.0000)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:14:44 2022\n",
            "Step: 13728\t Epoch: [92][20/149]\tTime 0.918 (0.897)\tData 0.417 (0.393)\tLoss 0.0503 (0.0509)\tSoftmaxLoss 0.0003 (0.0006)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:15:00 2022\n",
            "Step: 13748\t Epoch: [92][40/149]\tTime 1.046 (0.857)\tData 0.538 (0.354)\tLoss 0.0503 (0.0508)\tSoftmaxLoss 0.0003 (0.0005)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:15:16 2022\n",
            "Step: 13768\t Epoch: [92][60/149]\tTime 0.868 (0.836)\tData 0.375 (0.334)\tLoss 0.0503 (0.0507)\tSoftmaxLoss 0.0003 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:15:32 2022\n",
            "Step: 13788\t Epoch: [92][80/149]\tTime 0.732 (0.834)\tData 0.228 (0.332)\tLoss 0.0500 (0.0507)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:15:48 2022\n",
            "Step: 13808\t Epoch: [92][100/149]\tTime 0.904 (0.827)\tData 0.391 (0.325)\tLoss 0.0519 (0.0507)\tSoftmaxLoss 0.0004 (0.0004)\tRankLoss 0.0515 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:16:04 2022\n",
            "Step: 13828\t Epoch: [92][120/149]\tTime 1.049 (0.816)\tData 0.546 (0.314)\tLoss 0.0502 (0.0507)\tSoftmaxLoss 0.0002 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:16:20 2022\n",
            "Step: 13848\t Epoch: [92][140/149]\tTime 1.321 (0.816)\tData 0.816 (0.314)\tLoss 0.0505 (0.0506)\tSoftmaxLoss 0.0002 (0.0003)\tRankLoss 0.0502 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:16:29 2022\n",
            "Test: [0/11]\tTime 3.112 (3.112)\tSoftmaxLoss 0.1294 (0.1294)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.734\n",
            "Time: Thu Mar 31 02:16:45 2022\n",
            "Step: 13857\t Epoch: [93][0/149]\tTime 2.367 (2.367)\tData 1.855 (1.855)\tLoss 0.0506 (0.0506)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0502 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:17:04 2022\n",
            "Step: 13877\t Epoch: [93][20/149]\tTime 1.572 (0.981)\tData 1.058 (0.470)\tLoss 0.0504 (0.0510)\tSoftmaxLoss 0.0002 (0.0003)\tRankLoss 0.0502 (0.0507)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:17:20 2022\n",
            "Step: 13897\t Epoch: [93][40/149]\tTime 1.421 (0.904)\tData 0.929 (0.398)\tLoss 0.0507 (0.0509)\tSoftmaxLoss 0.0007 (0.0003)\tRankLoss 0.0500 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:17:35 2022\n",
            "Step: 13917\t Epoch: [93][60/149]\tTime 0.945 (0.847)\tData 0.439 (0.342)\tLoss 0.0501 (0.0516)\tSoftmaxLoss 0.0001 (0.0011)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:17:52 2022\n",
            "Step: 13937\t Epoch: [93][80/149]\tTime 1.346 (0.853)\tData 0.831 (0.348)\tLoss 0.0500 (0.0514)\tSoftmaxLoss 0.0000 (0.0010)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:18:08 2022\n",
            "Step: 13957\t Epoch: [93][100/149]\tTime 0.975 (0.840)\tData 0.470 (0.336)\tLoss 0.0504 (0.0514)\tSoftmaxLoss 0.0002 (0.0009)\tRankLoss 0.0502 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:18:23 2022\n",
            "Step: 13977\t Epoch: [93][120/149]\tTime 0.933 (0.827)\tData 0.421 (0.323)\tLoss 0.0503 (0.0513)\tSoftmaxLoss 0.0002 (0.0009)\tRankLoss 0.0502 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:18:40 2022\n",
            "Step: 13997\t Epoch: [93][140/149]\tTime 1.071 (0.829)\tData 0.576 (0.325)\tLoss 0.0503 (0.0513)\tSoftmaxLoss 0.0003 (0.0008)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:18:49 2022\n",
            "Test: [0/11]\tTime 3.191 (3.191)\tSoftmaxLoss 0.1128 (0.1128)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.734\n",
            "Time: Thu Mar 31 02:19:04 2022\n",
            "Step: 14006\t Epoch: [94][0/149]\tTime 1.687 (1.687)\tData 1.176 (1.176)\tLoss 0.0500 (0.0500)\tSoftmaxLoss 0.0000 (0.0000)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:19:22 2022\n",
            "Step: 14026\t Epoch: [94][20/149]\tTime 0.499 (0.905)\tData 0.007 (0.398)\tLoss 0.0517 (0.0506)\tSoftmaxLoss 0.0002 (0.0003)\tRankLoss 0.0515 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:19:39 2022\n",
            "Step: 14046\t Epoch: [94][40/149]\tTime 0.501 (0.882)\tData 0.002 (0.379)\tLoss 0.0502 (0.0507)\tSoftmaxLoss 0.0002 (0.0003)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:19:54 2022\n",
            "Step: 14066\t Epoch: [94][60/149]\tTime 0.500 (0.847)\tData 0.003 (0.344)\tLoss 0.0504 (0.0508)\tSoftmaxLoss 0.0004 (0.0003)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:20:10 2022\n",
            "Step: 14086\t Epoch: [94][80/149]\tTime 0.495 (0.835)\tData 0.002 (0.332)\tLoss 0.0507 (0.0507)\tSoftmaxLoss 0.0005 (0.0003)\tRankLoss 0.0503 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:20:26 2022\n",
            "Step: 14106\t Epoch: [94][100/149]\tTime 0.587 (0.829)\tData 0.083 (0.325)\tLoss 0.0501 (0.0508)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:20:44 2022\n",
            "Step: 14126\t Epoch: [94][120/149]\tTime 0.497 (0.834)\tData 0.002 (0.330)\tLoss 0.0501 (0.0509)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:21:00 2022\n",
            "Step: 14146\t Epoch: [94][140/149]\tTime 1.763 (0.832)\tData 1.268 (0.328)\tLoss 0.0502 (0.0509)\tSoftmaxLoss 0.0002 (0.0005)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:21:09 2022\n",
            "Test: [0/11]\tTime 3.137 (3.137)\tSoftmaxLoss 0.0963 (0.0963)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 79.817\n",
            "Time: Thu Mar 31 02:21:25 2022\n",
            "Step: 14155\t Epoch: [95][0/149]\tTime 1.835 (1.835)\tData 1.331 (1.331)\tLoss 0.0501 (0.0501)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:21:41 2022\n",
            "Step: 14175\t Epoch: [95][20/149]\tTime 0.875 (0.860)\tData 0.375 (0.352)\tLoss 0.0503 (0.0511)\tSoftmaxLoss 0.0003 (0.0009)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:21:58 2022\n",
            "Step: 14195\t Epoch: [95][40/149]\tTime 0.495 (0.847)\tData 0.002 (0.342)\tLoss 0.0507 (0.0509)\tSoftmaxLoss 0.0004 (0.0006)\tRankLoss 0.0503 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:22:14 2022\n",
            "Step: 14215\t Epoch: [95][60/149]\tTime 0.495 (0.837)\tData 0.002 (0.332)\tLoss 0.0519 (0.0508)\tSoftmaxLoss 0.0001 (0.0005)\tRankLoss 0.0519 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:22:29 2022\n",
            "Step: 14235\t Epoch: [95][80/149]\tTime 0.879 (0.821)\tData 0.376 (0.317)\tLoss 0.0506 (0.0508)\tSoftmaxLoss 0.0000 (0.0005)\tRankLoss 0.0506 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:22:46 2022\n",
            "Step: 14255\t Epoch: [95][100/149]\tTime 0.882 (0.824)\tData 0.388 (0.321)\tLoss 0.0508 (0.0509)\tSoftmaxLoss 0.0002 (0.0005)\tRankLoss 0.0505 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:23:03 2022\n",
            "Step: 14275\t Epoch: [95][120/149]\tTime 0.658 (0.824)\tData 0.157 (0.320)\tLoss 0.0502 (0.0509)\tSoftmaxLoss 0.0000 (0.0005)\tRankLoss 0.0502 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:23:18 2022\n",
            "Step: 14295\t Epoch: [95][140/149]\tTime 0.587 (0.814)\tData 0.096 (0.311)\tLoss 0.0518 (0.0509)\tSoftmaxLoss 0.0018 (0.0005)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:23:27 2022\n",
            "Test: [0/11]\tTime 3.159 (3.159)\tSoftmaxLoss 0.1271 (0.1271)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.734\n",
            "Time: Thu Mar 31 02:23:43 2022\n",
            "Step: 14304\t Epoch: [96][0/149]\tTime 2.096 (2.096)\tData 1.578 (1.578)\tLoss 0.0506 (0.0506)\tSoftmaxLoss 0.0002 (0.0002)\tRankLoss 0.0504 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:24:01 2022\n",
            "Step: 14324\t Epoch: [96][20/149]\tTime 1.405 (0.959)\tData 0.901 (0.453)\tLoss 0.0517 (0.0510)\tSoftmaxLoss 0.0017 (0.0006)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:24:19 2022\n",
            "Step: 14344\t Epoch: [96][40/149]\tTime 1.444 (0.919)\tData 0.953 (0.416)\tLoss 0.0510 (0.0508)\tSoftmaxLoss 0.0002 (0.0004)\tRankLoss 0.0507 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:24:34 2022\n",
            "Step: 14364\t Epoch: [96][60/149]\tTime 0.852 (0.869)\tData 0.352 (0.366)\tLoss 0.0506 (0.0507)\tSoftmaxLoss 0.0006 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:24:50 2022\n",
            "Step: 14384\t Epoch: [96][80/149]\tTime 1.553 (0.850)\tData 1.048 (0.347)\tLoss 0.0505 (0.0508)\tSoftmaxLoss 0.0002 (0.0004)\tRankLoss 0.0503 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:25:07 2022\n",
            "Step: 14404\t Epoch: [96][100/149]\tTime 1.159 (0.848)\tData 0.658 (0.345)\tLoss 0.0502 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0502 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:25:22 2022\n",
            "Step: 14424\t Epoch: [96][120/149]\tTime 0.653 (0.832)\tData 0.155 (0.329)\tLoss 0.0537 (0.0507)\tSoftmaxLoss 0.0010 (0.0004)\tRankLoss 0.0527 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:25:38 2022\n",
            "Step: 14444\t Epoch: [96][140/149]\tTime 1.099 (0.833)\tData 0.596 (0.329)\tLoss 0.0515 (0.0507)\tSoftmaxLoss 0.0012 (0.0004)\tRankLoss 0.0503 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:25:47 2022\n",
            "Test: [0/11]\tTime 3.104 (3.104)\tSoftmaxLoss 0.1183 (0.1183)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.428\n",
            "Time: Thu Mar 31 02:26:03 2022\n",
            "Step: 14453\t Epoch: [97][0/149]\tTime 2.033 (2.033)\tData 1.528 (1.528)\tLoss 0.0509 (0.0509)\tSoftmaxLoss 0.0002 (0.0002)\tRankLoss 0.0508 (0.0508)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:26:21 2022\n",
            "Step: 14473\t Epoch: [97][20/149]\tTime 1.298 (0.960)\tData 0.792 (0.453)\tLoss 0.0503 (0.0512)\tSoftmaxLoss 0.0003 (0.0006)\tRankLoss 0.0500 (0.0506)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:26:37 2022\n",
            "Step: 14493\t Epoch: [97][40/149]\tTime 1.065 (0.885)\tData 0.572 (0.380)\tLoss 0.0504 (0.0510)\tSoftmaxLoss 0.0001 (0.0005)\tRankLoss 0.0504 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:26:53 2022\n",
            "Step: 14513\t Epoch: [97][60/149]\tTime 0.922 (0.866)\tData 0.416 (0.362)\tLoss 0.0504 (0.0508)\tSoftmaxLoss 0.0004 (0.0004)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:27:10 2022\n",
            "Step: 14533\t Epoch: [97][80/149]\tTime 0.966 (0.861)\tData 0.459 (0.358)\tLoss 0.0500 (0.0508)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:27:26 2022\n",
            "Step: 14553\t Epoch: [97][100/149]\tTime 1.122 (0.844)\tData 0.621 (0.341)\tLoss 0.0541 (0.0509)\tSoftmaxLoss 0.0032 (0.0004)\tRankLoss 0.0509 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:27:42 2022\n",
            "Step: 14573\t Epoch: [97][120/149]\tTime 0.931 (0.839)\tData 0.432 (0.335)\tLoss 0.0501 (0.0508)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:27:58 2022\n",
            "Step: 14593\t Epoch: [97][140/149]\tTime 0.887 (0.833)\tData 0.391 (0.330)\tLoss 0.0505 (0.0509)\tSoftmaxLoss 0.0002 (0.0005)\tRankLoss 0.0503 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:28:07 2022\n",
            "Test: [0/11]\tTime 3.194 (3.194)\tSoftmaxLoss 0.1182 (0.1182)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.428\n",
            "Time: Thu Mar 31 02:28:23 2022\n",
            "Step: 14602\t Epoch: [98][0/149]\tTime 1.593 (1.593)\tData 1.074 (1.074)\tLoss 0.0503 (0.0503)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:28:41 2022\n",
            "Step: 14622\t Epoch: [98][20/149]\tTime 0.874 (0.933)\tData 0.362 (0.426)\tLoss 0.0528 (0.0507)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0528 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:28:56 2022\n",
            "Step: 14642\t Epoch: [98][40/149]\tTime 0.952 (0.859)\tData 0.449 (0.355)\tLoss 0.0500 (0.0508)\tSoftmaxLoss 0.0000 (0.0005)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:29:12 2022\n",
            "Step: 14662\t Epoch: [98][60/149]\tTime 1.154 (0.839)\tData 0.649 (0.335)\tLoss 0.0510 (0.0507)\tSoftmaxLoss 0.0010 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:29:28 2022\n",
            "Step: 14682\t Epoch: [98][80/149]\tTime 0.907 (0.831)\tData 0.407 (0.328)\tLoss 0.0501 (0.0509)\tSoftmaxLoss 0.0001 (0.0006)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:29:45 2022\n",
            "Step: 14702\t Epoch: [98][100/149]\tTime 1.239 (0.834)\tData 0.738 (0.331)\tLoss 0.0512 (0.0509)\tSoftmaxLoss 0.0008 (0.0005)\tRankLoss 0.0503 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:30:02 2022\n",
            "Step: 14722\t Epoch: [98][120/149]\tTime 0.726 (0.832)\tData 0.224 (0.329)\tLoss 0.0515 (0.0508)\tSoftmaxLoss 0.0001 (0.0005)\tRankLoss 0.0513 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:30:19 2022\n",
            "Step: 14742\t Epoch: [98][140/149]\tTime 1.463 (0.834)\tData 0.958 (0.331)\tLoss 0.0506 (0.0508)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0504 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:30:27 2022\n",
            "Test: [0/11]\tTime 3.151 (3.151)\tSoftmaxLoss 0.1265 (0.1265)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.734\n",
            "Time: Thu Mar 31 02:30:43 2022\n",
            "Step: 14751\t Epoch: [99][0/149]\tTime 1.838 (1.838)\tData 1.333 (1.333)\tLoss 0.0509 (0.0509)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0507 (0.0507)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:31:00 2022\n",
            "Step: 14771\t Epoch: [99][20/149]\tTime 0.520 (0.929)\tData 0.005 (0.420)\tLoss 0.0500 (0.0509)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:31:17 2022\n",
            "Step: 14791\t Epoch: [99][40/149]\tTime 0.502 (0.879)\tData 0.002 (0.373)\tLoss 0.0507 (0.0511)\tSoftmaxLoss 0.0007 (0.0007)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:31:33 2022\n",
            "Step: 14811\t Epoch: [99][60/149]\tTime 0.507 (0.857)\tData 0.003 (0.351)\tLoss 0.0509 (0.0513)\tSoftmaxLoss 0.0002 (0.0009)\tRankLoss 0.0507 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:31:49 2022\n",
            "Step: 14831\t Epoch: [99][80/149]\tTime 0.510 (0.843)\tData 0.002 (0.337)\tLoss 0.0509 (0.0511)\tSoftmaxLoss 0.0001 (0.0007)\tRankLoss 0.0509 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:32:05 2022\n",
            "Step: 14851\t Epoch: [99][100/149]\tTime 0.504 (0.835)\tData 0.002 (0.330)\tLoss 0.0500 (0.0511)\tSoftmaxLoss 0.0000 (0.0007)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:32:21 2022\n",
            "Step: 14871\t Epoch: [99][120/149]\tTime 1.345 (0.825)\tData 0.833 (0.321)\tLoss 0.0515 (0.0511)\tSoftmaxLoss 0.0004 (0.0008)\tRankLoss 0.0511 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:32:37 2022\n",
            "Step: 14891\t Epoch: [99][140/149]\tTime 1.163 (0.821)\tData 0.657 (0.317)\tLoss 0.0507 (0.0511)\tSoftmaxLoss 0.0000 (0.0008)\tRankLoss 0.0507 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:32:46 2022\n",
            "Test: [0/11]\tTime 3.110 (3.110)\tSoftmaxLoss 0.1112 (0.1112)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 81.040\n",
            "Time: Thu Mar 31 02:33:02 2022\n",
            "Step: 14900\t Epoch: [100][0/149]\tTime 1.720 (1.720)\tData 1.207 (1.207)\tLoss 0.0503 (0.0503)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:33:19 2022\n",
            "Step: 14920\t Epoch: [100][20/149]\tTime 1.393 (0.902)\tData 0.878 (0.394)\tLoss 0.0505 (0.0504)\tSoftmaxLoss 0.0003 (0.0002)\tRankLoss 0.0502 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:33:35 2022\n",
            "Step: 14940\t Epoch: [100][40/149]\tTime 0.718 (0.840)\tData 0.205 (0.334)\tLoss 0.0516 (0.0506)\tSoftmaxLoss 0.0004 (0.0004)\tRankLoss 0.0512 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:33:50 2022\n",
            "Step: 14960\t Epoch: [100][60/149]\tTime 0.508 (0.821)\tData 0.002 (0.317)\tLoss 0.0505 (0.0507)\tSoftmaxLoss 0.0005 (0.0004)\tRankLoss 0.0501 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:34:06 2022\n",
            "Step: 14980\t Epoch: [100][80/149]\tTime 0.510 (0.813)\tData 0.006 (0.310)\tLoss 0.0542 (0.0507)\tSoftmaxLoss 0.0041 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:34:23 2022\n",
            "Step: 15000\t Epoch: [100][100/149]\tTime 1.333 (0.822)\tData 0.825 (0.318)\tLoss 0.0504 (0.0507)\tSoftmaxLoss 0.0003 (0.0004)\tRankLoss 0.0501 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:34:39 2022\n",
            "Step: 15020\t Epoch: [100][120/149]\tTime 0.496 (0.818)\tData 0.002 (0.315)\tLoss 0.0502 (0.0508)\tSoftmaxLoss 0.0001 (0.0005)\tRankLoss 0.0501 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:34:55 2022\n",
            "Step: 15040\t Epoch: [100][140/149]\tTime 0.503 (0.817)\tData 0.002 (0.314)\tLoss 0.0500 (0.0508)\tSoftmaxLoss 0.0000 (0.0005)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:35:05 2022\n",
            "Test: [0/11]\tTime 3.151 (3.151)\tSoftmaxLoss 0.1334 (0.1334)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.734\n",
            "Time: Thu Mar 31 02:35:20 2022\n",
            "Step: 15049\t Epoch: [101][0/149]\tTime 1.739 (1.739)\tData 1.245 (1.245)\tLoss 0.0510 (0.0510)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0507 (0.0507)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:35:38 2022\n",
            "Step: 15069\t Epoch: [101][20/149]\tTime 0.718 (0.924)\tData 0.214 (0.410)\tLoss 0.0500 (0.0508)\tSoftmaxLoss 0.0000 (0.0005)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:35:55 2022\n",
            "Step: 15089\t Epoch: [101][40/149]\tTime 1.286 (0.880)\tData 0.780 (0.371)\tLoss 0.0505 (0.0507)\tSoftmaxLoss 0.0004 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:36:10 2022\n",
            "Step: 15109\t Epoch: [101][60/149]\tTime 0.812 (0.842)\tData 0.314 (0.334)\tLoss 0.0500 (0.0507)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:36:26 2022\n",
            "Step: 15129\t Epoch: [101][80/149]\tTime 1.119 (0.837)\tData 0.624 (0.331)\tLoss 0.0510 (0.0507)\tSoftmaxLoss 0.0007 (0.0003)\tRankLoss 0.0503 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:36:42 2022\n",
            "Step: 15149\t Epoch: [101][100/149]\tTime 0.495 (0.828)\tData 0.003 (0.323)\tLoss 0.0505 (0.0506)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0503 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:36:58 2022\n",
            "Step: 15169\t Epoch: [101][120/149]\tTime 0.496 (0.821)\tData 0.002 (0.316)\tLoss 0.0502 (0.0506)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0502 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:37:16 2022\n",
            "Step: 15189\t Epoch: [101][140/149]\tTime 1.484 (0.830)\tData 0.967 (0.326)\tLoss 0.0501 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:37:24 2022\n",
            "Test: [0/11]\tTime 3.149 (3.149)\tSoftmaxLoss 0.1240 (0.1240)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.428\n",
            "Time: Thu Mar 31 02:37:40 2022\n",
            "Step: 15198\t Epoch: [102][0/149]\tTime 2.222 (2.222)\tData 1.706 (1.706)\tLoss 0.0505 (0.0505)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0501 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:37:58 2022\n",
            "Step: 15218\t Epoch: [102][20/149]\tTime 1.218 (0.928)\tData 0.711 (0.421)\tLoss 0.0505 (0.0509)\tSoftmaxLoss 0.0002 (0.0004)\tRankLoss 0.0504 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:38:14 2022\n",
            "Step: 15238\t Epoch: [102][40/149]\tTime 1.288 (0.872)\tData 0.777 (0.367)\tLoss 0.0536 (0.0508)\tSoftmaxLoss 0.0015 (0.0003)\tRankLoss 0.0521 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:38:31 2022\n",
            "Step: 15258\t Epoch: [102][60/149]\tTime 1.409 (0.859)\tData 0.894 (0.356)\tLoss 0.0501 (0.0509)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:38:46 2022\n",
            "Step: 15278\t Epoch: [102][80/149]\tTime 1.399 (0.838)\tData 0.907 (0.336)\tLoss 0.0501 (0.0509)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:39:03 2022\n",
            "Step: 15298\t Epoch: [102][100/149]\tTime 1.348 (0.842)\tData 0.848 (0.340)\tLoss 0.0512 (0.0509)\tSoftmaxLoss 0.0010 (0.0004)\tRankLoss 0.0502 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:39:18 2022\n",
            "Step: 15318\t Epoch: [102][120/149]\tTime 0.670 (0.822)\tData 0.163 (0.319)\tLoss 0.0509 (0.0508)\tSoftmaxLoss 0.0002 (0.0004)\tRankLoss 0.0507 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:39:34 2022\n",
            "Step: 15338\t Epoch: [102][140/149]\tTime 0.494 (0.818)\tData 0.002 (0.316)\tLoss 0.0502 (0.0507)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0501 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:39:43 2022\n",
            "Test: [0/11]\tTime 3.107 (3.107)\tSoftmaxLoss 0.1147 (0.1147)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.734\n",
            "Time: Thu Mar 31 02:40:00 2022\n",
            "Step: 15347\t Epoch: [103][0/149]\tTime 2.539 (2.539)\tData 2.040 (2.040)\tLoss 0.0507 (0.0507)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0503 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:40:17 2022\n",
            "Step: 15367\t Epoch: [103][20/149]\tTime 0.907 (0.922)\tData 0.401 (0.415)\tLoss 0.0508 (0.0505)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0505 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:40:33 2022\n",
            "Step: 15387\t Epoch: [103][40/149]\tTime 1.478 (0.875)\tData 0.963 (0.369)\tLoss 0.0509 (0.0507)\tSoftmaxLoss 0.0004 (0.0003)\tRankLoss 0.0504 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:40:49 2022\n",
            "Step: 15407\t Epoch: [103][60/149]\tTime 0.632 (0.842)\tData 0.130 (0.337)\tLoss 0.0500 (0.0507)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:41:06 2022\n",
            "Step: 15427\t Epoch: [103][80/149]\tTime 0.506 (0.843)\tData 0.008 (0.339)\tLoss 0.0506 (0.0506)\tSoftmaxLoss 0.0006 (0.0003)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:41:21 2022\n",
            "Step: 15447\t Epoch: [103][100/149]\tTime 1.154 (0.833)\tData 0.647 (0.329)\tLoss 0.0569 (0.0507)\tSoftmaxLoss 0.0051 (0.0003)\tRankLoss 0.0518 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:41:37 2022\n",
            "Step: 15467\t Epoch: [103][120/149]\tTime 1.288 (0.827)\tData 0.787 (0.323)\tLoss 0.0508 (0.0507)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0505 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:41:53 2022\n",
            "Step: 15487\t Epoch: [103][140/149]\tTime 0.873 (0.823)\tData 0.379 (0.320)\tLoss 0.0510 (0.0507)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0510 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:42:03 2022\n",
            "Test: [0/11]\tTime 3.248 (3.248)\tSoftmaxLoss 0.1377 (0.1377)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 81.651\n",
            "Time: Thu Mar 31 02:42:18 2022\n",
            "Step: 15496\t Epoch: [104][0/149]\tTime 1.479 (1.479)\tData 0.979 (0.979)\tLoss 0.0503 (0.0503)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:42:36 2022\n",
            "Step: 15516\t Epoch: [104][20/149]\tTime 0.509 (0.920)\tData 0.002 (0.413)\tLoss 0.0505 (0.0512)\tSoftmaxLoss 0.0000 (0.0007)\tRankLoss 0.0505 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:42:52 2022\n",
            "Step: 15536\t Epoch: [104][40/149]\tTime 0.578 (0.852)\tData 0.071 (0.347)\tLoss 0.0506 (0.0510)\tSoftmaxLoss 0.0001 (0.0006)\tRankLoss 0.0506 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:43:09 2022\n",
            "Step: 15556\t Epoch: [104][60/149]\tTime 1.102 (0.846)\tData 0.605 (0.342)\tLoss 0.0506 (0.0509)\tSoftmaxLoss 0.0003 (0.0005)\tRankLoss 0.0503 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:43:25 2022\n",
            "Step: 15576\t Epoch: [104][80/149]\tTime 0.517 (0.836)\tData 0.002 (0.332)\tLoss 0.0500 (0.0508)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:43:42 2022\n",
            "Step: 15596\t Epoch: [104][100/149]\tTime 0.505 (0.841)\tData 0.003 (0.337)\tLoss 0.0501 (0.0508)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:43:57 2022\n",
            "Step: 15616\t Epoch: [104][120/149]\tTime 0.500 (0.829)\tData 0.003 (0.325)\tLoss 0.0503 (0.0509)\tSoftmaxLoss 0.0003 (0.0005)\tRankLoss 0.0501 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:44:14 2022\n",
            "Step: 15636\t Epoch: [104][140/149]\tTime 0.505 (0.832)\tData 0.002 (0.329)\tLoss 0.0504 (0.0509)\tSoftmaxLoss 0.0002 (0.0005)\tRankLoss 0.0502 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:44:24 2022\n",
            "Test: [0/11]\tTime 3.184 (3.184)\tSoftmaxLoss 0.1095 (0.1095)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.122\n",
            "Time: Thu Mar 31 02:44:40 2022\n",
            "Step: 15645\t Epoch: [105][0/149]\tTime 2.022 (2.022)\tData 1.520 (1.520)\tLoss 0.0501 (0.0501)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:44:58 2022\n",
            "Step: 15665\t Epoch: [105][20/149]\tTime 0.783 (0.930)\tData 0.278 (0.426)\tLoss 0.0503 (0.0506)\tSoftmaxLoss 0.0000 (0.0002)\tRankLoss 0.0503 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:45:14 2022\n",
            "Step: 15685\t Epoch: [105][40/149]\tTime 0.511 (0.868)\tData 0.002 (0.364)\tLoss 0.0502 (0.0506)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0501 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:45:31 2022\n",
            "Step: 15705\t Epoch: [105][60/149]\tTime 1.370 (0.871)\tData 0.865 (0.367)\tLoss 0.0503 (0.0507)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0502 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:45:47 2022\n",
            "Step: 15725\t Epoch: [105][80/149]\tTime 0.523 (0.845)\tData 0.006 (0.340)\tLoss 0.0500 (0.0509)\tSoftmaxLoss 0.0000 (0.0005)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:46:04 2022\n",
            "Step: 15745\t Epoch: [105][100/149]\tTime 0.509 (0.850)\tData 0.002 (0.345)\tLoss 0.0506 (0.0508)\tSoftmaxLoss 0.0006 (0.0004)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:46:19 2022\n",
            "Step: 15765\t Epoch: [105][120/149]\tTime 0.505 (0.835)\tData 0.002 (0.330)\tLoss 0.0513 (0.0509)\tSoftmaxLoss 0.0013 (0.0005)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:46:35 2022\n",
            "Step: 15785\t Epoch: [105][140/149]\tTime 0.503 (0.831)\tData 0.002 (0.327)\tLoss 0.0500 (0.0508)\tSoftmaxLoss 0.0000 (0.0005)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:46:44 2022\n",
            "Test: [0/11]\tTime 3.056 (3.056)\tSoftmaxLoss 0.1359 (0.1359)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 81.040\n",
            "Time: Thu Mar 31 02:47:00 2022\n",
            "Step: 15794\t Epoch: [106][0/149]\tTime 1.821 (1.821)\tData 1.295 (1.295)\tLoss 0.0500 (0.0500)\tSoftmaxLoss 0.0000 (0.0000)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:47:19 2022\n",
            "Step: 15814\t Epoch: [106][20/149]\tTime 1.129 (0.959)\tData 0.627 (0.449)\tLoss 0.0504 (0.0506)\tSoftmaxLoss 0.0000 (0.0002)\tRankLoss 0.0504 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:47:35 2022\n",
            "Step: 15834\t Epoch: [106][40/149]\tTime 0.497 (0.889)\tData 0.002 (0.384)\tLoss 0.0504 (0.0508)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0501 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:47:50 2022\n",
            "Step: 15854\t Epoch: [106][60/149]\tTime 0.761 (0.851)\tData 0.264 (0.347)\tLoss 0.0513 (0.0507)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0510 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:48:07 2022\n",
            "Step: 15874\t Epoch: [106][80/149]\tTime 1.393 (0.841)\tData 0.895 (0.337)\tLoss 0.0503 (0.0510)\tSoftmaxLoss 0.0002 (0.0007)\tRankLoss 0.0501 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:48:22 2022\n",
            "Step: 15894\t Epoch: [106][100/149]\tTime 1.435 (0.827)\tData 0.931 (0.324)\tLoss 0.0503 (0.0509)\tSoftmaxLoss 0.0003 (0.0006)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:48:38 2022\n",
            "Step: 15914\t Epoch: [106][120/149]\tTime 1.196 (0.824)\tData 0.702 (0.321)\tLoss 0.0502 (0.0509)\tSoftmaxLoss 0.0001 (0.0005)\tRankLoss 0.0502 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:48:54 2022\n",
            "Step: 15934\t Epoch: [106][140/149]\tTime 1.308 (0.820)\tData 0.806 (0.316)\tLoss 0.0506 (0.0508)\tSoftmaxLoss 0.0006 (0.0005)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:49:04 2022\n",
            "Test: [0/11]\tTime 3.152 (3.152)\tSoftmaxLoss 0.1238 (0.1238)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.734\n",
            "Time: Thu Mar 31 02:49:20 2022\n",
            "Step: 15943\t Epoch: [107][0/149]\tTime 1.890 (1.890)\tData 1.364 (1.364)\tLoss 0.0504 (0.0504)\tSoftmaxLoss 0.0002 (0.0002)\tRankLoss 0.0501 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:49:37 2022\n",
            "Step: 15963\t Epoch: [107][20/149]\tTime 0.511 (0.923)\tData 0.008 (0.417)\tLoss 0.0504 (0.0507)\tSoftmaxLoss 0.0003 (0.0004)\tRankLoss 0.0501 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:49:54 2022\n",
            "Step: 15983\t Epoch: [107][40/149]\tTime 0.506 (0.869)\tData 0.005 (0.365)\tLoss 0.0502 (0.0506)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0502 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:50:09 2022\n",
            "Step: 16003\t Epoch: [107][60/149]\tTime 0.495 (0.840)\tData 0.002 (0.336)\tLoss 0.0508 (0.0507)\tSoftmaxLoss 0.0002 (0.0004)\tRankLoss 0.0506 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:50:25 2022\n",
            "Step: 16023\t Epoch: [107][80/149]\tTime 0.495 (0.832)\tData 0.002 (0.329)\tLoss 0.0501 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0501 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:50:42 2022\n",
            "Step: 16043\t Epoch: [107][100/149]\tTime 1.136 (0.827)\tData 0.637 (0.324)\tLoss 0.0516 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0515 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:50:58 2022\n",
            "Step: 16063\t Epoch: [107][120/149]\tTime 0.505 (0.823)\tData 0.003 (0.320)\tLoss 0.0505 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0504 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:51:14 2022\n",
            "Step: 16083\t Epoch: [107][140/149]\tTime 0.717 (0.819)\tData 0.214 (0.316)\tLoss 0.0502 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0501 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:51:22 2022\n",
            "Test: [0/11]\tTime 3.102 (3.102)\tSoftmaxLoss 0.1099 (0.1099)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 81.040\n",
            "Time: Thu Mar 31 02:51:38 2022\n",
            "Step: 16092\t Epoch: [108][0/149]\tTime 1.537 (1.537)\tData 1.037 (1.037)\tLoss 0.0507 (0.0507)\tSoftmaxLoss 0.0002 (0.0002)\tRankLoss 0.0505 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:51:56 2022\n",
            "Step: 16112\t Epoch: [108][20/149]\tTime 0.507 (0.926)\tData 0.005 (0.420)\tLoss 0.0505 (0.0507)\tSoftmaxLoss 0.0005 (0.0004)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:52:11 2022\n",
            "Step: 16132\t Epoch: [108][40/149]\tTime 0.501 (0.859)\tData 0.003 (0.355)\tLoss 0.0520 (0.0509)\tSoftmaxLoss 0.0016 (0.0006)\tRankLoss 0.0504 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:52:28 2022\n",
            "Step: 16152\t Epoch: [108][60/149]\tTime 0.501 (0.851)\tData 0.002 (0.347)\tLoss 0.0500 (0.0510)\tSoftmaxLoss 0.0000 (0.0006)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:52:44 2022\n",
            "Step: 16172\t Epoch: [108][80/149]\tTime 0.497 (0.831)\tData 0.002 (0.327)\tLoss 0.0514 (0.0509)\tSoftmaxLoss 0.0003 (0.0006)\tRankLoss 0.0510 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:53:00 2022\n",
            "Step: 16192\t Epoch: [108][100/149]\tTime 0.501 (0.830)\tData 0.002 (0.326)\tLoss 0.0501 (0.0509)\tSoftmaxLoss 0.0001 (0.0006)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:53:16 2022\n",
            "Step: 16212\t Epoch: [108][120/149]\tTime 1.069 (0.825)\tData 0.559 (0.322)\tLoss 0.0508 (0.0509)\tSoftmaxLoss 0.0003 (0.0005)\tRankLoss 0.0504 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:53:32 2022\n",
            "Step: 16232\t Epoch: [108][140/149]\tTime 1.087 (0.823)\tData 0.596 (0.319)\tLoss 0.0502 (0.0508)\tSoftmaxLoss 0.0002 (0.0005)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:53:41 2022\n",
            "Test: [0/11]\tTime 3.096 (3.096)\tSoftmaxLoss 0.1276 (0.1276)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.734\n",
            "Time: Thu Mar 31 02:53:58 2022\n",
            "Step: 16241\t Epoch: [109][0/149]\tTime 2.320 (2.320)\tData 1.810 (1.810)\tLoss 0.0500 (0.0500)\tSoftmaxLoss 0.0000 (0.0000)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:54:16 2022\n",
            "Step: 16261\t Epoch: [109][20/149]\tTime 1.299 (0.969)\tData 0.791 (0.464)\tLoss 0.0502 (0.0513)\tSoftmaxLoss 0.0002 (0.0009)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:54:32 2022\n",
            "Step: 16281\t Epoch: [109][40/149]\tTime 1.174 (0.889)\tData 0.682 (0.385)\tLoss 0.0504 (0.0510)\tSoftmaxLoss 0.0001 (0.0006)\tRankLoss 0.0504 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:54:47 2022\n",
            "Step: 16301\t Epoch: [109][60/149]\tTime 0.516 (0.851)\tData 0.002 (0.347)\tLoss 0.0518 (0.0509)\tSoftmaxLoss 0.0004 (0.0005)\tRankLoss 0.0514 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:55:03 2022\n",
            "Step: 16321\t Epoch: [109][80/149]\tTime 0.511 (0.832)\tData 0.007 (0.329)\tLoss 0.0579 (0.0509)\tSoftmaxLoss 0.0078 (0.0005)\tRankLoss 0.0501 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:55:21 2022\n",
            "Step: 16341\t Epoch: [109][100/149]\tTime 1.288 (0.846)\tData 0.784 (0.342)\tLoss 0.0501 (0.0510)\tSoftmaxLoss 0.0001 (0.0006)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:55:37 2022\n",
            "Step: 16361\t Epoch: [109][120/149]\tTime 1.484 (0.838)\tData 0.984 (0.335)\tLoss 0.0501 (0.0510)\tSoftmaxLoss 0.0000 (0.0006)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:55:53 2022\n",
            "Step: 16381\t Epoch: [109][140/149]\tTime 1.258 (0.833)\tData 0.747 (0.329)\tLoss 0.0506 (0.0509)\tSoftmaxLoss 0.0000 (0.0005)\tRankLoss 0.0506 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:56:02 2022\n",
            "Test: [0/11]\tTime 3.086 (3.086)\tSoftmaxLoss 0.1116 (0.1116)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.428\n",
            "Time: Thu Mar 31 02:56:18 2022\n",
            "Step: 16390\t Epoch: [110][0/149]\tTime 1.878 (1.878)\tData 1.384 (1.384)\tLoss 0.0501 (0.0501)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:56:36 2022\n",
            "Step: 16410\t Epoch: [110][20/149]\tTime 0.587 (0.914)\tData 0.094 (0.408)\tLoss 0.0503 (0.0508)\tSoftmaxLoss 0.0001 (0.0005)\tRankLoss 0.0502 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:56:53 2022\n",
            "Step: 16430\t Epoch: [110][40/149]\tTime 1.145 (0.891)\tData 0.638 (0.386)\tLoss 0.0509 (0.0507)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0508 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:57:08 2022\n",
            "Step: 16450\t Epoch: [110][60/149]\tTime 1.074 (0.847)\tData 0.566 (0.343)\tLoss 0.0501 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:57:25 2022\n",
            "Step: 16470\t Epoch: [110][80/149]\tTime 1.640 (0.842)\tData 1.127 (0.338)\tLoss 0.0501 (0.0508)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:57:40 2022\n",
            "Step: 16490\t Epoch: [110][100/149]\tTime 0.515 (0.829)\tData 0.003 (0.325)\tLoss 0.0506 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0505 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:57:56 2022\n",
            "Step: 16510\t Epoch: [110][120/149]\tTime 1.210 (0.828)\tData 0.711 (0.323)\tLoss 0.0502 (0.0507)\tSoftmaxLoss 0.0002 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:58:14 2022\n",
            "Step: 16530\t Epoch: [110][140/149]\tTime 0.565 (0.834)\tData 0.056 (0.330)\tLoss 0.0502 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0502 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:58:23 2022\n",
            "Test: [0/11]\tTime 3.087 (3.087)\tSoftmaxLoss 0.1093 (0.1093)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.734\n",
            "Time: Thu Mar 31 02:58:38 2022\n",
            "Step: 16539\t Epoch: [111][0/149]\tTime 1.876 (1.876)\tData 1.380 (1.380)\tLoss 0.0511 (0.0511)\tSoftmaxLoss 0.0011 (0.0011)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:58:56 2022\n",
            "Step: 16559\t Epoch: [111][20/149]\tTime 1.028 (0.918)\tData 0.526 (0.411)\tLoss 0.0503 (0.0506)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0502 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:59:11 2022\n",
            "Step: 16579\t Epoch: [111][40/149]\tTime 1.128 (0.849)\tData 0.624 (0.343)\tLoss 0.0506 (0.0506)\tSoftmaxLoss 0.0006 (0.0004)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:59:28 2022\n",
            "Step: 16599\t Epoch: [111][60/149]\tTime 0.499 (0.837)\tData 0.002 (0.332)\tLoss 0.0515 (0.0506)\tSoftmaxLoss 0.0010 (0.0003)\tRankLoss 0.0505 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:59:44 2022\n",
            "Step: 16619\t Epoch: [111][80/149]\tTime 0.502 (0.831)\tData 0.003 (0.327)\tLoss 0.0503 (0.0507)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0502 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 02:59:59 2022\n",
            "Step: 16639\t Epoch: [111][100/149]\tTime 0.511 (0.819)\tData 0.002 (0.315)\tLoss 0.0524 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0522 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:00:16 2022\n",
            "Step: 16659\t Epoch: [111][120/149]\tTime 0.505 (0.824)\tData 0.003 (0.319)\tLoss 0.0501 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:00:32 2022\n",
            "Step: 16679\t Epoch: [111][140/149]\tTime 0.501 (0.822)\tData 0.003 (0.318)\tLoss 0.0505 (0.0507)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0504 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:00:41 2022\n",
            "Test: [0/11]\tTime 3.177 (3.177)\tSoftmaxLoss 0.1432 (0.1432)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 81.040\n",
            "Time: Thu Mar 31 03:00:57 2022\n",
            "Step: 16688\t Epoch: [112][0/149]\tTime 1.549 (1.549)\tData 1.042 (1.042)\tLoss 0.0501 (0.0501)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:01:15 2022\n",
            "Step: 16708\t Epoch: [112][20/149]\tTime 1.388 (0.940)\tData 0.895 (0.432)\tLoss 0.0503 (0.0507)\tSoftmaxLoss 0.0001 (0.0005)\tRankLoss 0.0502 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:01:31 2022\n",
            "Step: 16728\t Epoch: [112][40/149]\tTime 1.140 (0.884)\tData 0.638 (0.377)\tLoss 0.0501 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:01:47 2022\n",
            "Step: 16748\t Epoch: [112][60/149]\tTime 0.755 (0.854)\tData 0.255 (0.348)\tLoss 0.0501 (0.0508)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:02:02 2022\n",
            "Step: 16768\t Epoch: [112][80/149]\tTime 1.198 (0.828)\tData 0.699 (0.322)\tLoss 0.0504 (0.0507)\tSoftmaxLoss 0.0002 (0.0003)\tRankLoss 0.0502 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:02:18 2022\n",
            "Step: 16788\t Epoch: [112][100/149]\tTime 0.956 (0.822)\tData 0.439 (0.316)\tLoss 0.0503 (0.0507)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:02:34 2022\n",
            "Step: 16808\t Epoch: [112][120/149]\tTime 0.502 (0.819)\tData 0.002 (0.313)\tLoss 0.0501 (0.0507)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0501 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:02:50 2022\n",
            "Step: 16828\t Epoch: [112][140/149]\tTime 0.505 (0.818)\tData 0.003 (0.312)\tLoss 0.0524 (0.0507)\tSoftmaxLoss 0.0017 (0.0003)\tRankLoss 0.0507 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:03:00 2022\n",
            "Test: [0/11]\tTime 3.132 (3.132)\tSoftmaxLoss 0.1056 (0.1056)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.428\n",
            "Time: Thu Mar 31 03:03:15 2022\n",
            "Step: 16837\t Epoch: [113][0/149]\tTime 2.064 (2.064)\tData 1.560 (1.560)\tLoss 0.0506 (0.0506)\tSoftmaxLoss 0.0002 (0.0002)\tRankLoss 0.0504 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:03:34 2022\n",
            "Step: 16857\t Epoch: [113][20/149]\tTime 1.176 (0.971)\tData 0.661 (0.464)\tLoss 0.0501 (0.0505)\tSoftmaxLoss 0.0001 (0.0002)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:03:51 2022\n",
            "Step: 16877\t Epoch: [113][40/149]\tTime 1.524 (0.915)\tData 1.024 (0.410)\tLoss 0.0512 (0.0505)\tSoftmaxLoss 0.0003 (0.0002)\tRankLoss 0.0510 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:04:06 2022\n",
            "Step: 16897\t Epoch: [113][60/149]\tTime 1.146 (0.868)\tData 0.637 (0.364)\tLoss 0.0502 (0.0505)\tSoftmaxLoss 0.0002 (0.0002)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:04:22 2022\n",
            "Step: 16917\t Epoch: [113][80/149]\tTime 1.186 (0.853)\tData 0.680 (0.350)\tLoss 0.0501 (0.0505)\tSoftmaxLoss 0.0001 (0.0002)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:04:38 2022\n",
            "Step: 16937\t Epoch: [113][100/149]\tTime 0.512 (0.837)\tData 0.002 (0.334)\tLoss 0.0501 (0.0506)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:04:54 2022\n",
            "Step: 16957\t Epoch: [113][120/149]\tTime 0.606 (0.832)\tData 0.103 (0.328)\tLoss 0.0504 (0.0506)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0504 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:05:11 2022\n",
            "Step: 16977\t Epoch: [113][140/149]\tTime 0.506 (0.832)\tData 0.002 (0.328)\tLoss 0.0501 (0.0506)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0501 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:05:20 2022\n",
            "Test: [0/11]\tTime 3.148 (3.148)\tSoftmaxLoss 0.1289 (0.1289)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.734\n",
            "Time: Thu Mar 31 03:05:36 2022\n",
            "Step: 16986\t Epoch: [114][0/149]\tTime 1.778 (1.778)\tData 1.269 (1.269)\tLoss 0.0506 (0.0506)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0505 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:05:53 2022\n",
            "Step: 17006\t Epoch: [114][20/149]\tTime 0.496 (0.898)\tData 0.002 (0.394)\tLoss 0.0501 (0.0508)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:06:09 2022\n",
            "Step: 17026\t Epoch: [114][40/149]\tTime 0.522 (0.859)\tData 0.013 (0.356)\tLoss 0.0514 (0.0508)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0513 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:06:25 2022\n",
            "Step: 17046\t Epoch: [114][60/149]\tTime 0.899 (0.847)\tData 0.389 (0.344)\tLoss 0.0511 (0.0508)\tSoftmaxLoss 0.0011 (0.0004)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:06:41 2022\n",
            "Step: 17066\t Epoch: [114][80/149]\tTime 0.650 (0.833)\tData 0.157 (0.331)\tLoss 0.0508 (0.0509)\tSoftmaxLoss 0.0002 (0.0005)\tRankLoss 0.0506 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:06:57 2022\n",
            "Step: 17086\t Epoch: [114][100/149]\tTime 0.507 (0.822)\tData 0.002 (0.319)\tLoss 0.0507 (0.0508)\tSoftmaxLoss 0.0006 (0.0005)\tRankLoss 0.0501 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:07:13 2022\n",
            "Step: 17106\t Epoch: [114][120/149]\tTime 0.498 (0.823)\tData 0.002 (0.319)\tLoss 0.0500 (0.0508)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:07:30 2022\n",
            "Step: 17126\t Epoch: [114][140/149]\tTime 0.519 (0.822)\tData 0.002 (0.319)\tLoss 0.0503 (0.0508)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0502 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:07:39 2022\n",
            "Test: [0/11]\tTime 3.086 (3.086)\tSoftmaxLoss 0.1348 (0.1348)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.428\n",
            "Time: Thu Mar 31 03:07:56 2022\n",
            "Step: 17135\t Epoch: [115][0/149]\tTime 2.231 (2.231)\tData 1.726 (1.726)\tLoss 0.0501 (0.0501)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:08:12 2022\n",
            "Step: 17155\t Epoch: [115][20/149]\tTime 0.779 (0.888)\tData 0.287 (0.385)\tLoss 0.0503 (0.0506)\tSoftmaxLoss 0.0003 (0.0004)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:08:28 2022\n",
            "Step: 17175\t Epoch: [115][40/149]\tTime 0.878 (0.837)\tData 0.373 (0.333)\tLoss 0.0500 (0.0507)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:08:44 2022\n",
            "Step: 17195\t Epoch: [115][60/149]\tTime 1.005 (0.831)\tData 0.511 (0.328)\tLoss 0.0508 (0.0508)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0508 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:09:01 2022\n",
            "Step: 17215\t Epoch: [115][80/149]\tTime 0.679 (0.830)\tData 0.173 (0.325)\tLoss 0.0513 (0.0507)\tSoftmaxLoss 0.0010 (0.0004)\tRankLoss 0.0504 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:09:17 2022\n",
            "Step: 17235\t Epoch: [115][100/149]\tTime 0.520 (0.826)\tData 0.010 (0.322)\tLoss 0.0509 (0.0507)\tSoftmaxLoss 0.0006 (0.0004)\tRankLoss 0.0503 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:09:33 2022\n",
            "Step: 17255\t Epoch: [115][120/149]\tTime 0.501 (0.820)\tData 0.009 (0.316)\tLoss 0.0501 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:09:49 2022\n",
            "Step: 17275\t Epoch: [115][140/149]\tTime 0.502 (0.817)\tData 0.002 (0.313)\tLoss 0.0504 (0.0507)\tSoftmaxLoss 0.0004 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:09:58 2022\n",
            "Test: [0/11]\tTime 3.162 (3.162)\tSoftmaxLoss 0.1182 (0.1182)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.734\n",
            "Time: Thu Mar 31 03:10:14 2022\n",
            "Step: 17284\t Epoch: [116][0/149]\tTime 1.695 (1.695)\tData 1.179 (1.179)\tLoss 0.0510 (0.0510)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0509 (0.0509)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:10:31 2022\n",
            "Step: 17304\t Epoch: [116][20/149]\tTime 0.507 (0.909)\tData 0.003 (0.402)\tLoss 0.0501 (0.0521)\tSoftmaxLoss 0.0001 (0.0014)\tRankLoss 0.0500 (0.0507)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:10:47 2022\n",
            "Step: 17324\t Epoch: [116][40/149]\tTime 0.515 (0.853)\tData 0.002 (0.347)\tLoss 0.0500 (0.0514)\tSoftmaxLoss 0.0000 (0.0008)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:11:03 2022\n",
            "Step: 17344\t Epoch: [116][60/149]\tTime 0.992 (0.830)\tData 0.489 (0.325)\tLoss 0.0504 (0.0511)\tSoftmaxLoss 0.0004 (0.0007)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:11:19 2022\n",
            "Step: 17364\t Epoch: [116][80/149]\tTime 0.502 (0.830)\tData 0.002 (0.325)\tLoss 0.0501 (0.0510)\tSoftmaxLoss 0.0001 (0.0006)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:11:35 2022\n",
            "Step: 17384\t Epoch: [116][100/149]\tTime 0.504 (0.826)\tData 0.002 (0.322)\tLoss 0.0501 (0.0511)\tSoftmaxLoss 0.0001 (0.0007)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:11:52 2022\n",
            "Step: 17404\t Epoch: [116][120/149]\tTime 0.900 (0.827)\tData 0.407 (0.323)\tLoss 0.0505 (0.0510)\tSoftmaxLoss 0.0000 (0.0006)\tRankLoss 0.0505 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:12:10 2022\n",
            "Step: 17424\t Epoch: [116][140/149]\tTime 0.668 (0.834)\tData 0.168 (0.330)\tLoss 0.0500 (0.0510)\tSoftmaxLoss 0.0000 (0.0006)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:12:18 2022\n",
            "Test: [0/11]\tTime 3.061 (3.061)\tSoftmaxLoss 0.1342 (0.1342)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.428\n",
            "Time: Thu Mar 31 03:12:34 2022\n",
            "Step: 17433\t Epoch: [117][0/149]\tTime 1.972 (1.972)\tData 1.469 (1.469)\tLoss 0.0502 (0.0502)\tSoftmaxLoss 0.0002 (0.0002)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:12:52 2022\n",
            "Step: 17453\t Epoch: [117][20/149]\tTime 1.212 (0.966)\tData 0.704 (0.460)\tLoss 0.0511 (0.0509)\tSoftmaxLoss 0.0003 (0.0006)\tRankLoss 0.0508 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:13:08 2022\n",
            "Step: 17473\t Epoch: [117][40/149]\tTime 1.263 (0.884)\tData 0.764 (0.380)\tLoss 0.0501 (0.0506)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:13:24 2022\n",
            "Step: 17493\t Epoch: [117][60/149]\tTime 0.870 (0.861)\tData 0.372 (0.358)\tLoss 0.0505 (0.0507)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0504 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:13:40 2022\n",
            "Step: 17513\t Epoch: [117][80/149]\tTime 1.194 (0.837)\tData 0.699 (0.334)\tLoss 0.0501 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:13:56 2022\n",
            "Step: 17533\t Epoch: [117][100/149]\tTime 1.186 (0.832)\tData 0.688 (0.329)\tLoss 0.0504 (0.0508)\tSoftmaxLoss 0.0001 (0.0005)\tRankLoss 0.0503 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:14:11 2022\n",
            "Step: 17553\t Epoch: [117][120/149]\tTime 0.548 (0.822)\tData 0.051 (0.319)\tLoss 0.0500 (0.0507)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:14:28 2022\n",
            "Step: 17573\t Epoch: [117][140/149]\tTime 1.169 (0.825)\tData 0.668 (0.322)\tLoss 0.0503 (0.0508)\tSoftmaxLoss 0.0000 (0.0005)\tRankLoss 0.0502 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:14:37 2022\n",
            "Test: [0/11]\tTime 3.152 (3.152)\tSoftmaxLoss 0.1227 (0.1227)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 79.817\n",
            "Time: Thu Mar 31 03:14:53 2022\n",
            "Step: 17582\t Epoch: [118][0/149]\tTime 2.141 (2.141)\tData 1.625 (1.625)\tLoss 0.0502 (0.0502)\tSoftmaxLoss 0.0002 (0.0002)\tRankLoss 0.0501 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:15:11 2022\n",
            "Step: 17602\t Epoch: [118][20/149]\tTime 1.067 (0.948)\tData 0.573 (0.441)\tLoss 0.0502 (0.0505)\tSoftmaxLoss 0.0002 (0.0002)\tRankLoss 0.0501 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:15:27 2022\n",
            "Step: 17622\t Epoch: [118][40/149]\tTime 1.041 (0.879)\tData 0.539 (0.375)\tLoss 0.0506 (0.0505)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0506 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:15:44 2022\n",
            "Step: 17642\t Epoch: [118][60/149]\tTime 1.187 (0.868)\tData 0.685 (0.364)\tLoss 0.0504 (0.0505)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0503 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:15:59 2022\n",
            "Step: 17662\t Epoch: [118][80/149]\tTime 1.058 (0.846)\tData 0.548 (0.342)\tLoss 0.0501 (0.0505)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:16:16 2022\n",
            "Step: 17682\t Epoch: [118][100/149]\tTime 0.496 (0.841)\tData 0.003 (0.338)\tLoss 0.0545 (0.0506)\tSoftmaxLoss 0.0045 (0.0003)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:16:33 2022\n",
            "Step: 17702\t Epoch: [118][120/149]\tTime 0.521 (0.844)\tData 0.009 (0.341)\tLoss 0.0518 (0.0506)\tSoftmaxLoss 0.0018 (0.0003)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:16:47 2022\n",
            "Step: 17722\t Epoch: [118][140/149]\tTime 0.509 (0.827)\tData 0.002 (0.323)\tLoss 0.0506 (0.0506)\tSoftmaxLoss 0.0006 (0.0003)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:16:57 2022\n",
            "Test: [0/11]\tTime 3.164 (3.164)\tSoftmaxLoss 0.0912 (0.0912)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.428\n",
            "Time: Thu Mar 31 03:17:13 2022\n",
            "Step: 17731\t Epoch: [119][0/149]\tTime 2.127 (2.127)\tData 1.623 (1.623)\tLoss 0.0509 (0.0509)\tSoftmaxLoss 0.0004 (0.0004)\tRankLoss 0.0505 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:17:31 2022\n",
            "Step: 17751\t Epoch: [119][20/149]\tTime 1.034 (0.936)\tData 0.535 (0.427)\tLoss 0.0506 (0.0510)\tSoftmaxLoss 0.0006 (0.0005)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:17:48 2022\n",
            "Step: 17771\t Epoch: [119][40/149]\tTime 1.112 (0.891)\tData 0.598 (0.383)\tLoss 0.0509 (0.0509)\tSoftmaxLoss 0.0003 (0.0005)\tRankLoss 0.0506 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:18:04 2022\n",
            "Step: 17791\t Epoch: [119][60/149]\tTime 1.237 (0.870)\tData 0.722 (0.364)\tLoss 0.0501 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:18:22 2022\n",
            "Step: 17811\t Epoch: [119][80/149]\tTime 1.057 (0.870)\tData 0.555 (0.364)\tLoss 0.0500 (0.0506)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:18:38 2022\n",
            "Step: 17831\t Epoch: [119][100/149]\tTime 0.638 (0.858)\tData 0.137 (0.353)\tLoss 0.0502 (0.0506)\tSoftmaxLoss 0.0002 (0.0003)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:18:54 2022\n",
            "Step: 17851\t Epoch: [119][120/149]\tTime 0.503 (0.852)\tData 0.002 (0.347)\tLoss 0.0505 (0.0506)\tSoftmaxLoss 0.0005 (0.0003)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:19:11 2022\n",
            "Step: 17871\t Epoch: [119][140/149]\tTime 1.297 (0.848)\tData 0.796 (0.343)\tLoss 0.0507 (0.0507)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0507 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:19:20 2022\n",
            "Test: [0/11]\tTime 3.164 (3.164)\tSoftmaxLoss 0.1132 (0.1132)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.428\n",
            "Time: Thu Mar 31 03:19:36 2022\n",
            "Step: 17880\t Epoch: [120][0/149]\tTime 1.791 (1.791)\tData 1.280 (1.280)\tLoss 0.0521 (0.0521)\tSoftmaxLoss 0.0002 (0.0002)\tRankLoss 0.0520 (0.0520)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:19:53 2022\n",
            "Step: 17900\t Epoch: [120][20/149]\tTime 0.528 (0.893)\tData 0.006 (0.379)\tLoss 0.0501 (0.0505)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:20:08 2022\n",
            "Step: 17920\t Epoch: [120][40/149]\tTime 0.854 (0.830)\tData 0.361 (0.321)\tLoss 0.0502 (0.0505)\tSoftmaxLoss 0.0002 (0.0002)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:20:25 2022\n",
            "Step: 17940\t Epoch: [120][60/149]\tTime 0.504 (0.827)\tData 0.002 (0.320)\tLoss 0.0505 (0.0507)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0504 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:20:41 2022\n",
            "Step: 17960\t Epoch: [120][80/149]\tTime 0.499 (0.823)\tData 0.003 (0.317)\tLoss 0.0504 (0.0506)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0502 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:20:58 2022\n",
            "Step: 17980\t Epoch: [120][100/149]\tTime 0.512 (0.830)\tData 0.003 (0.324)\tLoss 0.0570 (0.0508)\tSoftmaxLoss 0.0068 (0.0005)\tRankLoss 0.0502 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:21:13 2022\n",
            "Step: 18000\t Epoch: [120][120/149]\tTime 0.657 (0.818)\tData 0.151 (0.313)\tLoss 0.0501 (0.0508)\tSoftmaxLoss 0.0000 (0.0005)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:21:31 2022\n",
            "Step: 18020\t Epoch: [120][140/149]\tTime 0.503 (0.824)\tData 0.002 (0.319)\tLoss 0.0516 (0.0508)\tSoftmaxLoss 0.0003 (0.0005)\tRankLoss 0.0513 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:21:40 2022\n",
            "Test: [0/11]\tTime 3.133 (3.133)\tSoftmaxLoss 0.1028 (0.1028)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.122\n",
            "Time: Thu Mar 31 03:21:55 2022\n",
            "Step: 18029\t Epoch: [121][0/149]\tTime 1.564 (1.564)\tData 1.056 (1.056)\tLoss 0.0503 (0.0503)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:22:11 2022\n",
            "Step: 18049\t Epoch: [121][20/149]\tTime 0.509 (0.844)\tData 0.003 (0.335)\tLoss 0.0505 (0.0507)\tSoftmaxLoss 0.0003 (0.0004)\tRankLoss 0.0502 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:22:28 2022\n",
            "Step: 18069\t Epoch: [121][40/149]\tTime 0.515 (0.834)\tData 0.002 (0.328)\tLoss 0.0510 (0.0507)\tSoftmaxLoss 0.0002 (0.0004)\tRankLoss 0.0508 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:22:45 2022\n",
            "Step: 18089\t Epoch: [121][60/149]\tTime 0.507 (0.840)\tData 0.003 (0.336)\tLoss 0.0505 (0.0508)\tSoftmaxLoss 0.0004 (0.0004)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:23:00 2022\n",
            "Step: 18109\t Epoch: [121][80/149]\tTime 0.591 (0.822)\tData 0.086 (0.318)\tLoss 0.0506 (0.0507)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0503 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:23:16 2022\n",
            "Step: 18129\t Epoch: [121][100/149]\tTime 1.059 (0.819)\tData 0.562 (0.314)\tLoss 0.0500 (0.0507)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:23:32 2022\n",
            "Step: 18149\t Epoch: [121][120/149]\tTime 1.129 (0.816)\tData 0.630 (0.312)\tLoss 0.0501 (0.0507)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:23:49 2022\n",
            "Step: 18169\t Epoch: [121][140/149]\tTime 0.508 (0.816)\tData 0.003 (0.312)\tLoss 0.0500 (0.0507)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:23:59 2022\n",
            "Test: [0/11]\tTime 3.171 (3.171)\tSoftmaxLoss 0.0978 (0.0978)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 79.511\n",
            "Time: Thu Mar 31 03:24:15 2022\n",
            "Step: 18178\t Epoch: [122][0/149]\tTime 2.287 (2.287)\tData 1.793 (1.793)\tLoss 0.0524 (0.0524)\tSoftmaxLoss 0.0021 (0.0021)\tRankLoss 0.0503 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:24:33 2022\n",
            "Step: 18198\t Epoch: [122][20/149]\tTime 1.192 (0.993)\tData 0.685 (0.487)\tLoss 0.0514 (0.0508)\tSoftmaxLoss 0.0008 (0.0003)\tRankLoss 0.0506 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:24:50 2022\n",
            "Step: 18218\t Epoch: [122][40/149]\tTime 0.883 (0.908)\tData 0.378 (0.403)\tLoss 0.0503 (0.0507)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0500 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:25:06 2022\n",
            "Step: 18238\t Epoch: [122][60/149]\tTime 0.733 (0.878)\tData 0.240 (0.373)\tLoss 0.0507 (0.0507)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0506 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:25:23 2022\n",
            "Step: 18258\t Epoch: [122][80/149]\tTime 0.923 (0.864)\tData 0.421 (0.360)\tLoss 0.0500 (0.0507)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:25:39 2022\n",
            "Step: 18278\t Epoch: [122][100/149]\tTime 1.430 (0.852)\tData 0.932 (0.348)\tLoss 0.0501 (0.0507)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:25:54 2022\n",
            "Step: 18298\t Epoch: [122][120/149]\tTime 0.507 (0.840)\tData 0.007 (0.336)\tLoss 0.0508 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0507 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:26:10 2022\n",
            "Step: 18318\t Epoch: [122][140/149]\tTime 0.505 (0.836)\tData 0.006 (0.332)\tLoss 0.0505 (0.0506)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0504 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:26:20 2022\n",
            "Test: [0/11]\tTime 3.179 (3.179)\tSoftmaxLoss 0.1045 (0.1045)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 79.817\n",
            "Time: Thu Mar 31 03:26:36 2022\n",
            "Step: 18327\t Epoch: [123][0/149]\tTime 2.094 (2.094)\tData 1.586 (1.586)\tLoss 0.0502 (0.0502)\tSoftmaxLoss 0.0000 (0.0000)\tRankLoss 0.0501 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:26:54 2022\n",
            "Step: 18347\t Epoch: [123][20/149]\tTime 1.215 (0.932)\tData 0.715 (0.424)\tLoss 0.0507 (0.0506)\tSoftmaxLoss 0.0001 (0.0002)\tRankLoss 0.0506 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:27:10 2022\n",
            "Step: 18367\t Epoch: [123][40/149]\tTime 0.506 (0.865)\tData 0.002 (0.360)\tLoss 0.0502 (0.0507)\tSoftmaxLoss 0.0002 (0.0003)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:27:27 2022\n",
            "Step: 18387\t Epoch: [123][60/149]\tTime 0.495 (0.859)\tData 0.002 (0.356)\tLoss 0.0502 (0.0506)\tSoftmaxLoss 0.0002 (0.0003)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:27:42 2022\n",
            "Step: 18407\t Epoch: [123][80/149]\tTime 0.502 (0.837)\tData 0.002 (0.334)\tLoss 0.0505 (0.0508)\tSoftmaxLoss 0.0003 (0.0006)\tRankLoss 0.0502 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:27:58 2022\n",
            "Step: 18427\t Epoch: [123][100/149]\tTime 0.576 (0.827)\tData 0.063 (0.324)\tLoss 0.0500 (0.0508)\tSoftmaxLoss 0.0000 (0.0005)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:28:15 2022\n",
            "Step: 18447\t Epoch: [123][120/149]\tTime 0.513 (0.829)\tData 0.010 (0.326)\tLoss 0.0500 (0.0508)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:28:31 2022\n",
            "Step: 18467\t Epoch: [123][140/149]\tTime 0.510 (0.826)\tData 0.003 (0.323)\tLoss 0.0503 (0.0508)\tSoftmaxLoss 0.0003 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:28:41 2022\n",
            "Test: [0/11]\tTime 3.119 (3.119)\tSoftmaxLoss 0.0928 (0.0928)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.122\n",
            "Time: Thu Mar 31 03:28:56 2022\n",
            "Step: 18476\t Epoch: [124][0/149]\tTime 1.613 (1.613)\tData 1.097 (1.097)\tLoss 0.0501 (0.0501)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:29:14 2022\n",
            "Step: 18496\t Epoch: [124][20/149]\tTime 0.503 (0.921)\tData 0.003 (0.418)\tLoss 0.0501 (0.0506)\tSoftmaxLoss 0.0001 (0.0002)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:29:30 2022\n",
            "Step: 18516\t Epoch: [124][40/149]\tTime 0.497 (0.860)\tData 0.002 (0.358)\tLoss 0.0502 (0.0506)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:29:47 2022\n",
            "Step: 18536\t Epoch: [124][60/149]\tTime 0.549 (0.854)\tData 0.047 (0.353)\tLoss 0.0511 (0.0506)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0508 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:30:03 2022\n",
            "Step: 18556\t Epoch: [124][80/149]\tTime 0.515 (0.836)\tData 0.007 (0.335)\tLoss 0.0502 (0.0506)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0501 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:30:19 2022\n",
            "Step: 18576\t Epoch: [124][100/149]\tTime 0.500 (0.830)\tData 0.002 (0.328)\tLoss 0.0502 (0.0506)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0501 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:30:35 2022\n",
            "Step: 18596\t Epoch: [124][120/149]\tTime 0.518 (0.827)\tData 0.017 (0.326)\tLoss 0.0501 (0.0507)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0500 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:30:50 2022\n",
            "Step: 18616\t Epoch: [124][140/149]\tTime 0.828 (0.817)\tData 0.325 (0.316)\tLoss 0.0505 (0.0508)\tSoftmaxLoss 0.0004 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:30:59 2022\n",
            "Test: [0/11]\tTime 3.099 (3.099)\tSoftmaxLoss 0.0947 (0.0947)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 81.040\n",
            "Time: Thu Mar 31 03:31:15 2022\n",
            "Step: 18625\t Epoch: [125][0/149]\tTime 1.720 (1.720)\tData 1.224 (1.224)\tLoss 0.0506 (0.0506)\tSoftmaxLoss 0.0006 (0.0006)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:31:33 2022\n",
            "Step: 18645\t Epoch: [125][20/149]\tTime 1.196 (0.960)\tData 0.686 (0.452)\tLoss 0.0501 (0.0505)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:31:48 2022\n",
            "Step: 18665\t Epoch: [125][40/149]\tTime 0.732 (0.865)\tData 0.238 (0.360)\tLoss 0.0506 (0.0505)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0505 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:32:06 2022\n",
            "Step: 18685\t Epoch: [125][60/149]\tTime 1.266 (0.869)\tData 0.754 (0.363)\tLoss 0.0507 (0.0508)\tSoftmaxLoss 0.0000 (0.0005)\tRankLoss 0.0507 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:32:21 2022\n",
            "Step: 18705\t Epoch: [125][80/149]\tTime 1.072 (0.844)\tData 0.576 (0.339)\tLoss 0.0503 (0.0508)\tSoftmaxLoss 0.0003 (0.0005)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:32:36 2022\n",
            "Step: 18725\t Epoch: [125][100/149]\tTime 1.089 (0.826)\tData 0.592 (0.321)\tLoss 0.0504 (0.0507)\tSoftmaxLoss 0.0002 (0.0005)\tRankLoss 0.0501 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:32:52 2022\n",
            "Step: 18745\t Epoch: [125][120/149]\tTime 1.249 (0.823)\tData 0.755 (0.319)\tLoss 0.0500 (0.0507)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:33:09 2022\n",
            "Step: 18765\t Epoch: [125][140/149]\tTime 0.913 (0.824)\tData 0.414 (0.319)\tLoss 0.0500 (0.0507)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:33:18 2022\n",
            "Test: [0/11]\tTime 3.283 (3.283)\tSoftmaxLoss 0.1116 (0.1116)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 81.040\n",
            "Time: Thu Mar 31 03:33:35 2022\n",
            "Step: 18774\t Epoch: [126][0/149]\tTime 1.861 (1.861)\tData 1.352 (1.352)\tLoss 0.0500 (0.0500)\tSoftmaxLoss 0.0000 (0.0000)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:33:52 2022\n",
            "Step: 18794\t Epoch: [126][20/149]\tTime 0.509 (0.925)\tData 0.004 (0.416)\tLoss 0.0504 (0.0504)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0504 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:34:08 2022\n",
            "Step: 18814\t Epoch: [126][40/149]\tTime 0.510 (0.866)\tData 0.002 (0.361)\tLoss 0.0505 (0.0505)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0505 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:34:26 2022\n",
            "Step: 18834\t Epoch: [126][60/149]\tTime 0.505 (0.868)\tData 0.003 (0.363)\tLoss 0.0505 (0.0505)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0505 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:34:41 2022\n",
            "Step: 18854\t Epoch: [126][80/149]\tTime 0.791 (0.841)\tData 0.282 (0.337)\tLoss 0.0507 (0.0505)\tSoftmaxLoss 0.0007 (0.0003)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:34:57 2022\n",
            "Step: 18874\t Epoch: [126][100/149]\tTime 0.501 (0.837)\tData 0.003 (0.333)\tLoss 0.0502 (0.0506)\tSoftmaxLoss 0.0002 (0.0003)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:35:13 2022\n",
            "Step: 18894\t Epoch: [126][120/149]\tTime 0.963 (0.831)\tData 0.471 (0.327)\tLoss 0.0504 (0.0506)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0501 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:35:30 2022\n",
            "Step: 18914\t Epoch: [126][140/149]\tTime 1.451 (0.832)\tData 0.941 (0.328)\tLoss 0.0507 (0.0506)\tSoftmaxLoss 0.0005 (0.0004)\tRankLoss 0.0502 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:35:39 2022\n",
            "Test: [0/11]\tTime 3.238 (3.238)\tSoftmaxLoss 0.0925 (0.0925)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 80.734\n",
            "Time: Thu Mar 31 03:35:56 2022\n",
            "Step: 18923\t Epoch: [127][0/149]\tTime 2.339 (2.339)\tData 1.840 (1.840)\tLoss 0.0502 (0.0502)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0501 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:36:14 2022\n",
            "Step: 18943\t Epoch: [127][20/149]\tTime 1.215 (0.954)\tData 0.722 (0.451)\tLoss 0.0504 (0.0506)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0502 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:36:30 2022\n",
            "Step: 18963\t Epoch: [127][40/149]\tTime 1.368 (0.893)\tData 0.869 (0.390)\tLoss 0.0508 (0.0505)\tSoftmaxLoss 0.0005 (0.0003)\tRankLoss 0.0504 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:36:47 2022\n",
            "Step: 18983\t Epoch: [127][60/149]\tTime 0.911 (0.875)\tData 0.410 (0.373)\tLoss 0.0501 (0.0506)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:37:02 2022\n",
            "Step: 19003\t Epoch: [127][80/149]\tTime 1.095 (0.847)\tData 0.600 (0.344)\tLoss 0.0505 (0.0506)\tSoftmaxLoss 0.0005 (0.0003)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:37:19 2022\n",
            "Step: 19023\t Epoch: [127][100/149]\tTime 1.035 (0.845)\tData 0.538 (0.344)\tLoss 0.0506 (0.0506)\tSoftmaxLoss 0.0004 (0.0003)\tRankLoss 0.0502 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:37:35 2022\n",
            "Step: 19043\t Epoch: [127][120/149]\tTime 1.315 (0.842)\tData 0.811 (0.340)\tLoss 0.0510 (0.0506)\tSoftmaxLoss 0.0002 (0.0003)\tRankLoss 0.0508 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:37:51 2022\n",
            "Step: 19063\t Epoch: [127][140/149]\tTime 0.521 (0.832)\tData 0.013 (0.330)\tLoss 0.0502 (0.0506)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0502 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:38:00 2022\n",
            "Test: [0/11]\tTime 3.144 (3.144)\tSoftmaxLoss 0.1039 (0.1039)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.428\n",
            "Time: Thu Mar 31 03:38:16 2022\n",
            "Step: 19072\t Epoch: [128][0/149]\tTime 2.067 (2.067)\tData 1.552 (1.552)\tLoss 0.0501 (0.0501)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:38:33 2022\n",
            "Step: 19092\t Epoch: [128][20/149]\tTime 0.849 (0.925)\tData 0.356 (0.416)\tLoss 0.0507 (0.0505)\tSoftmaxLoss 0.0001 (0.0002)\tRankLoss 0.0506 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:38:50 2022\n",
            "Step: 19112\t Epoch: [128][40/149]\tTime 1.038 (0.889)\tData 0.539 (0.384)\tLoss 0.0500 (0.0505)\tSoftmaxLoss 0.0000 (0.0002)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:39:06 2022\n",
            "Step: 19132\t Epoch: [128][60/149]\tTime 1.145 (0.850)\tData 0.645 (0.345)\tLoss 0.0513 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0511 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:39:22 2022\n",
            "Step: 19152\t Epoch: [128][80/149]\tTime 1.261 (0.840)\tData 0.763 (0.335)\tLoss 0.0512 (0.0507)\tSoftmaxLoss 0.0004 (0.0003)\tRankLoss 0.0508 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:39:38 2022\n",
            "Step: 19172\t Epoch: [128][100/149]\tTime 0.849 (0.830)\tData 0.346 (0.325)\tLoss 0.0502 (0.0507)\tSoftmaxLoss 0.0002 (0.0003)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:39:54 2022\n",
            "Step: 19192\t Epoch: [128][120/149]\tTime 0.511 (0.827)\tData 0.009 (0.322)\tLoss 0.0500 (0.0506)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:40:10 2022\n",
            "Step: 19212\t Epoch: [128][140/149]\tTime 0.557 (0.826)\tData 0.060 (0.321)\tLoss 0.0500 (0.0506)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:40:20 2022\n",
            "Test: [0/11]\tTime 3.167 (3.167)\tSoftmaxLoss 0.0936 (0.0936)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 81.040\n",
            "Time: Thu Mar 31 03:40:37 2022\n",
            "Step: 19221\t Epoch: [129][0/149]\tTime 1.943 (1.943)\tData 1.443 (1.443)\tLoss 0.0501 (0.0501)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:40:54 2022\n",
            "Step: 19241\t Epoch: [129][20/149]\tTime 0.518 (0.925)\tData 0.007 (0.416)\tLoss 0.0552 (0.0506)\tSoftmaxLoss 0.0052 (0.0005)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:41:11 2022\n",
            "Step: 19261\t Epoch: [129][40/149]\tTime 0.744 (0.885)\tData 0.242 (0.379)\tLoss 0.0500 (0.0505)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:41:28 2022\n",
            "Step: 19281\t Epoch: [129][60/149]\tTime 1.344 (0.877)\tData 0.832 (0.371)\tLoss 0.0503 (0.0505)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0502 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:41:43 2022\n",
            "Step: 19301\t Epoch: [129][80/149]\tTime 1.056 (0.844)\tData 0.561 (0.338)\tLoss 0.0501 (0.0505)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:42:00 2022\n",
            "Step: 19321\t Epoch: [129][100/149]\tTime 0.513 (0.845)\tData 0.009 (0.340)\tLoss 0.0509 (0.0505)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0508 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:42:16 2022\n",
            "Step: 19341\t Epoch: [129][120/149]\tTime 0.532 (0.838)\tData 0.002 (0.333)\tLoss 0.0508 (0.0505)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0508 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:42:33 2022\n",
            "Step: 19361\t Epoch: [129][140/149]\tTime 0.500 (0.840)\tData 0.002 (0.335)\tLoss 0.0505 (0.0505)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0504 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:42:42 2022\n",
            "Test: [0/11]\tTime 3.185 (3.185)\tSoftmaxLoss 0.0898 (0.0898)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 79.817\n",
            "Time: Thu Mar 31 03:42:58 2022\n",
            "Step: 19370\t Epoch: [130][0/149]\tTime 2.047 (2.047)\tData 1.548 (1.548)\tLoss 0.0508 (0.0508)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0507 (0.0507)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:43:15 2022\n",
            "Step: 19390\t Epoch: [130][20/149]\tTime 0.504 (0.890)\tData 0.003 (0.382)\tLoss 0.0502 (0.0503)\tSoftmaxLoss 0.0002 (0.0002)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:43:32 2022\n",
            "Step: 19410\t Epoch: [130][40/149]\tTime 0.496 (0.867)\tData 0.002 (0.362)\tLoss 0.0501 (0.0506)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:43:49 2022\n",
            "Step: 19430\t Epoch: [130][60/149]\tTime 0.514 (0.857)\tData 0.010 (0.352)\tLoss 0.0532 (0.0506)\tSoftmaxLoss 0.0025 (0.0004)\tRankLoss 0.0507 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:44:05 2022\n",
            "Step: 19450\t Epoch: [130][80/149]\tTime 1.159 (0.852)\tData 0.654 (0.347)\tLoss 0.0510 (0.0506)\tSoftmaxLoss 0.0007 (0.0003)\tRankLoss 0.0503 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:44:22 2022\n",
            "Step: 19470\t Epoch: [130][100/149]\tTime 0.511 (0.845)\tData 0.009 (0.340)\tLoss 0.0503 (0.0506)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0502 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:44:38 2022\n",
            "Step: 19490\t Epoch: [130][120/149]\tTime 0.516 (0.842)\tData 0.014 (0.338)\tLoss 0.0501 (0.0506)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:44:54 2022\n",
            "Step: 19510\t Epoch: [130][140/149]\tTime 1.021 (0.837)\tData 0.507 (0.333)\tLoss 0.0505 (0.0506)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0504 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:45:04 2022\n",
            "Test: [0/11]\tTime 3.214 (3.214)\tSoftmaxLoss 0.1064 (0.1064)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 81.346\n",
            "Time: Thu Mar 31 03:45:19 2022\n",
            "Step: 19519\t Epoch: [131][0/149]\tTime 1.893 (1.893)\tData 1.377 (1.377)\tLoss 0.0503 (0.0503)\tSoftmaxLoss 0.0002 (0.0002)\tRankLoss 0.0501 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:45:37 2022\n",
            "Step: 19539\t Epoch: [131][20/149]\tTime 1.265 (0.923)\tData 0.764 (0.416)\tLoss 0.0501 (0.0505)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:45:54 2022\n",
            "Step: 19559\t Epoch: [131][40/149]\tTime 1.121 (0.880)\tData 0.622 (0.374)\tLoss 0.0500 (0.0506)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:46:09 2022\n",
            "Step: 19579\t Epoch: [131][60/149]\tTime 1.211 (0.850)\tData 0.716 (0.345)\tLoss 0.0503 (0.0506)\tSoftmaxLoss 0.0002 (0.0004)\tRankLoss 0.0502 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:46:26 2022\n",
            "Step: 19599\t Epoch: [131][80/149]\tTime 0.837 (0.844)\tData 0.330 (0.340)\tLoss 0.0503 (0.0506)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0502 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:46:41 2022\n",
            "Step: 19619\t Epoch: [131][100/149]\tTime 0.508 (0.829)\tData 0.006 (0.325)\tLoss 0.0501 (0.0512)\tSoftmaxLoss 0.0001 (0.0010)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (99.992)\n",
            "Time: Thu Mar 31 03:46:58 2022\n",
            "Step: 19639\t Epoch: [131][120/149]\tTime 0.506 (0.833)\tData 0.002 (0.330)\tLoss 0.0501 (0.0511)\tSoftmaxLoss 0.0001 (0.0009)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (99.994)\n",
            "Time: Thu Mar 31 03:47:14 2022\n",
            "Step: 19659\t Epoch: [131][140/149]\tTime 0.748 (0.827)\tData 0.226 (0.324)\tLoss 0.0505 (0.0510)\tSoftmaxLoss 0.0001 (0.0008)\tRankLoss 0.0504 (0.0502)\tPrec@1 100.000 (99.994)\n",
            "Time: Thu Mar 31 03:47:23 2022\n",
            "Test: [0/11]\tTime 3.162 (3.162)\tSoftmaxLoss 0.0669 (0.0669)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.122\n",
            "Time: Thu Mar 31 03:47:39 2022\n",
            "Step: 19668\t Epoch: [132][0/149]\tTime 1.860 (1.860)\tData 1.352 (1.352)\tLoss 0.0505 (0.0505)\tSoftmaxLoss 0.0005 (0.0005)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:47:58 2022\n",
            "Step: 19688\t Epoch: [132][20/149]\tTime 1.168 (0.969)\tData 0.657 (0.461)\tLoss 0.0515 (0.0508)\tSoftmaxLoss 0.0013 (0.0004)\tRankLoss 0.0503 (0.0504)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:48:15 2022\n",
            "Step: 19708\t Epoch: [132][40/149]\tTime 1.774 (0.912)\tData 1.266 (0.407)\tLoss 0.0506 (0.0507)\tSoftmaxLoss 0.0004 (0.0004)\tRankLoss 0.0502 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:48:30 2022\n",
            "Step: 19728\t Epoch: [132][60/149]\tTime 1.251 (0.867)\tData 0.744 (0.363)\tLoss 0.0528 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0527 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:48:45 2022\n",
            "Step: 19748\t Epoch: [132][80/149]\tTime 0.879 (0.842)\tData 0.364 (0.337)\tLoss 0.0508 (0.0508)\tSoftmaxLoss 0.0003 (0.0005)\tRankLoss 0.0505 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:49:03 2022\n",
            "Step: 19768\t Epoch: [132][100/149]\tTime 0.510 (0.845)\tData 0.009 (0.342)\tLoss 0.0503 (0.0507)\tSoftmaxLoss 0.0002 (0.0005)\tRankLoss 0.0501 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:49:19 2022\n",
            "Step: 19788\t Epoch: [132][120/149]\tTime 0.950 (0.838)\tData 0.445 (0.335)\tLoss 0.0500 (0.0508)\tSoftmaxLoss 0.0000 (0.0006)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:49:35 2022\n",
            "Step: 19808\t Epoch: [132][140/149]\tTime 1.378 (0.836)\tData 0.880 (0.333)\tLoss 0.0501 (0.0507)\tSoftmaxLoss 0.0001 (0.0005)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:49:44 2022\n",
            "Test: [0/11]\tTime 3.215 (3.215)\tSoftmaxLoss 0.0553 (0.0553)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 80.734\n",
            "Time: Thu Mar 31 03:50:00 2022\n",
            "Step: 19817\t Epoch: [133][0/149]\tTime 1.952 (1.952)\tData 1.438 (1.438)\tLoss 0.0503 (0.0503)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0501 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:50:18 2022\n",
            "Step: 19837\t Epoch: [133][20/149]\tTime 0.517 (0.940)\tData 0.004 (0.436)\tLoss 0.0501 (0.0508)\tSoftmaxLoss 0.0000 (0.0006)\tRankLoss 0.0501 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:50:34 2022\n",
            "Step: 19857\t Epoch: [133][40/149]\tTime 1.047 (0.873)\tData 0.535 (0.371)\tLoss 0.0501 (0.0507)\tSoftmaxLoss 0.0001 (0.0005)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:50:51 2022\n",
            "Step: 19877\t Epoch: [133][60/149]\tTime 1.150 (0.868)\tData 0.654 (0.366)\tLoss 0.0503 (0.0506)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:51:06 2022\n",
            "Step: 19897\t Epoch: [133][80/149]\tTime 1.216 (0.843)\tData 0.709 (0.341)\tLoss 0.0503 (0.0506)\tSoftmaxLoss 0.0003 (0.0004)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:51:22 2022\n",
            "Step: 19917\t Epoch: [133][100/149]\tTime 1.015 (0.828)\tData 0.515 (0.325)\tLoss 0.0505 (0.0506)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0505 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:51:39 2022\n",
            "Step: 19937\t Epoch: [133][120/149]\tTime 1.233 (0.831)\tData 0.741 (0.328)\tLoss 0.0504 (0.0506)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0501 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:51:54 2022\n",
            "Step: 19957\t Epoch: [133][140/149]\tTime 1.083 (0.825)\tData 0.583 (0.322)\tLoss 0.0520 (0.0505)\tSoftmaxLoss 0.0020 (0.0003)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:52:03 2022\n",
            "Test: [0/11]\tTime 3.165 (3.165)\tSoftmaxLoss 0.0589 (0.0589)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.122\n",
            "Time: Thu Mar 31 03:52:19 2022\n",
            "Step: 19966\t Epoch: [134][0/149]\tTime 1.837 (1.837)\tData 1.325 (1.325)\tLoss 0.0506 (0.0506)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0505 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:52:36 2022\n",
            "Step: 19986\t Epoch: [134][20/149]\tTime 0.505 (0.892)\tData 0.002 (0.383)\tLoss 0.0541 (0.0519)\tSoftmaxLoss 0.0014 (0.0016)\tRankLoss 0.0527 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:52:53 2022\n",
            "Step: 20006\t Epoch: [134][40/149]\tTime 0.494 (0.857)\tData 0.002 (0.352)\tLoss 0.0502 (0.0513)\tSoftmaxLoss 0.0002 (0.0011)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:53:09 2022\n",
            "Step: 20026\t Epoch: [134][60/149]\tTime 1.181 (0.838)\tData 0.686 (0.334)\tLoss 0.0509 (0.0510)\tSoftmaxLoss 0.0009 (0.0008)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:53:25 2022\n",
            "Step: 20046\t Epoch: [134][80/149]\tTime 0.569 (0.828)\tData 0.059 (0.324)\tLoss 0.0502 (0.0510)\tSoftmaxLoss 0.0002 (0.0008)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:53:42 2022\n",
            "Step: 20066\t Epoch: [134][100/149]\tTime 1.240 (0.835)\tData 0.747 (0.331)\tLoss 0.0500 (0.0509)\tSoftmaxLoss 0.0000 (0.0007)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:53:57 2022\n",
            "Step: 20086\t Epoch: [134][120/149]\tTime 0.645 (0.826)\tData 0.153 (0.323)\tLoss 0.0503 (0.0508)\tSoftmaxLoss 0.0001 (0.0006)\tRankLoss 0.0503 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:54:14 2022\n",
            "Step: 20106\t Epoch: [134][140/149]\tTime 0.504 (0.824)\tData 0.002 (0.322)\tLoss 0.0500 (0.0508)\tSoftmaxLoss 0.0000 (0.0005)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:54:23 2022\n",
            "Test: [0/11]\tTime 3.162 (3.162)\tSoftmaxLoss 0.0861 (0.0861)\tPrec@1 93.750 (93.750)\n",
            " * Prec@1 80.122\n",
            "Time: Thu Mar 31 03:54:39 2022\n",
            "Step: 20115\t Epoch: [135][0/149]\tTime 1.765 (1.765)\tData 1.259 (1.259)\tLoss 0.0500 (0.0500)\tSoftmaxLoss 0.0000 (0.0000)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:54:57 2022\n",
            "Step: 20135\t Epoch: [135][20/149]\tTime 0.721 (0.929)\tData 0.218 (0.424)\tLoss 0.0500 (0.0506)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:55:14 2022\n",
            "Step: 20155\t Epoch: [135][40/149]\tTime 1.125 (0.903)\tData 0.630 (0.399)\tLoss 0.0501 (0.0506)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0501 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:55:29 2022\n",
            "Step: 20175\t Epoch: [135][60/149]\tTime 0.500 (0.859)\tData 0.002 (0.354)\tLoss 0.0504 (0.0505)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0504 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:55:46 2022\n",
            "Step: 20195\t Epoch: [135][80/149]\tTime 0.501 (0.851)\tData 0.002 (0.347)\tLoss 0.0505 (0.0506)\tSoftmaxLoss 0.0005 (0.0003)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:56:03 2022\n",
            "Step: 20215\t Epoch: [135][100/149]\tTime 0.693 (0.847)\tData 0.195 (0.343)\tLoss 0.0502 (0.0506)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0501 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:56:18 2022\n",
            "Step: 20235\t Epoch: [135][120/149]\tTime 0.704 (0.838)\tData 0.203 (0.334)\tLoss 0.0519 (0.0506)\tSoftmaxLoss 0.0002 (0.0004)\tRankLoss 0.0517 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:56:35 2022\n",
            "Step: 20255\t Epoch: [135][140/149]\tTime 0.598 (0.839)\tData 0.097 (0.335)\tLoss 0.0501 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:56:45 2022\n",
            "Test: [0/11]\tTime 3.130 (3.130)\tSoftmaxLoss 0.0540 (0.0540)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 80.122\n",
            "Time: Thu Mar 31 03:57:01 2022\n",
            "Step: 20264\t Epoch: [136][0/149]\tTime 2.222 (2.222)\tData 1.715 (1.715)\tLoss 0.0501 (0.0501)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:57:18 2022\n",
            "Step: 20284\t Epoch: [136][20/149]\tTime 1.023 (0.916)\tData 0.515 (0.409)\tLoss 0.0529 (0.0507)\tSoftmaxLoss 0.0029 (0.0004)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:57:33 2022\n",
            "Step: 20304\t Epoch: [136][40/149]\tTime 0.637 (0.845)\tData 0.145 (0.341)\tLoss 0.0512 (0.0516)\tSoftmaxLoss 0.0005 (0.0014)\tRankLoss 0.0507 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:57:50 2022\n",
            "Step: 20324\t Epoch: [136][60/149]\tTime 1.122 (0.848)\tData 0.630 (0.346)\tLoss 0.0502 (0.0512)\tSoftmaxLoss 0.0002 (0.0010)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:58:06 2022\n",
            "Step: 20344\t Epoch: [136][80/149]\tTime 0.864 (0.832)\tData 0.362 (0.329)\tLoss 0.0504 (0.0510)\tSoftmaxLoss 0.0004 (0.0008)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:58:22 2022\n",
            "Step: 20364\t Epoch: [136][100/149]\tTime 0.680 (0.830)\tData 0.186 (0.327)\tLoss 0.0501 (0.0510)\tSoftmaxLoss 0.0000 (0.0007)\tRankLoss 0.0501 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:58:38 2022\n",
            "Step: 20384\t Epoch: [136][120/149]\tTime 0.778 (0.823)\tData 0.260 (0.319)\tLoss 0.0501 (0.0509)\tSoftmaxLoss 0.0001 (0.0006)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:58:56 2022\n",
            "Step: 20404\t Epoch: [136][140/149]\tTime 1.398 (0.831)\tData 0.907 (0.328)\tLoss 0.0500 (0.0508)\tSoftmaxLoss 0.0000 (0.0006)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:59:04 2022\n",
            "Test: [0/11]\tTime 3.149 (3.149)\tSoftmaxLoss 0.0642 (0.0642)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 80.428\n",
            "Time: Thu Mar 31 03:59:20 2022\n",
            "Step: 20413\t Epoch: [137][0/149]\tTime 1.987 (1.987)\tData 1.492 (1.492)\tLoss 0.0516 (0.0516)\tSoftmaxLoss 0.0014 (0.0014)\tRankLoss 0.0502 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:59:37 2022\n",
            "Step: 20433\t Epoch: [137][20/149]\tTime 0.939 (0.909)\tData 0.416 (0.403)\tLoss 0.0502 (0.0504)\tSoftmaxLoss 0.0002 (0.0002)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 03:59:54 2022\n",
            "Step: 20453\t Epoch: [137][40/149]\tTime 0.503 (0.885)\tData 0.003 (0.379)\tLoss 0.0504 (0.0505)\tSoftmaxLoss 0.0004 (0.0002)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:00:10 2022\n",
            "Step: 20473\t Epoch: [137][60/149]\tTime 0.501 (0.853)\tData 0.002 (0.348)\tLoss 0.0501 (0.0505)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:00:26 2022\n",
            "Step: 20493\t Epoch: [137][80/149]\tTime 0.514 (0.841)\tData 0.002 (0.336)\tLoss 0.0500 (0.0506)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:00:42 2022\n",
            "Step: 20513\t Epoch: [137][100/149]\tTime 0.508 (0.834)\tData 0.002 (0.330)\tLoss 0.0501 (0.0506)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:00:59 2022\n",
            "Step: 20533\t Epoch: [137][120/149]\tTime 0.511 (0.835)\tData 0.002 (0.331)\tLoss 0.0526 (0.0506)\tSoftmaxLoss 0.0024 (0.0003)\tRankLoss 0.0502 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:01:15 2022\n",
            "Step: 20553\t Epoch: [137][140/149]\tTime 0.495 (0.828)\tData 0.003 (0.324)\tLoss 0.0500 (0.0506)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:01:24 2022\n",
            "Test: [0/11]\tTime 3.155 (3.155)\tSoftmaxLoss 0.0690 (0.0690)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 81.651\n",
            "Time: Thu Mar 31 04:01:40 2022\n",
            "Step: 20562\t Epoch: [138][0/149]\tTime 2.015 (2.015)\tData 1.503 (1.503)\tLoss 0.0502 (0.0502)\tSoftmaxLoss 0.0002 (0.0002)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:01:58 2022\n",
            "Step: 20582\t Epoch: [138][20/149]\tTime 1.119 (0.937)\tData 0.624 (0.429)\tLoss 0.0510 (0.0505)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0509 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:02:14 2022\n",
            "Step: 20602\t Epoch: [138][40/149]\tTime 0.523 (0.872)\tData 0.003 (0.366)\tLoss 0.0507 (0.0507)\tSoftmaxLoss 0.0005 (0.0005)\tRankLoss 0.0503 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:02:30 2022\n",
            "Step: 20622\t Epoch: [138][60/149]\tTime 0.512 (0.853)\tData 0.002 (0.347)\tLoss 0.0521 (0.0506)\tSoftmaxLoss 0.0007 (0.0004)\tRankLoss 0.0514 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:02:46 2022\n",
            "Step: 20642\t Epoch: [138][80/149]\tTime 0.900 (0.838)\tData 0.406 (0.333)\tLoss 0.0504 (0.0506)\tSoftmaxLoss 0.0004 (0.0003)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:03:02 2022\n",
            "Step: 20662\t Epoch: [138][100/149]\tTime 0.899 (0.830)\tData 0.387 (0.325)\tLoss 0.0500 (0.0505)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:03:18 2022\n",
            "Step: 20682\t Epoch: [138][120/149]\tTime 1.143 (0.824)\tData 0.650 (0.319)\tLoss 0.0510 (0.0505)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0508 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:03:33 2022\n",
            "Step: 20702\t Epoch: [138][140/149]\tTime 0.992 (0.816)\tData 0.483 (0.312)\tLoss 0.0502 (0.0505)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0501 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:03:42 2022\n",
            "Test: [0/11]\tTime 3.108 (3.108)\tSoftmaxLoss 0.0723 (0.0723)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 80.122\n",
            "Time: Thu Mar 31 04:03:59 2022\n",
            "Step: 20711\t Epoch: [139][0/149]\tTime 2.174 (2.174)\tData 1.662 (1.662)\tLoss 0.0501 (0.0501)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:04:17 2022\n",
            "Step: 20731\t Epoch: [139][20/149]\tTime 1.217 (0.977)\tData 0.717 (0.473)\tLoss 0.0501 (0.0507)\tSoftmaxLoss 0.0000 (0.0005)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:04:34 2022\n",
            "Step: 20751\t Epoch: [139][40/149]\tTime 1.270 (0.912)\tData 0.777 (0.408)\tLoss 0.0502 (0.0507)\tSoftmaxLoss 0.0002 (0.0005)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:04:49 2022\n",
            "Step: 20771\t Epoch: [139][60/149]\tTime 1.072 (0.858)\tData 0.566 (0.354)\tLoss 0.0509 (0.0506)\tSoftmaxLoss 0.0009 (0.0004)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:05:06 2022\n",
            "Step: 20791\t Epoch: [139][80/149]\tTime 0.495 (0.853)\tData 0.003 (0.350)\tLoss 0.0502 (0.0506)\tSoftmaxLoss 0.0002 (0.0003)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:05:21 2022\n",
            "Step: 20811\t Epoch: [139][100/149]\tTime 0.513 (0.841)\tData 0.002 (0.337)\tLoss 0.0501 (0.0507)\tSoftmaxLoss 0.0001 (0.0005)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:05:38 2022\n",
            "Step: 20831\t Epoch: [139][120/149]\tTime 0.501 (0.840)\tData 0.007 (0.337)\tLoss 0.0501 (0.0507)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:05:55 2022\n",
            "Step: 20851\t Epoch: [139][140/149]\tTime 0.508 (0.839)\tData 0.002 (0.336)\tLoss 0.0501 (0.0506)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:06:04 2022\n",
            "Test: [0/11]\tTime 3.114 (3.114)\tSoftmaxLoss 0.0475 (0.0475)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 80.122\n",
            "Time: Thu Mar 31 04:06:21 2022\n",
            "Step: 20860\t Epoch: [140][0/149]\tTime 2.016 (2.016)\tData 1.515 (1.515)\tLoss 0.0506 (0.0506)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0503 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:06:38 2022\n",
            "Step: 20880\t Epoch: [140][20/149]\tTime 0.723 (0.939)\tData 0.228 (0.432)\tLoss 0.0502 (0.0506)\tSoftmaxLoss 0.0002 (0.0004)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:06:54 2022\n",
            "Step: 20900\t Epoch: [140][40/149]\tTime 0.976 (0.867)\tData 0.464 (0.362)\tLoss 0.0502 (0.0504)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0501 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:07:11 2022\n",
            "Step: 20920\t Epoch: [140][60/149]\tTime 1.202 (0.855)\tData 0.679 (0.350)\tLoss 0.0503 (0.0504)\tSoftmaxLoss 0.0002 (0.0003)\tRankLoss 0.0502 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:07:26 2022\n",
            "Step: 20940\t Epoch: [140][80/149]\tTime 1.215 (0.834)\tData 0.713 (0.330)\tLoss 0.0501 (0.0504)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:07:43 2022\n",
            "Step: 20960\t Epoch: [140][100/149]\tTime 1.145 (0.834)\tData 0.634 (0.331)\tLoss 0.0506 (0.0505)\tSoftmaxLoss 0.0004 (0.0002)\tRankLoss 0.0503 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:07:59 2022\n",
            "Step: 20980\t Epoch: [140][120/149]\tTime 0.931 (0.829)\tData 0.428 (0.326)\tLoss 0.0509 (0.0505)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0509 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:08:15 2022\n",
            "Step: 21000\t Epoch: [140][140/149]\tTime 1.356 (0.822)\tData 0.851 (0.319)\tLoss 0.0515 (0.0505)\tSoftmaxLoss 0.0001 (0.0002)\tRankLoss 0.0514 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:08:25 2022\n",
            "Test: [0/11]\tTime 3.178 (3.178)\tSoftmaxLoss 0.0448 (0.0448)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 80.428\n",
            "Time: Thu Mar 31 04:08:41 2022\n",
            "Step: 21009\t Epoch: [141][0/149]\tTime 2.161 (2.161)\tData 1.647 (1.647)\tLoss 0.0502 (0.0502)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0501 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:08:58 2022\n",
            "Step: 21029\t Epoch: [141][20/149]\tTime 0.788 (0.904)\tData 0.287 (0.395)\tLoss 0.0503 (0.0505)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:09:14 2022\n",
            "Step: 21049\t Epoch: [141][40/149]\tTime 0.815 (0.864)\tData 0.294 (0.357)\tLoss 0.0501 (0.0505)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:09:31 2022\n",
            "Step: 21069\t Epoch: [141][60/149]\tTime 0.513 (0.847)\tData 0.002 (0.341)\tLoss 0.0500 (0.0505)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:09:47 2022\n",
            "Step: 21089\t Epoch: [141][80/149]\tTime 0.663 (0.835)\tData 0.156 (0.330)\tLoss 0.0502 (0.0505)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0501 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:10:02 2022\n",
            "Step: 21109\t Epoch: [141][100/149]\tTime 0.938 (0.826)\tData 0.443 (0.321)\tLoss 0.0500 (0.0505)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:10:18 2022\n",
            "Step: 21129\t Epoch: [141][120/149]\tTime 0.518 (0.820)\tData 0.009 (0.315)\tLoss 0.0504 (0.0504)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0503 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:10:34 2022\n",
            "Step: 21149\t Epoch: [141][140/149]\tTime 0.508 (0.816)\tData 0.002 (0.311)\tLoss 0.0506 (0.0505)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0505 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:10:44 2022\n",
            "Test: [0/11]\tTime 3.075 (3.075)\tSoftmaxLoss 0.0602 (0.0602)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 81.040\n",
            "Time: Thu Mar 31 04:11:00 2022\n",
            "Step: 21158\t Epoch: [142][0/149]\tTime 2.042 (2.042)\tData 1.548 (1.548)\tLoss 0.0502 (0.0502)\tSoftmaxLoss 0.0002 (0.0002)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:11:18 2022\n",
            "Step: 21178\t Epoch: [142][20/149]\tTime 0.921 (0.941)\tData 0.417 (0.432)\tLoss 0.0503 (0.0503)\tSoftmaxLoss 0.0002 (0.0003)\tRankLoss 0.0501 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:11:35 2022\n",
            "Step: 21198\t Epoch: [142][40/149]\tTime 1.365 (0.883)\tData 0.858 (0.377)\tLoss 0.0501 (0.0504)\tSoftmaxLoss 0.0001 (0.0002)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:11:51 2022\n",
            "Step: 21218\t Epoch: [142][60/149]\tTime 0.977 (0.855)\tData 0.475 (0.350)\tLoss 0.0501 (0.0505)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:12:07 2022\n",
            "Step: 21238\t Epoch: [142][80/149]\tTime 0.972 (0.847)\tData 0.468 (0.342)\tLoss 0.0500 (0.0504)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:12:23 2022\n",
            "Step: 21258\t Epoch: [142][100/149]\tTime 0.509 (0.840)\tData 0.002 (0.335)\tLoss 0.0500 (0.0504)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:12:40 2022\n",
            "Step: 21278\t Epoch: [142][120/149]\tTime 0.512 (0.841)\tData 0.002 (0.336)\tLoss 0.0502 (0.0504)\tSoftmaxLoss 0.0002 (0.0003)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:12:56 2022\n",
            "Step: 21298\t Epoch: [142][140/149]\tTime 1.051 (0.837)\tData 0.539 (0.332)\tLoss 0.0507 (0.0504)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0506 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:13:06 2022\n",
            "Test: [0/11]\tTime 3.145 (3.145)\tSoftmaxLoss 0.0619 (0.0619)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 81.346\n",
            "Time: Thu Mar 31 04:13:22 2022\n",
            "Step: 21307\t Epoch: [143][0/149]\tTime 1.776 (1.776)\tData 1.256 (1.256)\tLoss 0.0500 (0.0500)\tSoftmaxLoss 0.0000 (0.0000)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:13:40 2022\n",
            "Step: 21327\t Epoch: [143][20/149]\tTime 1.237 (0.939)\tData 0.732 (0.428)\tLoss 0.0515 (0.0503)\tSoftmaxLoss 0.0000 (0.0002)\tRankLoss 0.0515 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:13:57 2022\n",
            "Step: 21347\t Epoch: [143][40/149]\tTime 1.443 (0.889)\tData 0.950 (0.382)\tLoss 0.0500 (0.0505)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:14:13 2022\n",
            "Step: 21367\t Epoch: [143][60/149]\tTime 0.650 (0.859)\tData 0.144 (0.353)\tLoss 0.0509 (0.0505)\tSoftmaxLoss 0.0002 (0.0004)\tRankLoss 0.0507 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:14:29 2022\n",
            "Step: 21387\t Epoch: [143][80/149]\tTime 1.375 (0.853)\tData 0.882 (0.348)\tLoss 0.0500 (0.0505)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:14:45 2022\n",
            "Step: 21407\t Epoch: [143][100/149]\tTime 0.901 (0.836)\tData 0.383 (0.330)\tLoss 0.0501 (0.0505)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:15:01 2022\n",
            "Step: 21427\t Epoch: [143][120/149]\tTime 0.725 (0.831)\tData 0.212 (0.326)\tLoss 0.0501 (0.0505)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:15:18 2022\n",
            "Step: 21447\t Epoch: [143][140/149]\tTime 1.266 (0.836)\tData 0.770 (0.331)\tLoss 0.0503 (0.0505)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0503 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:15:27 2022\n",
            "Test: [0/11]\tTime 3.265 (3.265)\tSoftmaxLoss 0.0597 (0.0597)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 81.651\n",
            "Time: Thu Mar 31 04:15:44 2022\n",
            "Step: 21456\t Epoch: [144][0/149]\tTime 2.259 (2.259)\tData 1.763 (1.763)\tLoss 0.0508 (0.0508)\tSoftmaxLoss 0.0000 (0.0000)\tRankLoss 0.0508 (0.0508)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:16:01 2022\n",
            "Step: 21476\t Epoch: [144][20/149]\tTime 1.274 (0.950)\tData 0.780 (0.441)\tLoss 0.0501 (0.0506)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0500 (0.0503)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:16:17 2022\n",
            "Step: 21496\t Epoch: [144][40/149]\tTime 1.029 (0.861)\tData 0.512 (0.354)\tLoss 0.0503 (0.0504)\tSoftmaxLoss 0.0000 (0.0002)\tRankLoss 0.0502 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:16:34 2022\n",
            "Step: 21516\t Epoch: [144][60/149]\tTime 0.956 (0.858)\tData 0.463 (0.353)\tLoss 0.0509 (0.0504)\tSoftmaxLoss 0.0009 (0.0002)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:16:51 2022\n",
            "Step: 21536\t Epoch: [144][80/149]\tTime 0.779 (0.853)\tData 0.276 (0.349)\tLoss 0.0500 (0.0504)\tSoftmaxLoss 0.0000 (0.0002)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:17:06 2022\n",
            "Step: 21556\t Epoch: [144][100/149]\tTime 0.506 (0.837)\tData 0.002 (0.333)\tLoss 0.0505 (0.0503)\tSoftmaxLoss 0.0001 (0.0002)\tRankLoss 0.0505 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:17:22 2022\n",
            "Step: 21576\t Epoch: [144][120/149]\tTime 1.043 (0.831)\tData 0.544 (0.327)\tLoss 0.0501 (0.0503)\tSoftmaxLoss 0.0000 (0.0002)\tRankLoss 0.0501 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:17:39 2022\n",
            "Step: 21596\t Epoch: [144][140/149]\tTime 0.981 (0.834)\tData 0.490 (0.330)\tLoss 0.0500 (0.0503)\tSoftmaxLoss 0.0000 (0.0002)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:17:48 2022\n",
            "Test: [0/11]\tTime 3.211 (3.211)\tSoftmaxLoss 0.0521 (0.0521)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 81.346\n",
            "Time: Thu Mar 31 04:18:04 2022\n",
            "Step: 21605\t Epoch: [145][0/149]\tTime 1.990 (1.990)\tData 1.496 (1.496)\tLoss 0.0501 (0.0501)\tSoftmaxLoss 0.0000 (0.0000)\tRankLoss 0.0501 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:18:21 2022\n",
            "Step: 21625\t Epoch: [145][20/149]\tTime 0.612 (0.899)\tData 0.103 (0.392)\tLoss 0.0529 (0.0505)\tSoftmaxLoss 0.0027 (0.0003)\tRankLoss 0.0502 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:18:37 2022\n",
            "Step: 21645\t Epoch: [145][40/149]\tTime 0.506 (0.844)\tData 0.006 (0.338)\tLoss 0.0501 (0.0504)\tSoftmaxLoss 0.0000 (0.0002)\tRankLoss 0.0501 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:18:54 2022\n",
            "Step: 21665\t Epoch: [145][60/149]\tTime 0.507 (0.842)\tData 0.009 (0.337)\tLoss 0.0500 (0.0504)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:19:10 2022\n",
            "Step: 21685\t Epoch: [145][80/149]\tTime 0.508 (0.835)\tData 0.002 (0.330)\tLoss 0.0500 (0.0506)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:19:27 2022\n",
            "Step: 21705\t Epoch: [145][100/149]\tTime 0.520 (0.836)\tData 0.006 (0.332)\tLoss 0.0505 (0.0505)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0504 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:19:43 2022\n",
            "Step: 21725\t Epoch: [145][120/149]\tTime 0.509 (0.829)\tData 0.002 (0.325)\tLoss 0.0507 (0.0506)\tSoftmaxLoss 0.0007 (0.0004)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:19:59 2022\n",
            "Step: 21745\t Epoch: [145][140/149]\tTime 1.149 (0.829)\tData 0.656 (0.326)\tLoss 0.0505 (0.0505)\tSoftmaxLoss 0.0000 (0.0004)\tRankLoss 0.0505 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:20:08 2022\n",
            "Test: [0/11]\tTime 3.149 (3.149)\tSoftmaxLoss 0.0621 (0.0621)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 81.040\n",
            "Time: Thu Mar 31 04:20:24 2022\n",
            "Step: 21754\t Epoch: [146][0/149]\tTime 2.198 (2.198)\tData 1.690 (1.690)\tLoss 0.0506 (0.0506)\tSoftmaxLoss 0.0001 (0.0001)\tRankLoss 0.0505 (0.0505)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:20:42 2022\n",
            "Step: 21774\t Epoch: [146][20/149]\tTime 1.334 (0.942)\tData 0.830 (0.433)\tLoss 0.0505 (0.0503)\tSoftmaxLoss 0.0000 (0.0001)\tRankLoss 0.0505 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:20:58 2022\n",
            "Step: 21794\t Epoch: [146][40/149]\tTime 1.322 (0.864)\tData 0.824 (0.359)\tLoss 0.0501 (0.0503)\tSoftmaxLoss 0.0001 (0.0002)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:21:14 2022\n",
            "Step: 21814\t Epoch: [146][60/149]\tTime 0.836 (0.857)\tData 0.338 (0.353)\tLoss 0.0519 (0.0504)\tSoftmaxLoss 0.0000 (0.0002)\tRankLoss 0.0519 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:21:31 2022\n",
            "Step: 21834\t Epoch: [146][80/149]\tTime 1.908 (0.852)\tData 1.393 (0.348)\tLoss 0.0502 (0.0504)\tSoftmaxLoss 0.0002 (0.0002)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:21:45 2022\n",
            "Step: 21854\t Epoch: [146][100/149]\tTime 0.506 (0.825)\tData 0.002 (0.321)\tLoss 0.0500 (0.0503)\tSoftmaxLoss 0.0000 (0.0002)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:22:02 2022\n",
            "Step: 21874\t Epoch: [146][120/149]\tTime 1.165 (0.828)\tData 0.670 (0.324)\tLoss 0.0502 (0.0503)\tSoftmaxLoss 0.0002 (0.0002)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:22:19 2022\n",
            "Step: 21894\t Epoch: [146][140/149]\tTime 1.367 (0.832)\tData 0.866 (0.329)\tLoss 0.0507 (0.0503)\tSoftmaxLoss 0.0006 (0.0002)\tRankLoss 0.0501 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:22:29 2022\n",
            "Test: [0/11]\tTime 3.167 (3.167)\tSoftmaxLoss 0.0508 (0.0508)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 80.428\n",
            "Time: Thu Mar 31 04:22:44 2022\n",
            "Step: 21903\t Epoch: [147][0/149]\tTime 1.520 (1.520)\tData 1.021 (1.021)\tLoss 0.0500 (0.0500)\tSoftmaxLoss 0.0000 (0.0000)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:23:02 2022\n",
            "Step: 21923\t Epoch: [147][20/149]\tTime 0.548 (0.884)\tData 0.045 (0.375)\tLoss 0.0506 (0.0503)\tSoftmaxLoss 0.0001 (0.0002)\tRankLoss 0.0505 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:23:18 2022\n",
            "Step: 21943\t Epoch: [147][40/149]\tTime 0.506 (0.847)\tData 0.008 (0.341)\tLoss 0.0519 (0.0505)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0518 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:23:34 2022\n",
            "Step: 21963\t Epoch: [147][60/149]\tTime 0.916 (0.829)\tData 0.410 (0.324)\tLoss 0.0501 (0.0505)\tSoftmaxLoss 0.0001 (0.0004)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:23:50 2022\n",
            "Step: 21983\t Epoch: [147][80/149]\tTime 0.501 (0.827)\tData 0.002 (0.322)\tLoss 0.0501 (0.0504)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:24:05 2022\n",
            "Step: 22003\t Epoch: [147][100/149]\tTime 0.986 (0.817)\tData 0.486 (0.313)\tLoss 0.0501 (0.0505)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:24:23 2022\n",
            "Step: 22023\t Epoch: [147][120/149]\tTime 0.504 (0.823)\tData 0.002 (0.319)\tLoss 0.0502 (0.0505)\tSoftmaxLoss 0.0002 (0.0003)\tRankLoss 0.0500 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:24:39 2022\n",
            "Step: 22043\t Epoch: [147][140/149]\tTime 0.516 (0.822)\tData 0.002 (0.319)\tLoss 0.0509 (0.0504)\tSoftmaxLoss 0.0000 (0.0003)\tRankLoss 0.0509 (0.0502)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:24:48 2022\n",
            "Test: [0/11]\tTime 3.061 (3.061)\tSoftmaxLoss 0.0440 (0.0440)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 80.734\n",
            "Time: Thu Mar 31 04:25:05 2022\n",
            "Step: 22052\t Epoch: [148][0/149]\tTime 1.799 (1.799)\tData 1.285 (1.285)\tLoss 0.0503 (0.0503)\tSoftmaxLoss 0.0003 (0.0003)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:25:22 2022\n",
            "Step: 22072\t Epoch: [148][20/149]\tTime 0.512 (0.913)\tData 0.008 (0.404)\tLoss 0.0513 (0.0505)\tSoftmaxLoss 0.0007 (0.0004)\tRankLoss 0.0505 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:25:39 2022\n",
            "Step: 22092\t Epoch: [148][40/149]\tTime 0.499 (0.888)\tData 0.002 (0.381)\tLoss 0.0502 (0.0504)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0501 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:25:55 2022\n",
            "Step: 22112\t Epoch: [148][60/149]\tTime 0.786 (0.862)\tData 0.294 (0.357)\tLoss 0.0501 (0.0503)\tSoftmaxLoss 0.0001 (0.0002)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:26:13 2022\n",
            "Step: 22132\t Epoch: [148][80/149]\tTime 0.963 (0.864)\tData 0.467 (0.360)\tLoss 0.0512 (0.0504)\tSoftmaxLoss 0.0001 (0.0003)\tRankLoss 0.0510 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:26:29 2022\n",
            "Step: 22152\t Epoch: [148][100/149]\tTime 0.826 (0.853)\tData 0.320 (0.348)\tLoss 0.0500 (0.0503)\tSoftmaxLoss 0.0000 (0.0002)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:26:44 2022\n",
            "Step: 22172\t Epoch: [148][120/149]\tTime 0.538 (0.839)\tData 0.034 (0.334)\tLoss 0.0501 (0.0503)\tSoftmaxLoss 0.0001 (0.0002)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:27:02 2022\n",
            "Step: 22192\t Epoch: [148][140/149]\tTime 0.511 (0.843)\tData 0.002 (0.339)\tLoss 0.0502 (0.0503)\tSoftmaxLoss 0.0002 (0.0002)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:27:11 2022\n",
            "Test: [0/11]\tTime 3.122 (3.122)\tSoftmaxLoss 0.0501 (0.0501)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 80.428\n",
            "Time: Thu Mar 31 04:27:28 2022\n",
            "Step: 22201\t Epoch: [149][0/149]\tTime 2.397 (2.397)\tData 1.885 (1.885)\tLoss 0.0504 (0.0504)\tSoftmaxLoss 0.0004 (0.0004)\tRankLoss 0.0500 (0.0500)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:27:45 2022\n",
            "Step: 22221\t Epoch: [149][20/149]\tTime 0.959 (0.956)\tData 0.456 (0.451)\tLoss 0.0504 (0.0504)\tSoftmaxLoss 0.0004 (0.0003)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:28:02 2022\n",
            "Step: 22241\t Epoch: [149][40/149]\tTime 1.110 (0.902)\tData 0.618 (0.398)\tLoss 0.0501 (0.0503)\tSoftmaxLoss 0.0001 (0.0002)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:28:19 2022\n",
            "Step: 22261\t Epoch: [149][60/149]\tTime 0.599 (0.877)\tData 0.098 (0.373)\tLoss 0.0502 (0.0504)\tSoftmaxLoss 0.0001 (0.0002)\tRankLoss 0.0501 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:28:35 2022\n",
            "Step: 22281\t Epoch: [149][80/149]\tTime 0.919 (0.856)\tData 0.420 (0.352)\tLoss 0.0501 (0.0503)\tSoftmaxLoss 0.0001 (0.0002)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:28:52 2022\n",
            "Step: 22301\t Epoch: [149][100/149]\tTime 1.486 (0.856)\tData 0.986 (0.352)\tLoss 0.0506 (0.0504)\tSoftmaxLoss 0.0006 (0.0002)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:29:08 2022\n",
            "Step: 22321\t Epoch: [149][120/149]\tTime 0.731 (0.849)\tData 0.236 (0.345)\tLoss 0.0500 (0.0504)\tSoftmaxLoss 0.0000 (0.0002)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:29:24 2022\n",
            "Step: 22341\t Epoch: [149][140/149]\tTime 0.855 (0.842)\tData 0.361 (0.338)\tLoss 0.0502 (0.0504)\tSoftmaxLoss 0.0002 (0.0002)\tRankLoss 0.0500 (0.0501)\tPrec@1 100.000 (100.000)\n",
            "Time: Thu Mar 31 04:29:33 2022\n",
            "Test: [0/11]\tTime 3.171 (3.171)\tSoftmaxLoss 0.0406 (0.0406)\tPrec@1 96.875 (96.875)\n",
            " * Prec@1 80.122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cells Bellow are used for testing phase"
      ],
      "metadata": {
        "id": "z-Ek5yJMpQf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomDatasetTest(Dataset):\n",
        "    def __init__(self, transform=None, dataloader=default_loader):\n",
        "        self.transform = transform\n",
        "        self.dataloader = dataloader\n",
        "\n",
        "        with open('/content/drive/MyDrive/LV_data/main_closure_hardware/test/test.txt', 'r') as fid:\n",
        "            self.imglist = fid.readlines()\n",
        "\n",
        "        self.labels = []\n",
        "        for line in self.imglist:\n",
        "            #print(i)\n",
        "            image_path, label = line.strip().split()\n",
        "            self.labels.append(int(label))\n",
        "        self.labels = np.array(self.labels)\n",
        "        self.labels = torch.LongTensor(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_name, label = self.imglist[index].strip().split()\n",
        "        image_path = image_name\n",
        "        img = self.dataloader(image_path)\n",
        "        img = self.transform(img)\n",
        "        label = int(label)\n",
        "        label = torch.LongTensor([label])\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imglist)"
      ],
      "metadata": {
        "id": "j5bhDdca2Y5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = API_Net()\n",
        "model = model.to(device)\n",
        "model.conv = nn.DataParallel(model.conv)\n",
        "checkpoint = torch.load('/content/drive/MyDrive/main-closure_model_best.pth.tar')\n",
        "\n",
        "model.load_state_dict(checkpoint['state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m60jhsC3-qrS",
        "outputId": "84a543ae-f501-4abf-f2dd-370f879eb998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def confusion(scores, targets):\n",
        "    \"\"\" Returns the confusion matrix for the values in the `prediction` and `truth`\n",
        "    tensors, i.e. the amount of positions where the values of `prediction`\n",
        "    and `truth` are\n",
        "    - 1 and 1 (True Positive)\n",
        "    - 1 and 0 (False Positive)\n",
        "    - 0 and 0 (True Negative)\n",
        "    - 0 and 1 (False Negative)\n",
        "    \"\"\"\n",
        "\n",
        "    _, ind = scores.topk(1, 1, True, True)\n",
        "    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n",
        "    prediction = correct.view(-1).float()\n",
        "    #correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
        "    confusion_vector = prediction / targets\n",
        "    # Element-wise division of the 2 tensors returns a new tensor which holds a\n",
        "    # unique value for each case:\n",
        "    #   1     where prediction and truth are 1 (True Positive)\n",
        "    #   inf   where prediction is 1 and truth is 0 (False Positive)\n",
        "    #   nan   where prediction and truth are 0 (True Negative)\n",
        "    #   0     where prediction is 0 and truth is 1 (False Negative)\n",
        "\n",
        "    true_positives = torch.sum(confusion_vector == 1).item()\n",
        "    false_positives = torch.sum(confusion_vector == float('inf')).item()\n",
        "    true_negatives = torch.sum(torch.isnan(confusion_vector)).item()\n",
        "    false_negatives = torch.sum(confusion_vector == 0).item()\n",
        "\n",
        "    return true_positives, false_positives, true_negatives, false_negatives"
      ],
      "metadata": {
        "id": "ovp_hztJ-uN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = RandomDatasetTest(transform=transforms.Compose([\n",
        "                transforms.Resize([512, 512]),\n",
        "                transforms.CenterCrop([448, 448]),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=(0.485, 0.456, 0.406),\n",
        "                    std=(0.229, 0.224, 0.225)\n",
        "                )]))\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=6, shuffle=False,\n",
        "                num_workers=workers, pin_memory=True)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "TP=0\n",
        "TN=0\n",
        "FP=0\n",
        "FN=0\n",
        "for input, target in test_loader:\n",
        "  input_var = input.to(device)\n",
        "  target_var = target.to(device).squeeze()\n",
        "\n",
        "  logits = model(input_var, targets=None, flag='val')\n",
        "  tp, fp, tn, fn = confusion(logits, target_var)\n",
        "\n",
        "  TP += tp\n",
        "  FP += fp\n",
        "  TN += tn\n",
        "  FN += fn\n",
        "sp, se = TN / (TN+FP), TP / (TP+FN)\n",
        "print('specificity: {}\\t sensitivity: {}'.format(sp, se))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWX95fUy-w-_",
        "outputId": "807becf0-4665-4997-d763-1b442c8658b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "specificity: 0.49612403100775193\t sensitivity: 0.49504950495049505\n"
          ]
        }
      ]
    }
  ]
}